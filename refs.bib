@article{Cornebize2017b,
   abstract = {The capacity planning of supercomputers is a complex but important task. The best combination of hardware for the given budget has to be selected. Such choices are often guided by some common rules, elaborated after years of trial and error. The rapid development of the ﬁeld, however, makes this approach costly and ineffective. We believe that experts could beneﬁt from accurate and efﬁcient simulators to design the next generations of supercomputers. In this work, we simulate a complex MPI application at the scale of the current top500 supercomputers, using the Simgrid simulation toolkit. We present the optimization developed to enable this kind of large scale executions, we demonstrate the soundness of the resulting simulation and we illustrate the new possibilities of study.},
   author = {Tom Cornebize},
   title = {Capacity planning of supercomputers: Simulating MPI application at Scale},
   url = {https://hal.inria.fr/hal-01544827/document https://drive.google.com/file/d/0B0LPbUAljFxYWERHR0hVUDR1MGZoTFU5d0NWUzUzdlh3RHJF/view?usp=sharing},
   year = {2017},
}
@working_paper{Alverson2012,
   author = {Bob Alverson and Edwin Froese and Larry Kaplan and Duncan Roweth},
   pages = {1-28},
   title = {Cray ® XC™ Series Network Cost-Effective, High-Bandwidth Networks},
   url = {https://www.alcf.anl.gov/files/CrayXCNetwork.pdf},
   year = {2012},
}
@inproceedings{Hoefler2010,
   abstract = {We introduce LogGOPSim - a fast simulation framework for parallel algorithms at large-scale. LogGOPSim utilizes a slightly extended version of the well-known LogGPS model in combination with fullMPI message matching semantics and detailed simulation of collective operations. In addition, it enables simulation in the traditional LogP, LogGP, and LogGPS models. Its simple and fast single-queue design computes more than 1 million events per second on a single processor and enables large-scale simulations of more than 8 million processes. LogGOPSim also supports the sim- ulation of full MPI applications by reading and simulating MPI profiling traces. We analyze the accuracy and the performance of the simulation and propose a simple extrapolation scheme for parallel applications. Our scheme extrapolates collective opera- tions with high accuracy by rebuilding the communication pattern. Point-to-point operation patterns can be copied in the extrapola- tion and thus retain the main characteristics of scalable parallel applications.},
   author = {Torsten Hoefler and Timo Schneider and Andrew Lumsdaine},
   city = {New York, New York, USA},
   doi = {10.1145/1851476.1851564},
   isbn = {9781605589428},
   journal = {Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing - HPDC '10},
   keywords = {Collective operations,Large-scale performance,LogGOPS model,LogGP,LogGPS,LogP,Message passing interface,Simulation},
   pages = {597},
   publisher = {ACM Press},
   title = {LogGOPSim},
   url = {https://spcl.inf.ethz.ch/Publications/.pdf/hoefler-loggopsim.pdf http://portal.acm.org/citation.cfm?doid=1851476.1851564},
   year = {2010},
}
@article{Hoefler2007,
   abstract = {Network performance measurement and prediction is very important to predict the running time of high performance computing applications. The LogP model family has been proven to be a viable tool to assess the communication performance of parallel architectures. However, nonintrusive LogP parameter assessment is still a very difficult task. We compare well known measurement methods for Log(G)P parameters and discuss their accuracy and network contention. Based on this, a new theoretically exact measurement method that does not saturate the network is derived and explained in detail. Our method only uses benchmarked values instead of computed parameters to compute other parameters to avoid propagation of first-order errors. A methodology to detect protocol changes in the underlying communication subsystem is also proposed. The applicability of our method and the protocol change detection is shown for the low-level API as well as MPI implementations of different modern high performance interconnection networks. The whole method is implemented in the tool Netgauge and it is available as open source to the public. ©2007 IEEE.},
   author = {Torsten Hoefler and Andre Lichei and Wolfgang Rehm},
   doi = {10.1109/IPDPS.2007.370593},
   isbn = {1424409101},
   journal = {Proceedings - 21st International Parallel and Distributed Processing Symposium, IPDPS 2007; Abstracts and CD-ROM},
   pages = {1-8},
   title = {Low-overhead LogGP parameter assessment for modern interconnection networks},
   url = {https://htor.inf.ethz.ch/publications/index.php?pub=39},
   year = {2007},
}
@article{Hoefler2007a,
   abstract = {This paper introduces Netgauge, an extensible open-source framework for implementing network benchmarks. The structure of Netgauge abstracts and explicitly separates communication patterns from communication modules. As a result of this separation of concerns, new benchmark types and new network protocols can be added independently to Netgauge. We describe the rich set of pre-defined communication patterns and communication modules that are available in the current distribution. Benchmark results demonstrate the applicability of the current Netgauge distribution to to different networks. An assortment of use-cases is used to investigate the implementation quality of selected protocols and protocol layers. © Springer-Verlag Berlin Heidelberg 2007.},
   author = {Torsten Hoefler and Torsten Mehlan and Andrew Lumsdaine and Wolfgang Rehm},
   isbn = {9783540754435},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {659-671},
   title = {Netgauge: A network performance measurement framework},
   volume = {4782 LNCS},
   url = {https://htor.inf.ethz.ch/publications/index.php?pub=48},
   year = {2007},
}
@thesis{Vienne2010,
   author = {Jérôme Vienne},
   title = {Prédiction de performances d'applications de calcul haute performance sur réseau Infiniband},
   url = {http://polaris.imag.fr/jean-marc.vincent/index.html/PhD/Vienne.pdf},
   year = {2010},
}
@inproceedings{Casanova2001a,
   abstract = {Advances in hardware and software technologies have made it possible to deploy parallel applications over increasingly large sets of distributed resources. Consequently, the study of scheduling algorithms for such applications has been an active area of research. Given the nature of most scheduling problems one must resort to simulation to effectively evaluate and compare their efficacy over a wide range of scenarios. It has thus become necessary to simulate those algorithms for increasingly complex distributed dynamic, heterogeneous environments. We present Simgrid a simulation toolkit for the study of scheduling algorithms for distributed application. We give the main concepts and models behind Simgrid, describe its API and highlight current implementation issues. We also give some experimental results and describe work that builds on Simgrid's functionalities},
   author = {Henri Casanova},
   doi = {10.1109/CCGRID.2001.923223},
   isbn = {0-7695-1010-8},
   journal = {Proceedings First IEEE/ACM International Symposium on Cluster Computing and the Grid},
   pages = {430-437},
   publisher = {IEEE Comput. Soc},
   title = {Simgrid: a toolkit for the simulation of application scheduling},
   url = {http://ieeexplore.ieee.org/document/923223/},
   year = {2001},
}
@article{Degomme2017a,
   abstract = {© 1990-2012 IEEE. This article summarizes our recent work and developments on SMPI, a flexible simulator of MPI applications. In this tool, we took a particular care to ensure our simulator could be used to produce fast and accurate predictions in a wide variety of situations. Although we did build SMPI on SimGrid whose speed and accuracy had already been assessed in other contexts, moving such techniques to a HPC workload required significant additional effort. Obviously, an accurate modeling of communications and network topology was one of the key to such achievements. Another less obvious key was the choice to combine in a single tool the possibility to do both offline and online simulation.},
   author = {Augustin Degomme and Arnaud Legrand and George S. Markomanolis and Martin Quinson and Mark Stillwell and Frederic Suter},
   doi = {10.1109/TPDS.2017.2669305},
   issn = {1045-9219},
   issue = {8},
   journal = {IEEE Transactions on Parallel and Distributed Systems},
   keywords = {MPI runtime and applications,Simulation,performance prediction and extrapolation},
   month = {8},
   pages = {2387-2400},
   title = {Simulating MPI Applications: The SMPI Approach},
   volume = {28},
   url = {http://ieeexplore.ieee.org/document/7855780/},
   year = {2017},
}
@article{Menard2017,
   author = {Christian Menard and Jeronimo Castrillon and Matthias Jung and Norbert Wehn},
   isbn = {9781538634370},
   journal = {IEEE International Conference on Embedded Computer Systems Architectures Modeling and Simulation (SAMOS)},
   pages = {8},
   title = {System Simulation with gem5 and SystemC},
   year = {2017},
}
@article{Kim2008,
   abstract = {Evolving technology and increasing pin-bandwidth motivate the use of high-radix routers to reduce the diameter, latency, and cost of interconnection networks. High-radix networks, however, require longer cables than their low-radix counterparts. Because cables dominate network cost, the number of cables, and particularly the number of long, global cables should be minimized to realize an efficient network. In this paper, we introduce the dragonfly topology which uses a group of high-radix routers as a virtual router to increase the effective radix of the network. With this organization, each minimally routed packet traverses at most one global channel. By reducing global channels, a dragonfly reduces cost by 20% compared to a flattened butterfly and by 52% compared to a folded Clos network in configurations with ges 16K nodes.We also introduce two new variants of global adaptive routing that enable load-balanced routing in the dragonfly. Each router in a dragonfly must make an adaptive routing decision based on the state of a global channel connected to a different router. Because of the indirect nature of this routing decision, conventional adaptive routing algorithms give degraded performance. We introduce the use of selective virtual-channel discrimination and the use of credit round-trip latency to both sense and signal channel congestion. The combination of these two methods gives throughput and latency that approaches that of an ideal adaptive routing algorithm.},
   author = {John Kim and William J. Dally and Steve Scott and Dennis Abts},
   doi = {10.1109/ISCA.2008.19},
   isbn = {9780769531748},
   issn = {10636897},
   journal = {Proceedings - International Symposium on Computer Architecture},
   pages = {77-88},
   title = {Technology-driven, highly-scalable dragonfly topology},
   url = {https://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/34926.pdf},
   year = {2008},
}
@thesis{Martinasso2007,
   author = {Maxime Martinasso},
   title = {Analyse et modélisation des communications concurrentes dans les réseaux haute performance},
   url = {http://tel.archives-ouvertes.fr/tel-00165164/},
   year = {2007},
}
@generic{Cornebize2017a,
   author = {Tom Cornebize and Franz Heinrich and Arnaud Legrand and Jérôme Vienne},
   doi = {10.1007/s005350300016},
   issn = {09441174},
   title = {Emulating High Performance Linpack on a Commodity Server at the Scale of a Supercomputer},
   url = {https://hal.inria.fr/hal-01654804/document},
   year = {2017},
}
@article{Casanova2008,
   abstract = {Distributed computing is a very broad and active research area comprising elds such as cluster computing, computational grids, desktop grids and peer-to-peer (P2P) systems. Unfortunately, it is often impossible to obtain theoretical or analytical results to compare the performance of algorithms targeting such systems. One possibility is to conduct large numbers of back-to-back experiments on real platforms. While this is possible on tightly-coupled platforms, it is infeasible on modern distributed platforms as experiments are labor-intensive and results typically not reproducible. Consequently, one must resort to simulations, which enable reproducible results and also make it possible to explore wide ranges of platform and application scenarios. In this paper we describe the SIMGrid framework, a simulation-based framework for evaluating cluster, grid and P2P algorithms and heuristics. This paper focuses on SIMGrid v3, which greatly improves on previous versions thanks to a novel and validated modular simulation engine that achieves higher simulation speed without hindering simulation accuracy. Also, two new user interfaces were added to broaden the targeted research community. After surveying existing tools and methodologies we describe the key features and bene ts of SIMGrid. © 2008 IEEE.},
   author = {Henri Casanova and Arnaud Legrand and Martin Quinson},
   doi = {10.1109/UKSIM.2008.28},
   isbn = {0769531148},
   journal = {Proceedings - UKSim 10th International Conference on Computer Modelling and Simulation, EUROSIM/UKSim2008},
   pages = {126-131},
   title = {SimGrid: A generic framework for large-scale distributed experiments},
   year = {2008},
}
@article{Zheng2011,
   abstract = {Conventional implementations of the MPI standard tend to associate one MPI process per processor, which limits their support for modern multi-core platforms. An increasingly popular approach is to combine MPI with threads where MPI "processes" are light-weight threads. Global variables in legacy MPI applications, however, present a challenge because they may be accessed by multiple MPI threads simultaneously. Thus, transforming legacy MPI applications to become thread-safe in such MPI execution environments requires proper handling of global variables. In this paper, we present three approaches to automatically eliminate global variables to ensure thread-safety for an MPI program. These approaches include: (a) a compilerbased refactoring technique, using a Photran-based tool as an example, which automates the source-to-source transformation for programs written in Fortran; (b) a technique based on a global offset table (GOT); and (c) a technique based on thread local storage (TLS). The second and third methods automatically detect global variables and privatize them for each thread at runtime. We discuss the advantages and disadvantages of these approaches and compare their performance using both synthetic benchmarks, such as the NAS Benchmarks, and a real scientific application, the FLASH code. © 2011 IEEE.},
   author = {Gengbin Zheng and Stas Negara and Celso L. Mendes and Laxmikant V. Kal and Eduardo R. Rodrigues},
   doi = {10.1109/ICPADS.2011.33},
   isbn = {9780769545769},
   issn = {15219097},
   journal = {Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS},
   pages = {220-227},
   title = {Automatic handling of global variables for multi-threaded MPI programs},
   year = {2011},
}
@generic{Leiserson1985,
   author = {Charles E. Leiserson},
   issn = {0018-9340},
   journal = {IEEE Transactions on Computers},
   pages = {892–901},
   title = {Fat-Trees Universal Networks for Hardware-Efficient Supercomputing},
   volume = {34},
   url = {http://www.csa.com/partners/viewrecord.php?requester=gs&amp;collection=TRD&amp;recid=A8612029AH},
   year = {1985},
}
@article{Kim2007,
   abstract = {Increasing integrated-circuit pin bandwidth has motivateda corresponding increase in the degree or radix of interconnection networksand their routers. This paper introduces the flattened butterfly, a cost-efficient topology for high-radix networks. On benign (load-balanced) traffic, the flattened butterfly approaches the cost/performance of a butterfly network and has roughly half the cost of a comparable performance Clos network.The advantage over the Clos is achieved by eliminating redundant hopswhen they are not needed for load balance. On adversarial traffic, the flattened butterfly matches the cost/performance of a folded-Clos network and provides an order of magnitude better performance than a conventional butterfly.In this case, global adaptive routing is used to switchthe flattened butterfly from minimal to non-minimal routing - usingredundant hops only when they are needed. Minimal and non-minimal, oblivious and adaptive routing algorithms are evaluated on the flattened butterfly.We show that load-balancing adversarial traffic requires non-minimalglobally-adaptive routing and show that sequential allocators are required to avoid transient load imbalance when using adaptive routing algorithms.We also compare the cost of the flattened butterfly to folded-Clos, hypercube,and butterfly networks with identical capacityand show that the flattened butterfly is more cost-efficient thanfolded-Clos and hypercube topologies.},
   author = {John Kim and William J. Dally and Dennis Abts},
   doi = {10.1145/1273440.1250679},
   isbn = {9781595937063},
   issn = {01635964},
   issue = {2},
   journal = {ACM SIGARCH Computer Architecture News},
   keywords = {cost model,flat- tened butterfly,global adaptive routing,high-radix routers,interconnection networks,topology},
   pages = {126},
   title = {Flattened Butterfly : A Cost-Efficient Topology for High-Radix Networks},
   volume = {35},
   url = {http://cva.stanford.edu/publications/2007/ISCA_FBFLY.pdf},
   year = {2007},
}
@generic{Closs1952,
   abstract = {This paper describes a method of design arrays of crosspoints for use in telephone switching systems in which it will always be possilbe to establish a connection from an idle inlet to an idle outlet regardless of the number of calls served by the system.},
   author = {C Closs},
   journal = {Bell System Technical Journal},
   keywords = {Link system},
   pages = {341},
   title = {A Study of Non-Blocking Rearrangeable Switching Networks},
   year = {1952},
}
@article{Oladipo2018,
   abstract = {In this paper, we present a newly developed simulator based on the Omnet++ framework, for simulating Dense Wavelength Division Multiplexed, optical burst switched networks in the presence of optical impairments. The simulator is validated against a reduced link load model for optical burst switched networks, and simulations are performed in order to determine the effects of flexi-grid and impairments modelling on simulation results. We find that flexi-grid substantially reduces the burst loss probability on the simulated network, and that impairments modelling has a noticeable effect on simulation results, particularly in flexi-grid scenarios.},
   author = {Joshua Oladipo and Mathys C. Duplessis and Timothy B. Gibbon},
   doi = {10.23919/PEMWN.2017.8308024},
   isbn = {9783901882968},
   journal = {PEMWN 2017 - 6th IFIP International Conference on Performance Evaluation and Modeling in Wired and Wireless Networks},
   keywords = {Dense Wavelength Division Multiplexing,Flexi-grid,Network Simulation,Omnet++,Optical Burst Switching,Optical Impairments},
   pages = {1-6},
   title = {Implementation and validation of an Omnet++ optical burst switching simulator},
   volume = {2018-Janua},
   year = {2018},
}
@article{Zahavi2010,
   author = {Eitan Zahavi},
   issue = {September},
   journal = {Technical Report},
   pages = {1-7},
   title = {D-Mod-K routing providing non-blocking traffic for shift permutations on real life fat trees},
   url = {http://webee.eedev.technion.ac.il/wp-content/uploads/2014/08/publication_574.pdf https://web.archive.org/web/20131008172038/http://www.technion.ac.il/~ezahavi/papers/d_mod_k_routing_is_non_blocking_for_rlft_shifts_final.pdf},
   year = {2010},
}
@article{Varga2008,
   author = {András Varga and Rudolf Hornig},
   isbn = {9789639799202},
   keywords = {computer systems,discrete simulation,hierarchical,integrated development environment,network simulation,performance analysis,simulation tools,telecommunications},
   title = {An overview of the OMNeT++ simulation environment},
   url = {https://doc.omnetpp.org/workshop2008/omnetpp40-paper.pdf},
   year = {2008},
}
@article{Kielmann2000,
   abstract = {Performance modeling is important for implementing efficient parallel applications and runtime systems. The LogP model captures the relevant aspects of message passing in distributed-memory architectures. In this paper we describe an efficient method that measures LogP parameters for a given message passing platform. Measurements are performed for messages of different sizes, as covered by the parameterized LogP model, a slight extension of LogP and LogGP. To minimize both intrusiveness and completion time of the measurement, we propose a procedure that sends as few messages as possible. An implementation of this procedure, called the MPI LogP benchmark, is available from our WWW site. © 2000 Springer-Verlag Berlin Heidelberg.},
   author = {Thilo Kielmann and Henri E. Bal and Kees Verstoep},
   doi = {10.1007/3-540-45591-4_162},
   isbn = {354067442X},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {1176-1183},
   title = {Fast measurement of LogP parameters for message passing platforms},
   volume = {1800 LNCS},
   year = {2000},
}
@article{Geier2013,
   author = {Maximiliano Geier and Lucas Nussbaum and Martin Quinson},
   title = {On the Convergence of Experimental Methodologies for Distributed Systems : Where do we stand ?},
   year = {2013},
}
@article{Knupfer2012,
   abstract = {This paper gives an overview about the Score-P performance measurement infrastructure which is being jointly developed by leading HPC performance tools groups. It motivates the advantages of the joint undertaking from both the developer and the user perspectives, and presents the design and components of the newly developed Score-P performance measurement infrastructure. Furthermore, it contains first evaluation results in comparison with existing performance tools and presents an outlook to the long-term cooperative development of the new system. © Springer-Verlag Berlin Heidelberg 2012.},
   author = {Andreas Knüpfer and Christian Rössel and Dieter An Mey and Scott Biersdorff and Kai Diethelm and Dominic Eschweiler and Markus Geimer and Michael Gerndt and Daniel Lorenz and Allen Malony and Wolfgang E. Nagel and Yury Oleynik and Peter Philippen and Pavel Saviankou and Dirk Schmidl and Sameer Shende and Ronny Tschüter and Michael Wagner and Bert Wesarg and Felix Wolf},
   doi = {10.1007/978-3-642-31476-6_7},
   isbn = {9783642314759},
   journal = {Proceedings of the 5th International Workshop on Parallel Tools for High Performance Computing 2011},
   pages = {79-91},
   title = {Score-P – A joint performance measurement run-time infrastructure for periscope, Scalasca, TAU, and Vampir},
   url = {https://www.vi-hps.org/cms/upload/packages/scorep/Score-P_-_A_Joint_Performance_Measurement_Run-Time_Infrastructure.pdf},
   year = {2012},
}
@article{Muller2008,
   abstract = {This paper presents some scalability studies of the performance analysis tools Vampir and VampirTrace. The usability is analyzed with data collected from real applications, i.e. the thirteen applications contained in the SPEC MPI 1.0 benchmark suite. The analysis covers all phases of performance analysis: instrumenting the application, collecting the performance data, and finally viewing and analyzing the data. The aspects examined include instrumenting effort, monitoring overhead, trace file sizes, load time and response time during analysis. © 2008 The authors and IOS Press. All rights reserved.},
   author = {Matthias S. Müller and Andreas Knüpfer and Matthias Jurenz and Matthias Lieber and Holger Brunst and Hartmut Mix and Wolfgang E. Nagel},
   isbn = {9781586037963},
   issn = {09275452},
   journal = {Advances in Parallel Computing},
   pages = {637-644},
   title = {Developing scalable applications with vampir, vampirserver and vampirtrace},
   volume = {15},
   year = {2008},
}
@article{Knupfer2008,
   abstract = {This paper presents the Vampir tool-set for performance analysis of parallel applications. It consists of the run-time measurement system VampirTrace and the visualization tools Vampir and VampirServer. It describes the major features and outlines the underlying implementation that is necessary to provide low overhead and good scalability. Furthermore, it gives a short overview about the development history and future work as well as related work.},
   author = {Andreas Knüpfer and Holger Brunst and Jens Doleschal and Matthias Jurenz and Matthias Lieber and Holger Mickler and Matthias S. Müller and Wolfgang E. Nagel},
   doi = {10.1007/978-3-540-68564-7},
   isbn = {9783540685616},
   journal = {Proceedings of the 2nd International Workshop on Parallel Tools for High Performance Computing},
   pages = {139-155},
   title = {The Vampir Performance analysis tool-set},
   year = {2008},
}
@article{Girona2000,
   abstract = {This paper presents an extension of Dimemas to enable accurate performance prediction of message passing applications with collective communication primitives. The main contribution is a simple model for collective communication operations that can be user-parameterized. The experiments performed with a set of MPI benchmarks demonstrate the utility of the model.},
   author = {Sergi Girona and Jesús Labarta and Rosa M. Badia},
   doi = {10.1007/3-540-45255-9_9},
   isbn = {3540410104},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {39-46},
   title = {Validation of dimemas communication model for MPI collective operations},
   volume = {1908},
   year = {2000},
}
@article{Vigneras2016,
   abstract = {BXI, Bull eXascale Interconnect, is the new interconnection network developed by Atos for high-performance computing. It has been designed to meet the requirements of exascale supercomputers. At such scale, faults have to be expected and dealt with transparently so that applications remain unaffected by them. BXI features various mechanisms for this purpose, one of which is based on a clear separation between two modes of routing tables computation: offline mode used during bring-up and online mode used to deal with link failures and recoveries. This new architecture is presented along with several offline and online routing algorithms and their actual performance: the full routing tables for a 64k-node fat-tree can be computed in a few minutes in offline mode; and the online mode can withstand numerous inter-router link failures without any noticeable impact on running applications.},
   author = {Pierre Vignéras and Jean Noël Quintin},
   doi = {10.1007/s11227-016-1755-2},
   issn = {15730484},
   issue = {12},
   journal = {Journal of Supercomputing},
   keywords = {BXI,Fabric management,Fault-tolerant routing,High-performance computing,Interconnect management,Routing},
   pages = {4418-4437},
   title = {The BXI routing architecture for exascale supercomputer},
   volume = {72},
   url = {https://www.researchgate.net/profile/Pierre_Vigneras/publication/303502864_The_BXI_routing_architecture_for_exascale_supercomputer/links/5ad9ffd3458515c60f5ac594/The-BXI-routing-architecture-for-exascale-supercomputer.pdf},
   year = {2016},
}
@article{Domke2017,
   abstract = {The interconnection network has a large influence on total cost, application performance, energy consumption, and overall system efficiency of a supercomputer. Unfortunately, today's routing algorithms do not utilize this important resource most efficiently. We first demonstrate this by defining the dark fiber metric as a measure of unused resource in networks. To improve the utilization, we propose scheduling-aware routing, a new technique that uses the current state of the batch system to determine a new set of network routes and so increases overall system utilization by up to 17.74%. We also show that our proposed routing increases the throughput of communication benchmarks by up to 17.6% on a practical InfiniBand installation. Our routing method is implemented in the standard InfiniBand tool set and can immediately be used to optimize systems. In fact, we are using it to improve the utilization of our production petascale supercomputer for more than one year.},
   author = {Jens Domke and Torsten Hoefler},
   doi = {10.1109/SC.2016.12},
   isbn = {9781467388153},
   issn = {21674337},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   keywords = {Computer network management,High performance computing,Routing protocols,Unicast},
   pages = {142-153},
   title = {Scheduling-Aware Routing for Supercomputers},
   volume = {0},
   year = {2017},
}
@article{Jain2017,
   abstract = {The fat-tree topology is one of the most commonly used network topologies in HPC systems. Vendors support several options that can be configured when deploying fat-tree networks on production systems, such as link bandwidth, number of rails, number of planes, and tapering. This paper showcases the use of simulations to compare the impact of these design options on representative production HPC applications, libraries, and multi-job workloads. We present advances in the TraceR-CODES simulation framework that enable this analysis and evaluate its prediction accuracy against experiments on a production fat-tree network. In order to understand the impact of different network configurations on various anticipated scenarios, we study workloads with different communication patterns, computation-to-communication ratios, and scaling characteristics. Using multi-job workloads, we also study the impact of inter-job interference on performance and compare the cost-performance tradeoffs.},
   author = {Nikhil Jain and Abhinav Bhatele and Louis H. Howell and David Böhme and Ian Karlin and Edgar A. León and Misbah Mubarak and Noah Wolfe and Todd Gamblin and Matthew L. Leininger},
   doi = {10.1145/3126908.3126967},
   isbn = {9781450351140},
   journal = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2017},
   keywords = {Fat-tree topology,Network simulation,Performance prediction,Procurement},
   title = {Predicting the performance impact of different fat-tree configurations},
   year = {2017},
}
@article{Jyothi2017,
   abstract = {High throughput is of particular interest in data center and HPC networks. Although myriad network topologies have been proposed, a broad head-to-head comparison across topologies and across traffic patterns is absent, and the right way to compare worst-case throughput performance is a subtle problem. In this paper, we develop a framework to benchmark the throughput of network topologies, using a two-pronged approach. First, we study performance on a variety of synthetic and experimentally-measured traffic matrices (TMs). Second, we show how to measure worst-case throughput by generating a near-worst-case TM for any given topology. We apply the framework to study the performance of these TMs in a wide range of network topologies, revealing insights into the performance of topologies with scaling, robustness of performance across TMs, and the effect of scattered workload placement. Our evaluation code is freely available.},
   author = {Sangeetha Abdu Jyothi and Ankit Singla and P. Brighten Godfrey and Alexandra Kolla},
   doi = {10.1109/SC.2016.64},
   isbn = {9781467388153},
   issn = {21674337},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   pages = {761-772},
   title = {Measuring and Understanding Throughput of Network Topologies},
   url = {https://people.inf.ethz.ch/asingla/papers/sc16.pdf},
   year = {2017},
}
@article{Papadopoulou2017,
   abstract = {In this paper, we present a methodology for predictive modeling of communication of HPC applications. Communication time depends on a complex set of parameters, relevant to the application, the system architecture, the runtime configuration and runtime conditions. To handle this complexity, we define features that can be extracted from the application, the process mapping and the allocation shape ahead of execution, deploy a single benchmark to sweep over the parameter space and develop predictive models for communication time on two supercomputers, Vilje and Piz Daint, using different subsets of our features, machine-learning methods and training sets. We compare the predictive power of our models on two common communication patterns and one application, for various problem sizes, executions and runtime configurations, ranging from a few dozen to a few thousand cores. Our methodology is successful across all tested communication patterns on both systems and exhibits high prediction accuracy and goodness-of-fit, scoring 23.98% in MMRE, 0.942 in RCC and 61.43% in Pred0.25 on Vilje and 21.31%, 0.940 and 66.57% respectively on Piz Daint, with models that are applicable just-in-time ahead of the execution of an HPC application.},
   author = {Nikela Papadopoulou and Georgios Goumas and Nectarios Koziris},
   doi = {10.1007/s10586-017-0821-8},
   issn = {15737543},
   issue = {3},
   journal = {Cluster Computing},
   keywords = {Communication time,MPI applications,Machine learning,Predictive modeling,Supercomputers},
   pages = {2725-2747},
   publisher = {Springer US},
   title = {Predictive communication modeling for HPC applications},
   volume = {20},
   year = {2017},
}
@inproceedings{Schonbein2019,
   abstract = {Current proposals for in-network data processing operate on data as it streams through a network switch or endpoint. Since compute resources must be available when data arrives, these approaches provide deadline-based models ofexecution. This paper introduces a deadline-free general compute model for network endpoints called INCA: In-Network Compute Assistance. INCA builds upon contem- porary NIC offload capabilities to provide on-NIC, deadline-free, general-purpose compute capacities that can be utilized when the network is inactive. We demonstrate INCA is Turing complete, and provide a detailed design for extending existing hardware to sup- port this model. We evaluate runtimes for a selection of kernels, including several optimizations, and showINCA can provide up to a 11% speedup for applications with minimal code modifications and between 25% to 37% when applications are optimized for INCA.},
   author = {Whit Schonbein and Ryan E Grant and Matthew G F Dosanjh and Dorian Arnold},
   city = {New York, NY, USA},
   doi = {10.1145/3295500.3356153},
   isbn = {9781450362290},
   journal = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
   month = {11},
   pages = {1-13},
   publisher = {ACM},
   title = {INCA},
   url = {https://dl.acm.org/doi/10.1145/3295500.3356153},
   year = {2019},
}
@article{Riley2010,
   author = {George F. Riley and Thomas R. Henderson},
   journal = {Modeling and Tools for Network Simulation},
   pages = {15-34},
   title = {The ns–3 Network Simulator},
   year = {2010},
}
@article{Velho2009a,
   abstract = {Distributed computing is a very broad and active research area comprising fields such as cluster computing, computational grids, desktop grids and peer-to-peer (P2P) systems. Studies in this area generally resort to simulations, which enable reproducible results and make it possible to explore wide ranges of platform and application scenarios. In this context, network simulation is certainly the most critical part. Many packet-level network simulators are available and enable high-accuracy simulation but they lead to prohibitively long simulation times. Therefore, many simulation frameworks have been developed that simulate networks at higher levels, thus enabling fast simulation but losing accuracy. One such framework, SimGrid, uses a flow-level approach that approximates the behavior of TCP networks, including TCP's bandwidth sharing properties. A preliminary study of the accuracy loss by comparing it to popular packet-level simulators has been proposed in [11]and in which regimes in which SimGrid's accuracy is comparable to that of these packet-level simulators are identified. In this article we come back on this study, reproduce these experiments and provide a deeper analysis that enables us to greatly improve SimGrid's range of validity.},
   author = {Pedro Velho and Arnaud Legrand},
   doi = {10.4108/ICST.SIMUTOOLS2009.5592},
   isbn = {9789639799455},
   journal = {SIMUTools 2009 - 2nd International ICST Conference on Simulation Tools and Techniques},
   title = {Accuracy study and improvement of network simulation in the SimGrid framework},
   url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.566.286&rep=rep1&type=pdf},
   year = {2009},
}
@article{Jha2019,
   abstract = {Network congestion in high-speed interconnects is a major source of application run time performance variation. Recent years have witnessed a surge of interest from both academia and industry in the development of novel approaches for congestion control at the network level and in application placement, mapping, and scheduling at the system-level. However, these studies are based on proxy applications and benchmarks that are not representative of field-congestion characteristics of high-speed interconnects. To address this gap, we present (a) an end-to-end framework for monitoring and analysis to support long-term field-congestion characterization studies, and (b) an empirical study of network congestion in petascale systems across two different interconnect technologies: (i) Cray Gemini, which uses a 3-D torus topology, and (ii) Cray Aries, which uses the DragonFly topology.},
   author = {Saurabh Jha and Archit Patke and Jim Brandt and Ann Gentile and Mike Showerman and Eric Roman and Zbigniew T. Kalbarczyk and William T. Kramer and Ravishankar K. Iyer},
   title = {A Study of Network Congestion in Two Supercomputing High-Speed Interconnects},
   url = {http://arxiv.org/abs/1907.05312},
   year = {2019},
}
@article{Jha,
   abstract = {While it is widely acknowledged that network congestion in High Performance Computing (HPC) systems can significantly degrade application performance, there has been little to no quantification of congestion on credit-based interconnect networks. We present a methodology for detecting, extracting , and characterizing regions of congestion in networks. We have implemented the methodology in a deployable tool, Monet, which can provide such analysis and feedback at run-time. Using Monet, we characterize and diagnose congestion in the world's largest 3D torus network of Blue Waters, a 13.3-petaflop supercomputer at the National Center for Supercom-puting Applications. Our study deepens the understanding of production congestion at a scale that has never been evaluated before.},
   author = {Saurabh Jha and Archit Patke and Jim Brandt and Ann Gentile and Benjamin Lim and Mike Showerman and Greg Bauer and Larry Kaplan and Zbigniew Kalbarczyk and William Kramer and Ravi Iyer},
   title = {Measuring Congestion in High-Performance Datacenter Interconnects},
   url = {https://saurabhjha1.github.io/pubs/congestion_embed.pdf},
}
@article{Quetier2005,
   author = {Benjamin Quetier and Franck Cappello},
   journal = {IMACS World Congress},
   title = {A survey of Grid research tools: simulators, emulators and real life platforms},
   url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:A+survey+of+Grid+research+tools+:+simulators+,+emulators+and+real+life+platforms#0},
   year = {2005},
}
@patent{Cady2017,
   author = {Alain Cady},
   title = {FR3076142_A1-1__routing.pdf},
   year = {2017},
}
@patent{Petit2018,
   author = {Enguerrand Petit and Cyril Mazauric and Xavier Vigouroux},
   title = {FR_3078182_A1-1__MPI_tracing.pdf},
   year = {2018},
}
@article{Gliksberg2018,
   abstract = {High-Performance Computing (HPC) clusters are made up of a variety of node types (usually compute, I/O, service, and GPGPU nodes) and applications don't use nodes of a different type the same way. Resulting communication patterns reflect organization of groups of nodes, and current optimal routing algorithms for all-To-All patterns will not always maximize performance for group-specific communications. Since application communication patterns are rarely available beforehand, we choose to rely on node types as a good guess for node usage. We provide a description of node type heterogeneity and analyse performance degradation caused by unlucky repartition of nodes of the same type. We provide an extension to routing algorithms for Parallel Generalized Fat-Tree topologies (PGFTs) which balances load amongst groups of nodes of the same type. We show how it removes these performance issues by comparing results in a variety of situations against corresponding classical algorithms.},
   author = {John Gliksberg and Jean Noël Quintin and Pedro Javier Garćia},
   doi = {10.1109/HiPINEB.2018.00010},
   isbn = {0769563732},
   journal = {Proceedings - 2018 IEEE 4th International Workshop on High-Performance Interconnection Networks in the Exascale and Big-Data Era, HiPINEB 2018},
   keywords = {Dmodk,Fat-Tree,Heterogeneity,Hpc,Pgft,Routing,Smodk},
   pages = {9-15},
   title = {Node-Type-based load-balancing routing for parallel generalized fat-Trees},
   volume = {2018-Janua},
   year = {2018},
}
@patent{Quintin2018,
   author = {Jean Noël Quintin and John Gliksberg},
   title = {FR_3078220_A1-7__routing.pdf},
   year = {2018},
}
@article{Casanova2014a,
   abstract = {The study of parallel and distributed applications and platforms, whether in the cluster, grid, peer-to-peer, volunteer, or cloud computing domain, often mandates empirical evaluation of proposed algorithmic and system solutions via simulation. Unlike direct experimentation via an application deployment on a real-world testbed, simulation enables fully repeatable and configurable experiments for arbitrary hypothetical scenarios. Two key concerns are accuracy (so that simulation results are scientifically sound) and scalability (so that simulation experiments can be fast and memory-efficient). While the scalability of a simulator is easily measured, the accuracy of many state-of-the-art simulators is largely unknown because they have not been sufficiently validated. In this work we describe recent accuracy and scalability advances made in the context of the SimGrid simulation framework. A design goal of SimGrid is that it should be versatile, i.e., applicable across all aforementioned domains. We present quantitative results that show that SimGrid compares favorably with state-of-the-art domain-specific simulators in terms of scalability, accuracy, or the trade-off between the two. An important implication is that, contrary to popular wisdom, striving for versatility in a simulator is not an impediment but instead is conducive to improving both accuracy and scalability. © 2014 Elsevier Inc. All rights reserved.},
   author = {Henri Casanova and Arnaud Giersch and Arnaud Legrand and Martin Quinson and Frédéric Suter},
   doi = {10.1016/j.jpdc.2014.06.008},
   issn = {07437315},
   issue = {10},
   journal = {Journal of Parallel and Distributed Computing},
   keywords = {Scalability,SimGrid,Simulation,Validation,Versatility},
   pages = {2899-2917},
   title = {Versatile, scalable, and accurate simulation of distributed applications and platforms},
   volume = {74},
   url = {https://hal.inria.fr/hal-01017319v2/document},
   year = {2014},
}
@article{Velho2013a,
   abstract = {Researchers in the area of grid/cloud computing perform many of their experiments using simulations that must capture network behavior. In this context, packet-level simulations, which are widely used to study network protocols, are too costly given the typical large scales of simulated systems and applications. An alternative is to implement network simulations with less costly flow-level models. Several flow-level models have been proposed and implemented in grid/cloud simulators. Surprisingly, published validations of these models, if any, consist of verifications for only a few simple cases. Consequently, even when they have been used to obtain published results, the ability of these simulators to produce scientifically meaningful results is in doubt. This work evaluates these state-of-the-art flow-level network models of TCP communication via comparison to packet-level simulation. While it is straightforward to show cases in which previously proposed models lead to good results, instead we follow the critical method, which places model refutation at the center of the scientific activity, and we systematically seek cases that lead to invalid results. Careful analysis of these cases reveals fundamental flaws and also suggests improvements. One contribution of this work is that these improvements lead to a new model that, while far from being perfect, improves upon all previously proposed models in the context of simulation of grids or clouds. A more important contribution, perhaps, is provided by the pitfalls and unexpected behaviors encountered in this work, leading to a number of enlightening lessons. In particular, this work shows that model validation cannot be achieved solely by exhibiting (possibly many) “good cases.” Confidence in the quality of a model can only be strengthened through an invalidation approach that attempts to prove the model wrong. © 2013, ACM. All rights reserved.},
   author = {Pedro Velho and Lucas Mello Schnorr and Henri Casanova and Arnaud Legrand},
   doi = {10.1145/2517448},
   issn = {15581195},
   issue = {4},
   journal = {ACM Transactions on Modeling and Computer Simulation},
   keywords = {Experimentation,Grid and cloud computing simulation,SimGrid,scalability,validation},
   pages = {1-26},
   title = {On the Validity of Flow-level TCP Network Models for Grid and Cloud Simulations},
   volume = {23},
   url = {https://hal.inria.fr/hal-00872476/file/tomacs.pdf},
   year = {2013},
}
@article{Fujiwara2012a,
   author = {Kayo Fujiwara and Henri Casanova},
   doi = {10.4108/nstools.2007.2010},
   isbn = {9789639799004},
   title = {Speed and Accuracy of Network Simulation in the SimGrid Framework},
   url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.94.6192&rep=rep1&type=pdf},
   year = {2012},
}
@inproceedings{Clauss2011a,
   abstract = {Simulation is a popular approach for predicting the performance of MPI applications for platforms that are not at one's disposal. It is also a way to teach the principles of parallel programming and high-performance computing to students without access to a parallel computer. In this work we present SMPI, a simulator for MPI applications that uses on-line simulation, i.e., the application is executed but part of the execution takes place within a simulation component. SMPI simulations account for network contention in a fast and scalable manner. SMPI also implements an original and validated piece-wise linear model for data transfer times between cluster nodes. Finally SMPI simulations of large-scale applications on large-scale platforms can be executed on a single node thanks to techniques to reduce the simulation's compute time and memory footprint. These contributions are validated via a large set of experiments in which SMPI is compared to popular MPI implementations with a view to assess its accuracy, scalability, and speed. © 2011 IEEE.},
   author = {Pierre-Nicolas Clauss and Mark Stillwell and Stephane Genaud and Frederic Suter and Henri Casanova and Martin Quinson},
   doi = {10.1109/IPDPS.2011.69},
   isbn = {978-1-61284-372-8},
   journal = {2011 IEEE International Parallel & Distributed Processing Symposium},
   keywords = {Message Passing Interface,On-line simulation,Performance prediction},
   month = {5},
   pages = {664-675},
   publisher = {IEEE},
   title = {Single Node On-Line Simulation of MPI Applications with SMPI},
   url = {http://ieeexplore.ieee.org/document/6012878/},
   year = {2011},
}
@article{Penaranda2012,
   abstract = {In large supercomputers the topology of the interconnection network is a key design issue that impacts the performance and cost of the whole system. Direct topologies provide a reduced hardware cost, but as the number of dimensions is conditioned by 3D wiring restrictions, a high number of nodes per dimension is used, which increases communication latency and reduces network throughput. On the other hand, indirect topologies can provide better performance for large network sizes, but at the cost of a high amount of switches and links. In this paper we propose a new family of topologies that combines the best features of both direct and indirect topologies to efficiently connect an extremely high number of nodes. In particular, we propose a $n$-dimensional topology where the nodes of each dimension are connected through a small indirect topology. This combination results in a family of topologies that provides high performance, with latency and throughput figures of merit close to indirect topologies, but with a lower hardware cost. In particular, it is able to double the throughput obtained per switching element of indirect topologies. Moreover, the layout of the topology is much simpler than in indirect topologies. Indeed, its fault-tolerance degree is equal or higher than the one for direct and indirect topologies. © 2012 IEEE.},
   author = {Roberto Peñaranda and Crispín Gómez and María Engracia Gómez and Pedro López and Jose Duato},
   doi = {10.1109/NCA.2012.22},
   isbn = {9780769547732},
   journal = {Proceedings - IEEE 11th International Symposium on Network Computing and Applications, NCA 2012},
   keywords = {Interconnection networks,supercomputers,topology},
   pages = {220-227},
   title = {A new family of hybrid topologies for large-scale interconnection networks},
   year = {2012},
}
@article{Guay2014,
   author = {Wei Lin Guay},
   title = {Dynamic Reconfiguration in Interconnection Networks},
   year = {2014},
}
@report{Brightwell2022,
   author = {Ronald Brightwell and William (Whit) Schonbein and Kevin Pedretti and Karl Hemmert and Arthur Maccabe and Ryan Grant and Brian Barrett and Keith Underwood and Rolf Riesen and Torsten Hoefler and Mathieu Barbe and Luiz Suraty Filho and Alexandre Ratchov},
   city = {Albuquerque, NM, and Livermore, CA (United States)},
   doi = {10.2172/1875218},
   institution = {Sandia National Laboratories (SNL)},
   keywords = {Portals Sandia network programming API},
   month = {6},
   title = {The Portals 4.3 Network Programming Interface},
   url = {https://www.osti.gov/servlets/purl/1875218/},
   year = {2022},
}
@thesis{Miura2010,
   abstract = {We introduce three adaptive routing algorithms for the hierarchical interconnection network TESH (tori-connected meshes) and evaluate their hardware costs and delays. TESH, which consists of a hierarchical torus interconnection between meshes as a basic module, is one of the k-ary n-cube networks, for which many adaptive routing algorithms have already been proposed. Adaptive routings for TESH have also been proposed in previous work. © 2010 IEEE.},
   author = {Yasuyuki Miura and Masahiro Kaneko and Shigeyoshi Watanabe},
   doi = {10.1109/LCN.2010.5735729},
   isbn = {9781424483877},
   journal = {Proceedings - Conference on Local Computer Networks, LCN},
   pages = {308-311},
   title = {Adaptive routing algorithms and implementation for interconnection network TESH for parallel processing},
   year = {2010},
}
@article{Schneider2013,
   abstract = {With each successive generation, network adapters for high-performance networks are becoming more powerful and feature rich. High-performance NICs can now provide support for performing complex group communication operations on the NIC without any host CPU involvement. Several "offloading interfaces" have been designed with the collective communications goal being the complete offloading of arbitrary communication patterns. In this work, we analyze the offloading model offered in the Portals 4 specification in detail. We perform a theoretical analysis based on abstract communication graphs and show several protocols for implementing offloaded communication schedules. Based on our analysis, we propose and implement an extension to the Portals 4 specification that enables offloading any communication pattern completely to the NIC. Our measurements with several advanced communication algorithms confirm that the enhancements provide good overlap and asynchronous progress in practical settings. Altogether, we demonstrate a complete and simple scheme for implementing arbitrary offloaded communication algorithms and hardware. Our protocols can act as a blueprint for the development of communication hardware and middleware while optimizing the whole communication stack. © 2013 IEEE.},
   author = {Timo Schneider and Torsten Hoefler and Ryan E. Grant and Brian W. Barrett and Ron Brightwell},
   doi = {10.1109/ICPP.2013.73},
   isbn = {9780769551173},
   issn = {01903918},
   journal = {Proceedings of the International Conference on Parallel Processing},
   pages = {593-602},
   title = {Protocols for fully offloaded collective operations on accelerated network adapters},
   url = {http://ww.unixer.de/publications/img/schneider-portalsoffload.pdf},
   year = {2013},
}
@article{Bedaride2013a,
   author = {Paul Bedaride and Augustin Degomme and Arnaud Legrand and George Markomanolis and Martin Quinson and Mark Stillwell and Paul Bedaride and Augustin Degomme and Arnaud Legrand and George Markomanolis},
   title = {Toward Better Simulation of MPI Applications on Ethernet / TCP Networks},
   url = {https://hal.inria.fr/hal-00919507/document},
   year = {2013},
}
@article{Inozemtsev2014,
   author = {Grigori Inozemtsev},
   issue = {February},
   title = {Overlapping Computation and Communication through Offloading in MPI over InfiniBand},
   year = {2014},
}
@inproceedings{Maglione-Mathey2016,
   abstract = {The design of interconnection networks is becoming extremely important for High-Performance Computing (HPC) systems in the Exascale Era. Design decisions like the selection of the network topology, routing algorithm, fault tolerance and/or congestion control are crucial for the network performance. Besides, the interconnection network designers are also focused on creating middleware layers compatible to different network technologies, which make it possible for these technologies to interoperate. One example is the OpenFabrics Software (OFS) used in HPC for breakthrough applications that require high efficiency computing, wire-speed messaging, microsecond latencies and fast I/O for storage and file systems. OFS is compatible with several HPC interconnect technologies, like InfiniBand, iWarp or RoCE. One challenge in the design of new features for improving the interconnection network performance is to model in specific simulation tools the latency introduced by the OFS modules into the network traffic. In this paper, we present a work-in-progress methodology to combine the OFS middleware with OMNeT++-based simulation tools, so that we can use some of the OFS modules, like OpenSM or ibsim, combined with simulation tools. We also propose a set of tools for analyzing the properties of different network topologies. Future work will consist on modeling other OFS modules functionality in network simulators.},
   author = {German Maglione-Mathey and Pedro Yebenes and Jesus Escudero-Sahuquillo and Pedro J. Garcia and Francisco J. Quiles},
   doi = {10.1109/HIPINEB.2016.7},
   isbn = {978-1-5090-2121-5},
   journal = {2016 2nd IEEE International Workshop on High-Performance Interconnection Networks in the Exascale and Big-Data Era (HiPINEB)},
   keywords = {OMNeT++,infiniband,interconnection networks,simulation},
   month = {3},
   pages = {55-58},
   publisher = {IEEE},
   title = {Combining OpenFabrics Software and Simulation Tools for Modeling InfiniBand-Based Interconnection Networks},
   url = {http://ieeexplore.ieee.org/document/7457768/},
   year = {2016},
}
@working_paper{MellanoxTechnologies,
   author = {Mellanox Technologies},
   issue = {2379},
   note = {Interesting white paper because it mentions the E2E mechanism (timeout and retries)},
   title = {QoS Concepts and Requirements QoS Implementation in InfiniBand Architecture},
}
@working_paper{MellanoxTechnologies2003,
   abstract = {Executive Summary InfiniBand is a powerful new architecture designed to support I/O connectivity for the Internet infrastructure. InfiniBand is supported by all the major OEM server vendors as a means to expand beyond and create the next generation I/O interconnect standard in servers. For the first time, a high volume, industry standard I/O interconnect extends the role of traditional "in the box" busses. InfiniBand is unique in providing both, an "in the box" backplane solution, an external interconnect, and "Bandwidth Out of the box", thus it provides connectivity in a way previously reserved only for traditional networking interconnects. This unification of I/O and system area networking requires a new architecture that supports the needs of these two previously separate domains. Underlying this major I/O transition is InfiniBand's ability to support the Internet's requirement for RAS: reliability, availability, and serviceability. This white paper discusses the features and capabilities which demonstrate InfiniBand's superior abilities to support RAS relative to the legacy PCI bus and other proprietary switch fabric and I/O solutions. Further, it provides an overview of how the InfiniBand architecture supports a comprehensive silicon, software, and system solution. The comprehensive nature of the architecture is illustrated by providing an overview of the major sections of the InfiniBand 1.1 specification. The scope of the 1.1 specification ranges from industry standard electrical interfaces and mechanical connectors to well defined software and management interfaces. The paper is divided into four sections. The introduction sets the stage for InfiniBand and illustrates why all the major server vendors have made the decision to embrace this new standard. The next section reviews the effect Infini-Band will have on various markets that are currently being addressed by legacy technologies. The third section provides a comparison between switch fabrics and bus architecture in general and then delves into details comparing InfiniBand to PCI and other proprietary solutions. The final section goes into details about the architecture, reviewing at a high level the most important features of InfiniBand.},
   author = {Mellanox Technologies},
   journal = {Technical Report},
   pages = {1-20},
   title = {Introduction to InfiniBand},
   year = {2003},
}
@book{PaulGrun2010,
   author = {Paul Grun},
   pages = {54},
   title = {Introduction to InfiniBand(TM) for End Users},
   year = {2010},
}
@article{Jhaa,
   abstract = {Availability of the interconnection network in high-performance computing (HPC) systems is fundamental to sustaining the continuous execution of applications at scale. When failures occur, interconnect recovery mechanisms orchestrate complex operations to recover network connectivity between the nodes. As the scale and design complexity of HPC systems increase, so does the system’s susceptibility to failures during execution of interconnect-recovery procedures. This study characterizes the recovery procedures of the Gemini interconnect network, the largest Gemini network built by Cray, on Blue Waters, a 13.3 petaflop supercomputer at the National Center for Supercomputing Applications (NCSA). We propose a propagation model that captures interconnect failures and recovery procedures to help understand types of failures and their propagation in both the system and applications during recovery. The measurements show that recovery procedures occur very frequently and that the unsuccessful execution of recovery procedures when additional failures occur during recovery causes system-wide outages (SWOs, 28 out of 101) and application failures (3.4% of all running applications).},
   author = {Saurabh Jha and Valerio Formicola and Catello Di Martino and Mark Dalton and William T Kramer and Zbigniew Kalbarczyk and Ravishankar K Iyer},
   keywords = {Fault diagnosis,Fault tolerance,Networks,Reliability},
   pages = {1-15},
   title = {Resiliency of HPC Interconnects : A Case Study of Interconnect Failures and Recovery in Blue Waters},
   url = {https://saurabhjha1.github.io/pubs/network-tdsc-2017.pdf},
}
@article{Chunduri2019,
   abstract = {MPI is the most prominent programming model used in scientific computing today. Despite the importance of MPI, however, how scientific computing applications use it in production is not well understood. This lack of understanding is attributed primarily to the fact that production systems are often wary of incorporating automatic profiling tools that perform such analysis because of concerns about potential performance over-heads. In this study, we used a lightweight profiling tool, called Autoperf, to log the MPI usage characteristics of production applications on a large IBM BG/Q supercomputing system (Mira) and its corresponding development system (Cetus). Autoperf limits the amount of information that it records, in order to keep the overhead to a minimum while still storing enough data to derive useful insights. MPI usage statistics have been collected for over 100K jobs that were run within a two-year period and are analyzed in this paper. The analysis is intended to provide useful insights for MPI developers and network hardware developers for their next generation of improvements and for supercomputing center operators for their next system procurements.},
   author = {Sudheer Chunduri and Scott Parker and Pavan Balaji and Kevin Harms and Kalyan Kumaran},
   doi = {10.1109/SC.2018.00033},
   isbn = {9781538683842},
   journal = {Proceedings - International Conference for High Performance Computing, Networking, Storage, and Analysis, SC 2018},
   keywords = {Autoperf,Core-hours,MPI,Monitoring},
   pages = {386-400},
   publisher = {IEEE},
   title = {Characterization of MPI usage on a production supercomputer},
   year = {2019},
}
@article{Vishnu2005,
   abstract = {InfiniBand is becoming increasingly popular in the area of cluster computing due to its open standard and high performance. Fat Tree is a primary interconnection topology for building large scale InfiniBand clusters. Instead of using a shared bus approach, InfiniBand employs an arbitrary switched point-to-point topology. In order to manage the subnet, InfiniBand specifies a basic management infrastructure responsible for discovery, configuration and maintaining the active state of the network. In the literature, simulation studies have been done on irregular topologies to characterize the subnet management mechanism. However, there is no study to model subnet management mechanism on regular topologies using actual implementations. In this paper, we take up the challenge of modeling subnet management mechanism for Fat Tree InfiniBand networks using a popular subnet manager OpenSM. We present the timings for various subnet management phases namely topology discovery, path computation and path distribution for large scale fate tree InfiniBand subnets and present basic performance evaluation on small scale InfiniBand cluster. We verify our model with the basic set of results obtained, and present the results for the model by varying different parameters on Fat Trees.},
   author = {Abhinav Vishnu and Amith R. Mamidala and Hyun Wook Jin and Dhabaleswar K. Panda},
   doi = {10.1109/IPDPS.2005.339},
   isbn = {0769523129},
   journal = {Proceedings - 19th IEEE International Parallel and Distributed Processing Symposium, IPDPS 2005},
   title = {Performance modeling of subnet management on fat tree infiniBand networks using OpenSM},
   volume = {2005},
   year = {2005},
}
@article{Hammond2009,
   abstract = {There are a number of challenges facing the High Performance Computing (HPC) community, including increasing levels of concurrency (threads, cores, nodes), deeper and more complex memory hierarchies (register, cache, disk, network), mixed hardware sets (CPUs and GPUs) and increasing scale (tens or hundreds of thousands of processing elements). Assessing the performance of complex scientific applications on specialised high-performance computing architectures is difficult. In many cases, traditional computer benchmarking is insufficient as it typically requires access to physical machines of equivalent (or similar) specification and rarely relates to the potential capability of an application. A technique known as application performance modelling addresses many of these additional requirements. Modelling allows future architectures and/or applications to be explored in a mathematical or simulated setting, thus enabling hypothetical questions relating to the configuration of a potential future architecture to be assessed in terms of its impact on key scientific codes. This paper describes theWarwick Performance Prediction (WARPP) simulator, which is used to construct application performance models for complex industry-strength parallel scientific codes executing on thousands of processing cores. The capability and accuracy of the simulator is demonstrated through its application to a scientific benchmark developed by the United Kingdom Atomic Weapons Establishment (AWE). The results of the simulations are validated for two different HPC architectures, each case demonstrating a greater than 90% accuracy for run-time prediction. Simulation results, collected from runs on a standard PC, are provided for up to 65,000 processor cores. It is also shown how the addition of operating system jitter to the simulator can improve the quality of the application performance model results.},
   author = {S. D. Hammond and G. R. Mudalige and J. A. Smith and S. A. Jarvis and J. A. Herdman and A. Vadgama},
   doi = {10.4108/ICST.SIMUTOOLS2009.5753},
   isbn = {9789639799455},
   journal = {SIMUTools 2009 - 2nd International ICST Conference on Simulation Tools and Techniques},
   keywords = {Application performance modelling,High Performance Computing,Simulation},
   title = {WARPP - A toolkit for simulating high-performance parallel scientific codes},
   year = {2009},
}
@article{Casanova2019,
   abstract = {Scientific workflows are used routinely in numerous scientific domains, and Workflow Management Systems (WMSs) have been developed to orchestrate and optimize workflow executions on distributed platforms. WMSs are complex software systems that interact with complex software infrastructures. Most WMS research and development activities rely on empirical experiments conducted with full-fledged software stacks on actual hardware platforms. Such experiments, however, are limited to hardware and software infrastructures at hand and can be labor- and/or time-intensive. As a result, relying solely on real-world experiments impedes WMS research and development. An alternative is to conduct experiments in simulation. In this work we present WRENCH, a WMS simulation framework, whose objectives are (i) accurate and scalable simulations; and (ii) easy simulation software development. WRENCH achieves its first objective by building on the SimGrid framework. While SimGrid is recognized for the accuracy and scalability of its simulation models, it only provides low-level simulation abstractions and thus large software development efforts are required when implementing simulators of complex systems. WRENCH thus achieves its second objective by providing high- level and directly re-usable simulation abstractions on top of SimGrid. After describing and giving rationales for WRENCH's software architecture and APIs, we present a case study in which we apply WRENCH to simulate the Pegasus production WMS. We report on ease of implementation, simulation accuracy, and simulation scalability so as to determine to which extent WRENCH achieves its two above objectives. We also draw both qualitative and quantitative comparisons with a previously proposed workflow simulator.},
   author = {Henri Casanova and Suraj Pandey and James Oeth and Ryan Tanaka and Frédéric Suter and Rafael Ferreira Da Silva},
   doi = {10.1109/WORKS.2018.00013},
   isbn = {9781728101965},
   issue = {Ci},
   journal = {Proceedings of WORKS 2018: 13th Workshop on Workflows in Support of Large-Scale Science, Held in conjunction with SC 2018: The International Conference for High Performance Computing, Networking, Storage and Analysis},
   keywords = {Distributed-Computing,Scientific-Workflows,Simulation,Workflow-Management-Systems},
   pages = {74-85},
   title = {WRENCH: A Framework for Simulating Workflow Management Systems},
   url = {https://scitech.isi.edu/wordpress/wp-content/papercite-data/pdf/casanova-works-2018.pdf},
   year = {2019},
}
@article{Jain2016,
   abstract = {This paper presents an evaluation and comparison of three topologies that are popular for building interconnection networks in large-scale supercomputers: Torus, fat-tree, and dragonfly. To perform this evaluation, we propose a comprehensive methodology and present a scalable packet-level network simulator, TraceR. Our methodology includes design of prototype systems that are being evaluated, use of proxy applications to determine computation and communication load, simulating individual proxy applications and multi-job workloads, and computing aggregated performance metrics. Using the proposed methodology, prototype systems based on torus, fat-tree, and dragonfly networks with up to 730K endpoints (MPI processes) executed on 46K nodes are compared in the context of multi-job workloads from capability and capacity systems. For the 180 Petaflop/s prototype systems simulated in this paper, we show that different topologies are superior in different scenarios, i.e. there is no single best topology, and the characteristics of parallel workloads determine the optimal choice.},
   author = {Nikhil Jain and Abhinav Bhatele and Sam White and Todd Gamblin and Laxmikant V. Kale},
   doi = {10.1109/SC.2016.13},
   isbn = {9781467388153},
   issn = {21674337},
   issue = {November},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   keywords = {Computer simulation,High performance computing,Multiprocessor interconnection networks,Network topology,Performance evaluation},
   pages = {154-165},
   title = {Evaluating HPC Networks via Simulation of Parallel Workloads},
   volume = {0},
   url = {http://charm.cs.illinois.edu/newPapers/16-11/network_comparison_tracer.pdf},
   year = {2016},
}
@article{Mubarak2017,
   abstract = {With the increasing complexity of today's high-performance computing (HPC) architectures, simulation has become an indispensable tool for exploring the design space of HPC systems-in particular, networks. In order to make effective design decisions, simulations of these systems must possess the following properties: (1) have high accuracy and fidelity, (2) produce results in a timely manner, and (3) be able to analyze a broad range of network workloads. Most state-of-the-art HPC network simulation frameworks, however, are constrained in one or more of these areas. In this work, we present a simulation framework for modeling two important classes of networks used in today's IBM and Cray supercomputers: torus and dragonfly networks. We use the Co-Design of Multi-layer Exascale Storage Architecture (CODES) simulation framework to simulate these network topologies at a flit-level detail using the Rensselaer Optimistic Simulation System (ROSS) for parallel discrete-event simulation. Our simulation framework meets all the requirements of a practical network simulation and can assist network designers in design space exploration. First, it uses validated and detailed flit-level network models to provide an accurate and high-fidelity network simulation. Second, instead of relying on serial time-stepped or traditional conservative discrete-event simulations that limit simulation scalability and efficiency, we use the optimistic event-scheduling capability of ROSS to achieve efficient and scalable HPC network simulations on today's high-performance cluster systems. Third, our models give network designers a choice in simulating a broad range of network workloads, including HPC application workloads using detailed network traces, an ability that is rarely offered in parallel with high-fidelity network simulations.},
   author = {Misbah Mubarak and Christopher D. Carothers and Robert B. Ross and Philip Carns},
   doi = {10.1109/TPDS.2016.2543725},
   issn = {1045-9219},
   issue = {1},
   journal = {IEEE Transactions on Parallel and Distributed Systems},
   keywords = {Massively parallel discrete-event simulation,interconnect networks,trace-based simulation},
   month = {1},
   pages = {87-100},
   title = {Enabling Parallel Simulation of Large-Scale HPC Network Systems},
   volume = {28},
   url = {https://www.osti.gov/servlets/purl/1366454 http://ieeexplore.ieee.org/document/7448965/},
   year = {2017},
}
@article{Misbah2017,
   author = {Bhatele Misbah and Nikhil Jain and Jens Domke and Noah Wolfe and Caitlin Ross and Abhinav Bhatele and Kelvin Li and Christopher D. Carothers and Kwan-Liu Ma and Robert B. Ross},
   title = {Toward reliable validation of HPC network simulation models},
   url = {https://www.cs.umd.edu/~bhatele/pubs/pdf/2017/wsc2017.pdf},
   year = {2017},
}
@article{Jiang2013,
   abstract = {Network-on-Chips (NoCs) are becoming integral parts of modern microprocessors as the number of cores and modules integrated on a single chip continues to increase. Research and development of future NoC technology relies on accurate modeling and simulations to evaluate the performance impact and analyze the cost of novel NoC architectures. In this work, we present BookSim, a cycle-accurate simulator for NoCs. The simulator is designed for simulation flexibility and accurate modeling of network components. It features a modular design and offers a large set of configurable network parameters in terms of topology, routing algorithm, flow control, and router microarchitecture, including buffer management and allocation schemes. BookSim furthermore emphasizes detailed implementations of network components that accurately model the behavior of actual hardware. We have validated the accuracy of the simulator against RTL implementations of NoC routers. © 2013 IEEE.},
   author = {Nan Jiang and James Balfour and Daniel U. Becker and Brian Towles and William J. Dally and George Michelogiannakis and John Kim},
   doi = {10.1109/ISPASS.2013.6557149},
   isbn = {9781467357777},
   issue = {March 2014},
   journal = {ISPASS 2013 - IEEE International Symposium on Performance Analysis of Systems and Software},
   pages = {86-96},
   title = {A detailed and flexible cycle-accurate Network-on-Chip simulator},
   year = {2013},
}
@article{Liu2015,
   abstract = {Fat-tree topologies have been widely adopted as the communication network in data centers in the past decade. Nowadays, high-performance computing (HPC) system designers are considering using fat-tree as the interconnection network for the next generation supercomputers. For extreme-scale computing systems like the data centers and supercomputers, the performance is highly dependent on the interconnection networks. In this paper, we present FatTreeSim, a PDES-based toolkit consisting of a highly scalable fat-tree network model, with the goal of better understanding the design constraints of fat-tree networking architectures in data centers and HPC systems, as well as evaluating the applications running on top of the network. FatTreeSim is designed to model and simulate large-scale fat-tree networks up to millions of nodes with protocol-level fidelity. We have conducted extensive experiments to validate and demonstrate the accuracy, scalability and usability of FatTreeSim. On Argonne Leadership Computing Facility's Blue Gene/Q system, Mira, FatTreeSim is capable of achieving a peak event rate of 305 M/s for a 524,288-node fat-tree model with a total of 567 billion committed events. The strong scaling experiments use up to 32,768 cores and show a near linear scalability. Comparing with a small-scale physical system in Emulab, FatTreeSim can accurately model the latency in the same fat-tree network with less than 10% error rate for most cases. Finally, we demonstrate FatTreeSim's usability through a case study in which FatTreeSim serves as the network module of the YARNsim system, and the error rates for all test cases are less than 13.7%.},
   author = {Ning Liu and Adnan Haider and Xian He Sun and Dong Jin},
   doi = {10.1145/2769458.2769474},
   isbn = {9781450335836},
   journal = {SIGSIM-PADS 2015 - Proceedings of the 3rd ACM Conference on SIGSIM-Principles of Advanced Discrete Simulation},
   keywords = {Blue Gene/Q,Datacenter interconnection network,Fat-tree networks,Parallel discrete event simulation,Supercomputer interconnection networks},
   pages = {199-210},
   title = {FatTreeSim: Modeling large-scale fat-tree networks for HPC systems and data centers using parallel and discrete event simulation},
   year = {2015},
}
@article{Liu,
   abstract = {Exascale supercomputers will have the potential for billion- way parallelism. While physical implementations of these systems are currently not available, HPC system designers can develop models of exascale systems to evaluate system design points. Modeling these sys- tems and associated subsystems is a significant challenge. In this paper, we present the Co-design of Exascale Storage System (CODES) frame- work for evaluating exascale storage system design points. As part of our early work with CODES, we discuss the use of the CODES framework to simulate leadership-scale storage systems in a tractable amount of time using parallel discrete-event simulation. We describe the current stor- age system models and protocols included with the CODES framework and demonstrate the use of CODES through simulations of an existing petascale storage system.},
   author = {Ning Liu and Christopher Carothers and Jason Cope and Philip Carns and Robert Ross and Adam Crume and Carlos Maltzahn},
   keywords = {exascale computing,parallel discrete-,storage system design},
   title = {Modeling a Leadership-scale Storage System},
   url = {https://www.mcs.anl.gov/uploads/cels/papers/1972-1011.pdf},
}
@article{Carothers2002,
   abstract = {In this paper, we introduce a new Time Warp system called ROSS: Rensselaer's optimistic simulation system. ROSS is an extremely modular kernel that is capable of achieving event rates as high as 1,250,000 events per second when simulating a wireless telephone network model (PCS) on a quad processor PC server. In a head-to-head comparison, we observe that ROSS out performs the Georgia Tech Time Warp (GTW) system by up to 180% on a quad processor PC server and up to 200% on the SGI Origin 2000. ROSS only requires a small constant amount of memory buffers greater than the amount needed by the sequential simulation for a constant number of processors. ROSS demonstrates for the first time that stable, highly efficient execution using little memory above what the sequential model would require is possible for low-event granularity simulation models. The driving force behind these high-performance and low-memory utilization results is the coupling of an efficient pointer-based implementation framework, Fujimoto's fast GVT algorithm for shared memory multiprocessors, reverse computation and the introduction of kernel processes (KPs). KPs lower fossil collection overheads by aggregating processed event lists. This aspect allows fossil collection to be done with greater frequency, thus lowering the overall memory necessary to sustain stable, efficient parallel execution. These characteristics make ROSS an ideal system for use in large-scale networking simulation models. The principle conclusion drawn from this study is that the performance of an optimistic simulator is largely determined by its memory usage. © 002 Published by Elsevier Science (USA).},
   author = {Christopher D. Carothers and David Bauer and Shawn Pearce},
   doi = {10.1016/S0743-7315(02)00004-7},
   issn = {07437315},
   issue = {11},
   journal = {Journal of Parallel and Distributed Computing},
   keywords = {Discrete-event simulation,Parallel simulation,Reverse computation,Time warp},
   month = {11},
   pages = {1648-1669},
   title = {ROSS: A high-performance, low-memory, modular Time Warp system},
   volume = {62},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0743731502000047},
   year = {2002},
}
@article{Castain2018,
   abstract = {High-Performance Computing (HPC) applications have historically executed in static resource allocations, using programming models that ran independently from the resident system management stack (SMS). Achieving exascale performance that is both cost-effective and fits within site-level environmental constraints will, however, require that the application and SMS collaboratively orchestrate the flow of work to optimize resource utilization and compensate for on-the-fly faults. The Process Management Interface - Exascale (PMIx) community is committed to establishing scalable workflow orchestration by defining an abstract set of interfaces by which not only applications and tools can interact with the resident SMS, but also the various SMS components can interact with each other. This paper presents a high-level overview of the goals and current state of the PMIx standard, and lays out a roadmap for future directions.},
   author = {Ralph H. Castain and Joshua Hursey and Aurelien Bouteiller and David Solt},
   doi = {10.1016/j.parco.2018.08.002},
   issn = {01678191},
   journal = {Parallel Computing},
   pages = {9-29},
   publisher = {Elsevier B.V.},
   title = {PMIx: Process management for exascale environments},
   volume = {79},
   url = {https://doi.org/10.1016/j.parco.2018.08.002},
   year = {2018},
}
@article{Leon2016,
   abstract = {Understanding the characteristics and requirements of applications that run on commodity clusters is key to properly configuring current machines and, more importantly, procuring future systems effectively. There are only a few studies, however, that are current and characterize realistic workloads. For HPC practitioners and researchers, this limits our ability to design solutions that will have an impact on real systems. We present a systematic study that characterizes applications with an emphasis on communication requirements. It includes cluster utilization data, identifying a representative set of applications from a U.S. Department of Energy laboratory, and characterizing their communication requirements. The driver for this work is understanding application sensitivity to a tapered fat-tree network. These results provided key insights into the procurement of our next generation commodity systems. We believe this investigation can provide valuable input to the HPC community in terms of workload characterization and requirements from a large supercomputing center.},
   author = {Edgar A. Leon and Ian Karlin and Abhinav Bhatele and Steven H. Langer and Chris Chambreau and Louis H. Howell and Trent D'Hooge and Matthew L. Leininger},
   doi = {10.1109/SC.2016.77},
   isbn = {9781467388153},
   issn = {21674337},
   issue = {November},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   keywords = {High performance computing,computer performance,high-speed networks,network topology,scientific computing},
   pages = {909-920},
   title = {Characterizing Parallel Scientific Applications on Commodity Clusters: An Empirical Study of a Tapered Fat-Tree},
   volume = {0},
   year = {2016},
}
@article{Cornet2008,
   abstract = {Transaction Level Modeling (TLM) captures abstract models of Systems-on-Chip that simulate faster than traditional RTL simulations and are available earlier in the design flow. Such models allow the development of the embedded software on a virtual prototype of the hardware, before the chip is available. Various levels of details in TL models are needed; using untimed and timed models for different purposes is a usual practice. We present a method for developing very abstract un-timed models first, and then enriching them to get detailed timed models, while preserving the functionality. The timed models can be as rich as the models usually written from scratch. The experiments with industrial case-studies show improved simulation speed and reduced modeling effort for both untimed and timed models. © 2008 EDAA.},
   author = {Jérôme Cornet and Florence Maraninchi and Laurent Maillet-Contoz},
   doi = {10.1109/DATE.2008.4484652},
   isbn = {9783981080},
   issn = {15301591},
   journal = {Proceedings -Design, Automation and Test in Europe, DATE},
   pages = {9-14},
   title = {A method for the efficient development of timed and untimed Transaction-Level Models of Systems-on-Chip},
   url = {https://past.date-conference.com/proceedings-archive/PAPERS/2008/DATE08/PDFFILES/01.2_2.PDF},
   year = {2008},
}
@article{Escudero-Sahuquillo2011,
   abstract = {Existing congestion control mechanisms in interconnects can be divided into two general approaches. One is to throttle traffic injection at the sources that contribute to congestion, and the other is to isolate the congested traffic in specially designated resources. These two approaches have different, but non-overlapping weaknesses. In this paper we present in detail a method that combines injection throttling and congested-flow isolation. Through simulation studies we first demonstrate the respective flaws of the injection throttling and of flow isolation. Thereafter we show that our combined method extracts the best of both approaches in the sense that it gives fast reaction to congestion, it is scalable and it has good fairness properties with respect to the congested flows. © 2011 IEEE.},
   author = {Jesus Escudero-Sahuquillo and Ernst Gunnar Gran and Pedro Javier Garcia and Jose Flich and Tor Skeie and Olav Lysne and Francisco Jose Quiles and Jose Duato},
   doi = {10.1109/ICPP.2011.80},
   isbn = {9780769545103},
   issn = {01903918},
   journal = {Proceedings of the International Conference on Parallel Processing},
   keywords = {Congestion management,HoL-blocking,Interconnection networks},
   pages = {662-672},
   title = {Combining congested-flow isolation and injection throttling in HPC interconnection networks},
   year = {2011},
}
@article{Calotoiu2016,
   abstract = {Tuning large applications requires a clever exploration of the design and configuration space. Especially on supercomputers, this space is so large that its exhaustive traversal via performance experiments becomes too expensive, if not impossible. Manually creating analytical performance models provides insights into optimization opportunities but is extremely laborious if done for applications of realistic size. If we must consider multiple performance-relevant parameters and their possible interactions, a common requirement, this task becomes even more complex. We build on previous work on automatic scalability modeling and significantly extend it to allow insightful modeling of any combination of application execution parameters. Multi-parameter modeling has so far been outside the reach of automatic methods due to the exponential growth of the model search space. We develop a new technique to traverse the search space rapidly and generate insightful performance models that enable a wide range of uses from performance predictions for balanced machine design to performance tuning.},
   author = {Alexandru Calotoiu and David Beckingsale and Christopher W. Earl and Torsten Hoefler and Ian Karlin and Martin Schulz and Felix Wolf},
   doi = {10.1109/CLUSTER.2016.57},
   isbn = {9781509036530},
   issn = {15525244},
   journal = {Proceedings - IEEE International Conference on Cluster Computing, ICCC},
   pages = {172-181},
   title = {Fast multi-parameter performance modeling},
   year = {2016},
}
@book{Wagner20010,
   author = {Rainer Keller and Edgar Gabriel and Michael Resch and Jack Dongarra},
   isbn = {3540851739},
   issn = {03029743},
   title = {Recent Advances in the Message Passing Interface},
   year = {2010},
}
@article{Vialle2017,
   author = {Stephane Vialle and Jean-Philippe Tavella and Cherifa Dad and Remi Corniglion and Mathieu Caujolle and Vincent Reinbold},
   doi = {10.3384/ecp17132673},
   journal = {Proceedings of the 12th International Modelica Conference, Prague, Czech Republic, May 15-17, 2017},
   pages = {673-682},
   title = {Scaling FMI-CS Based Multi-Simulation Beyond Thousand FMUs on Infiniband Cluster},
   volume = {132},
   year = {2017},
}
@book_section{Pfister2001,
   abstract = {The InfiniBand Architecture (IBA) is a new industry-standard architecture for server I/O and inter-server communication. It was developed by the InfiniBandSM Trade Association (IBTA) to provide the levels of reliability, availability, performance, and scalability necessary for present and future server systems, levels significantly better than can be achieved with bus-oriented I/O structures. This chapter provides a description of the reason IBA was developed, an brief overview of the architecture as a whole, more detailed information about several selected IBA topics, and discussion of industry implications of this architecture.},
   author = {Gregory F. Pfister},
   doi = {10.1109/9780470544839.ch42},
   isbn = {9780470544839},
   journal = {High Performance Mass Storage and Parallel I/O: Technologies and Applications},
   keywords = {Bandwidth,Copper,Industries,Optical fibers,Optical switches,Servers,Software},
   pages = {617-632},
   title = {An introduction to the InfiniBand™ architecture},
   url = {http://gridbus.csse.unimelb.edu.au/~raj/superstorage/chap42.pdf},
   year = {2001},
}
@article{Derradji2015,
   abstract = {BXI, Bull eXascale Interconnect, is the new interconnection network developed by Atos for High Performance Computing. In this paper, we first present an overview of the BXI network. The BXI network is designed and optimized for HPC workloads at very large scale. It is based on the Portals 4 protocol and permits a complete offload of communication primitives in hardware, thus enabling independent progress of computation and communication. We then describe the two BXI ASIC components, the network interface and the BXI switch, and the BXI software environment. We finally explain how the Bull exascale platform integrates BXI to build a large scale parallel system and we give some performance estimations.},
   author = {Saïd Derradji and Thibaut Palfer-Sollier and Jean Pierre Panziera and Axel Poudes and François Wellenreiter},
   doi = {10.1109/HOTI.2015.15},
   isbn = {9781467391603},
   journal = {Proceedings - 2015 IEEE 23rd Annual Symposium on High-Performance Interconnects, HOTI 2015},
   keywords = {Interconnect technologies,high performance computing,parallel and scalable system architecture},
   pages = {18-25},
   title = {The BXI Interconnect Architecture},
   year = {2015},
}
@article{Shamis2015,
   abstract = {This paper presents Unified Communication X (UCX), a set of network APIs and their implementations for high throughput computing. UCX comes from the combined effort of national laboratories, industry, and academia to design and implement a high-performing and highly-scalable network stack for next generation applications and systems. UCX design provides the ability to tailor its APIs and network functionality to suit a wide variety of application domains and hardware. We envision these APIs to satisfy the networking needs of many programming models such as Message Passing Interface (MPI), OpenSHMEM, Partitioned Global Address Space (PGAS) languages, task-based paradigms and I/O bound applications. To evaluate the design we implement the APIs and protocols, and measure the performance of overhead-critical network primitives fundamental for implementing many parallel programming models and system libraries. Our results show that the latency, bandwidth, and message rate achieved by the portable UCX prototype is very close to that of the underlying driver. With UCX, we achieved a message exchange latency of 0.89 us, a bandwidth of 6138.5 MB/s, and a message rate of 14 million messages per second. As far as we know, this is the highest bandwidth and message rate achieved by any network stack (publicly known) on this hardware.},
   author = {Pavel Shamis and Manjunath Gorentla Venkata and M. Graham Lopez and Matthew B. Baker and Oscar Hernandez and Yossi Itigin and Mike Dubman and Gilad Shainer and Richard L. Graham and Liran Liss and Yiftah Shahar and Sreeram Potluri and Davide Rossetti and Donald Becker and Duncan Poole and Christopher Lamb and Sameer Kumar and Craig Stunkel and George Bosilca and Aurelien Bouteiller},
   doi = {10.1109/HOTI.2015.13},
   isbn = {9781467391603},
   journal = {Proceedings - 2015 IEEE 23rd Annual Symposium on High-Performance Interconnects, HOTI 2015},
   keywords = {HPC,Infiniband,MPI,Middleware,OpenSHMEM,PGAS,RDMA},
   pages = {40-43},
   title = {UCX: An Open Source Framework for HPC Network APIs and Beyond},
   year = {2015},
}
@book_section{Gabriel2004,
   abstract = {A large number of MPI implementations are currently available, each of which emphasize different aspects of high-performance computing or are intended to solve a specific research problem. The result is a myriad of incompatible MPI implementations, all of which require separate installation, and the combination of which present significant logistical challenges for end users. Building upon prior research, and influenced by experience gained from the code bases of the LAM/MPI, LA-MPI, and FT-MPI projects, Open MPI is an all-new, production-quality MPI-2 implementation that is fundamentally centered around component concepts. Open MPI provides a unique combination of novel features previously unavailable in an open-source, production-quality implementation of MPI. Its component architecture provides both a stable platform for third-party research as well as enabling the run-time composition of independent software add-ons. This paper presents a high-level overview the goals, design, and implementation of Open MPI. © Springer-Verlag 2004.},
   author = {Edgar Gabriel and Graham E. Fagg and George Bosilca and Thara Angskun and Jack J. Dongarra and Jeffrey M. Squyres and Vishal Sahay and Prabhanjan Kambadur and Brian Barrett and Andrew Lumsdaine and Ralph H. Castain and David J. Daniel and Richard L. Graham and Timothy S. Woodall},
   doi = {10.1007/978-3-540-30218-6_19},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {97-104},
   title = {Open MPI: Goals, Concept, and Design of a Next Generation MPI Implementation},
   volume = {3241},
   url = {http://link.springer.com/10.1007/978-3-540-30218-6_19},
   year = {2004},
}
@article{Sibilano2016,
   abstract = {An Extra Electric Energy System that supplies electrical power to an aircraft of the continuous flight duration day/ night cycle solar powered type is described where the extra electric energy is generated as static charge by collisions of the aircraft with particles existing in the earth's atmosphere. The manner in which the static charge is collected, stored and used immediately to meet present or future aircraft power needs is considered. An example of the application of the system to a high altitude Solar powered Helios type aircraft is offered along with a discussion of how its structure is modi fied to store static charge.},
   author = {Riccardo Sibilano},
   issue = {12},
   pages = {2-6},
   title = {METHOD AND APPARATUS FOR THE SIMULATION OF COMPUTER NETWORKS},
   volume = {1},
   url = {https://patentimages.storage.googleapis.com/b2/63/b7/76d446a673e691/US7620535.pdf},
   year = {2016},
}
@article{Wu,
   author = {Jiesheng Wu and Amith R. Mamidala and Dhabaleswar K Panda},
   title = {Can NIC Memory in InfiniBand Benefit Communication Performance ? — A Study with Mellanox Adapter},
   url = {https://d1wqtxts1xzle7.cloudfront.net/34256614/TR20.pdf?1405948935=&response-content-disposition=inline%3B+filename%3DCan_NIC_Memory_in_InfiniBand_Benefit_Com.pdf&Expires=1593619044&Signature=NXImjYM06s5VYAuy0EhJZEFNbbGVtuwB~glBwQkRxVDOlgTgxp1zbANTQK6VUHe},
}
@article{Sur2007,
   abstract = {InfiniBand is an emerging networking technology that is gaining rapid acceptance in the HPC domain. Currently, several systems in the Top500 list use InfiniBand as their primary interconnect, with more being planned for near future. The fundamental architecture of the systems are undergoing a sea-change due to the advent of commodity multi-core computing. Due to the increase in the number of processes in each compute node, the network interface is expected to handle more communication traffic as compared to older dual or quad SMP systems. Thus, the network architecture should provide scalable performance as the number of processing cores increase.ConnectX is the fourth generation InfiniBand adapter from Mellanox Technologies. Its novel architecture enhances the scalability and performance of InfiniBand on multi-core clusters. In this paper, we carry out an in-depth performance analysis of ConnectX architecture comparing it with the third generation InfiniHost III architecture on the Intel Bensley platform with Dual Clovertown processors. Our analysis reveals that the aggregate bandwidth for small and medium sized messages can be increased by a factor of 10 as compared to the third generation InfiniHost III adapters. Similarly, RDMA-Write and RDMA-Read latencies for 1-byte messages can be reduced by a factor of 6 and 3, respectively, even when all cores are communicating simultaneously. Evaluation with communication kernel Halo reveals a performance benefit of a factor of 2 to 5. Finally, the performance of LAMMPS, a molecular dynamics simulator, is improved by 10% for the in.rhodo benchmark. © 2007 IEEE.},
   author = {Sayantan Sur and Matthew J. Koop and Chai Lei and Dhabaleswar K. Panda},
   doi = {10.1109/HOTI.2007.16},
   isbn = {0769529798},
   journal = {Proceedings - 15th Annual IEEE Symposium on High-Performance Interconnects, HOT Interconnects},
   pages = {125-131},
   title = {Performance analysis and evaluation of mellanox ConnectX InfiniBand architecture with multi-core platforms},
   url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.4890&rep=rep1&type=pdf},
   year = {2007},
}
@article{Subotic2010,
   abstract = {Overlapping communication and computation has been devised as an attractive technique to alleviate the huge application's network requirements at large scale. Overlapping will allow to fully or partially hide the long communication delays suffered when transferring messages through the network. This will relax the application's network requirements, and hence allow to deploy more cost-effective network designs. However, today's scientific applications make little use of overlapping. In addition, there is no support to analyze how overlap could impact the performance of real scientific applications. In this paper we address this issue by presenting a simulation framework to automatically analyze the benefits of communication-computation overlap. The simulation framework consists of a binary translation tool (Valgrind), a distributed machine simulator (Dimemas), and a visualization tool (Paraver). Valgrind instruments the legacy MPI application and generates the execution traces, then Dimemas uses the obtained traces and reconstructs the application's time-behavior on a configurable parallel platform, and finally Paraver visualizes the obtained time-behaviors. Our simulation methodology brings two new features into the study of overlap: 1) automatic simulation of the overlapped execution - as there is no need for code restructuring in applications; and 2) visualization of simulated time behaviors, that further allows useful comparisons of the non-overlapped and the overlapped executions. © 2010 IEEE.},
   author = {Vladimir Subotic and Jose Carlos Sancho and Jesus Labarta and Mateo Valero},
   doi = {10.1109/CLUSTER.2010.33},
   isbn = {9780769542201},
   issn = {15525244},
   journal = {Proceedings - IEEE International Conference on Cluster Computing, ICCC},
   keywords = {Communication-computation overlap,MPI},
   pages = {275-283},
   title = {A simulation framework to automatically analyze the communication- computation overlap in scientific applications},
   url = {https://upcommons.upc.edu/bitstream/handle/2117/109234/05600298.pdf},
   year = {2010},
}
@report{Brief,
   title = {4th Generation Server & Storage Adapter Architecture},
   year = {2007},
}
@article{Vaughan2011,
   abstract = {Cielo, a Cray XE6, is the Department of Energy NNSA Advanced Simulation and Computing (ASC) campaign's newest capability machine. Rated at 1.37 PFLOPS, it consists of 8,944 dual-socket oct-core AMD Magny-Cours compute nodes, linked using Cray's Gemini interconnect. Its primary mission objective is to enable a suite of the ASC applications implemented using MPI to scale to tens of thousands of cores. Cielo is an evolutionary improvement to a successful architecture previously available to many of our codes, thus enabling a basis for understanding the capabilities of this new architecture. Using three codes strategically important to the ASC campaign, and supplemented with some micro-benchmarks that expose the fundamental capabilities of the XE6, we report on the performance characteristics and capabilities of Cielo. © 2011 IEEE.},
   author = {Courtenay Vaughan and Mahesh Rajan and Richard Barrett and Doug Doerfler and Kevin Pedretti},
   doi = {10.1109/IPDPS.2011.342},
   isbn = {9780769543857},
   journal = {IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum},
   keywords = {High performance computing,Message passing communication,Parallel architectures,Performance evaluation,Scientific applications},
   pages = {1831-1837},
   title = {Investigating the impact of the Cielo Cray XE6 architecture on scientific application codes},
   year = {2011},
}
@article{Shan2011,
   abstract = {The Gemini interconnect on the Cray XE6 platform provides for lightweight remote direct memory access (RDMA) between nodes, which is useful for implementing partitioned global address space languages like UPC and Co-Array Fortran. In this paper, we perform a study of Gemini performance using a set of communication microbenchmarks and compare the performance of one-sided communication in PGAS languages with two-sided MPI. Our results demonstrate the performance benefits of the PGAS model on Gemini hardware, showing in what circumstances and by how much one-sided communication outperforms two-sided in terms of messaging rate, aggregate bandwidth, and computation and communication overlap capability. For example, for 8-byte and 2KB messages the one-sided messaging rate is 5 and 10 times greater respectively than the two-sided one. The study also reveals important information about how to optimize one-sided Gemini communication.},
   author = {Hongzhang Shan and Nicholas J. Wright and John Shalf and Katherine Yelick and Marcus Wagner and Nathan Wichmann},
   doi = {10.1145/2088457.2088467},
   isbn = {9781450311021},
   issue = {2},
   journal = {PMBS'11 - Proceedings of the 2nd International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computing Systems, Co-located with SC'11},
   keywords = {CAF,MPI,Messaging rate,PGAS,Performance},
   pages = {13-14},
   title = {A preliminary evaluation of the hardware acceleration of the Cray Gemini interconnect for PGAS languages and comparison with MPI},
   url = {https://www.researchgate.net/profile/Haihang_You/publication/228946119_The_Design_of_an_Auto-Tuning_IO_Framework_on_Cray_XT5_System/links/0046352a0b5ea607e4000000.pdf},
   year = {2011},
}
@techreport{Cray2010,
author = {Cray},
file = {:home/julien/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Using the GNI and DMAPP APIs.pdf:pdf},
institution = {Cray},
title = {{Using the GNI and DMAPP APIs}},
url = {https://web.archive.org/web/20160615083140/http://docs.cray.com:80/books/S-2446-3103/S-2446-3103.pdf},
year = {2010}
}

@article{Sun2012,
   abstract = {Gemini, the network for the new Cray XE/XK systems, features low latency, high bandwidth and strong scalability. Its hardware support for remote direct memory access enables efficient implementation of the global address space programming languages. Although the user Generic Network Interface (uGNI) provides a low-level interface for Gemini with support to the message-passing programming model (MPI), it remains challenging to port alternative programming models with scalable performance. CHARM++ is an object-oriented message-driven programming model. Its applications have been shown to scale up to the full Jaguar Cray XT machine. In this paper, we present an implementation of this programming model on uGNIfor the Cray XE/XK systems. Several techniques are presented to exploit the uGNI capabilites by reducing memory copy and registration overhead, taking advantage of the persistent communication, and improving intra-node communication. Our micro-benchmark results demonstrate that the uGNI-based runtime system outperforms the MPI-based implementation by up to 50% in terms of message latency. For communication intensive applications such as N-Queens, this implementation scales up to 15,360 cores of a Cray XE6 machine and is 70% faster than the MPI-based implementation. In molecular dynamics application NAMD, the performance is also considerably improved by as much as 18%. © 2012 IEEE.},
   author = {Yanhua Sun and Gengbin Zheng and Laximant V. Kalé and Terry R. Jones and Ryan Olson},
   doi = {10.1109/IPDPS.2012.127},
   isbn = {9780769546759},
   journal = {Proceedings of the 2012 IEEE 26th International Parallel and Distributed Processing Symposium, IPDPS 2012},
   keywords = {Asynchronous message-driven,Cray XE/XT,Gemini Interconnect,Low Level Runtime System},
   pages = {751-762},
   title = {A uGNI-based asynchronous message-driven runtime system for Cray supercomputers with Gemini interconnect},
   url = {https://d1wqtxts1xzle7.cloudfront.net/40617539/A_uGNI-based_Asynchronous_Message-driven20151203-17667-1n4ckvy.pdf?1449178391=&response-content-disposition=inline%3B+filename%3DA_uGNI-based_Asynchronous_Message-driven.pdf&Expires=1593796901&Signature=HYTh6},
   year = {2012},
}
@article{Brightwell2005,
   abstract = {The Cray SeaStar is a new network interface and router for the Cray Red Storm and XT3 supercomputer. The SeaStar was designed specifically to meet the performance and reliability needs of a large-scale, distributed-memory scientific computing platform. In this paper, we present an initial performance evaluation of the SeaStar. We first provide a detailed overview of the hardware and software features of the SeaStar, followed by the results of several low-level micro-benchmarks. These initial results indicate that SeaStar is on a path to achieving its performance targets. © 2005 IEEE.},
   author = {Ron Brightwell and Kevin Pedretti and Keith D. Underwood},
   doi = {10.1109/CONECT.2005.24},
   isbn = {0769524494},
   issn = {15504794},
   journal = {Proceedings - Symposium on the High Performance Interconnects, Hot Interconnects},
   pages = {51-57},
   title = {Initial performance evaluation of the cray seaStar interconnect},
   volume = {2005},
   url = {http://www.cs.sandia.gov/~ktpedre/copyrighted-papers/01544577.pdf},
   year = {2005},
}
@article{You2011,
   author = {Haihang You and Qing Liu and Zhiqiang Li and Shirley Moore},
   journal = {Cray User Group meeting},
   keywords = {-i,auto-tuning,o,performance modeling,queuing},
   pages = {10},
   title = {The Design of an Auto-Tuning I/O Framework on Cray XT5 System},
   year = {2011},
}
@thesis{Heinrich2019,
   abstract = {The High-Performance Computing (HPC) community is currently undergoingdisruptive technology changes in almost all fields, including a switch towardsmassive parallelism with several thousand compute cores on a single GPU oraccelerator and new, complex networks. The energy consumption of these machines will continue to grow in the future,making energy one of the principal cost factors of machine ownership. This explainswhy even the classic metric "flop/s", generally used to evaluate HPC applicationsand machines, is widely regarded as to be replaced by an energy-centric metric"flop/watt". One approach to predict energy consumption is through simulation, however,an accurate simulation of the system is crucial to estimate the energy faithfully.In this thesis, we contribute to the performance and energy prediction of HPCarchitectures. We propose an energy model which we have implemented in the opensource SimGrid simulator. We validate this model by carefully and systematicallycomparing it with real experiments. We leverage this contribution to both evaluateexisting and propose new DVFS governors that are designed to suit the HPCcontext.},
   author = {Franz Heinrich},
   institution = {Université Grenoble Alpes},
   title = {Modeling , Prediction and Optimization of Energy Consumption of MPI Applications using SimGrid},
   url = {https://tel.archives-ouvertes.fr/tel-02269894/document},
   year = {2019},
}
@article{Gran2012,
   author = {Ernst Gunnar Gran and Sven-Arne Reinemo},
   doi = {10.4108/icst.simutools.2011.245509},
   isbn = {9781936968008},
   issue = {2},
   keywords = {congestion control,infiniband,omnet},
   pages = {390-397},
   title = {InfiniBand Congestion Control, Modelling and validation},
   volume = {1},
   url = {https://www.simula.no/sites/default/files/publications/Simula.simula.362.pdf},
   year = {2012},
}
@article{VonEicken1992,
   abstract = {The design challenge for large-scale multiprocessors is (1) to minimize communication overhead, (2) allow communication to overlap computation, and (3) coordinate the two without sacrificing processor cost/performance. We show that existing message passing ...},
   author = {Thorsten von Eicken and David E. Culler and Seth Copen Goldstein and Klaus Erik Schauser},
   doi = {10.1145/146628.140382},
   issn = {01635964},
   issue = {2},
   journal = {ACM SIGARCH Computer Architecture News},
   pages = {256-266},
   title = {Active messages: A Mechanism for Integrated Communication and Computation},
   volume = {20},
   url = {https://dl.acm.org/doi/pdf/10.1145/146628.140382?casa_token=zM4x2G5qHZkAAAAA:bpT82AbSBoAErXa4ZnCw76-DIh7JQEU914aB1PpNBYNQw561CACATbyUNrQLwh45JdGEZGYsh5sPrQ},
   year = {1992},
}
@article{Nobach2016,
   abstract = {P4 is a high-level language for programming protocol-inde- pendent packet processors. P4 works in conjunction with SDN control protocols like OpenFlow. In its current form, OpenFlow explicitly specifies protocol headers on which it operates. This set has grown from 12 to 41 fields in a few years, increasing the complexity of the specification while still not providing the flexibility to add new headers. In this paper we propose P4 as a strawman proposal for how Open- Flow should evolve in the future. We have three goals: (1) Reconfigurability in the field: Programmers should be able to change the way switches process packets once they are deployed. (2) Protocol independence: Switches should not be tied to any specific network protocols. (3) Target inde- pendence: Programmers should be able to describe packet- processing functionality independently of the specifics of the underlying hardware. As an example, we describe how to use P4 to configure a switch to add a new hierarchical label.},
   author = {Leonhard Nobach and Ivica Rimac and Volker Hilt and David Hausheer},
   doi = {10.1109/ICNP.2016.7784459},
   isbn = {9781509032815},
   issn = {10921648},
   issue = {3},
   journal = {ACM SIGCOMM Computer Communication Review},
   title = {P4: Programming Protocol-Independent Packet Processors},
   volume = {44},
   url = {https://dl.acm.org/doi/pdf/10.1145/2656877.2656890?casa_token=G4m-R9RNosAAAAAA:_UdsTy-b6nHbE0aMRIRn6nNv-_8CQuoixwq9mcOJbKIUeqXnZe_PluuYRVmI20Jvj9XYFWhbc7yjaw},
   year = {2014},
}
@article{Hoefer2017,
   abstract = {Optimizing communication performance is imperative for large-scale computing because communication overheads limit the strong scalability of parallel applications. Today's network cards contain rather powerful processors optimized for data movement. However, these devices are limited to fixed functions, such as remote direct memory access. We develop sPIN, a portable programming model to offload simple packet processing functions to the network card. To demonstrate the potential of the model, we design a cycle-accurate simulation environment by combining the network simulator Log-GOPSim and the CPU simulator gem5. We implement offloaded message matching, datatype processing, and collective communications and demonstrate transparent full-application speedups. Furthermore, we show how sPIN can be used to accelerate redundant in-memory filesystems and several other use cases. Our work investigates a portable packet-processing network acceleration model similar to compute acceleration with CUDA or OpenCL. We show how such network acceleration enables an eco-system that can significantly speed up applications and system services.},
   author = {Torsten Hoefer and Salvatore Di Girolamo and Konstantin Taranov and Ryan E. Grant and Ron Brightwell},
   doi = {10.1145/3126908.3126970},
   isbn = {9781450351140},
   journal = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2017},
   title = {sPIN: High-performance streaming processing in the network},
   url = {https://arxiv.org/pdf/1709.05483.pdf https://dl.acm.org/doi/pdf/10.1145/3126908.3126970?casa_token=RjGQh_VaODkAAAAA:0tFh74T6iSCLjqxx32CcLVOvMlOT5jw9OZa-l_Ed7AwyUShXz38dsuEJ1sLjjUpcOQpzb0M-Jc3wEA},
   year = {2017},
}
@article{Kekely2018,
   abstract = {CESNET (Czech NREN) is ready to demonstrate a new NFB-200G2QL accelerator with Virtex UltraScale+ FPGA specifically designed to push the achievable traffic processing throughput to 200 Gbps in a single card. Unique high-speed DMA engines in the FPGA together with highly optimized Linux drivers enable to achieve 200 Gbps data transfer through two PCIe Gen3 χ 16 interfaces with minimal CPU overhead. Captured network traffic can be independently distributed among individual cores of two physical CPUs (NUMA nodes) without utilization of QPI. As a result, wire-speed packet capture to the host memory from two fully saturated 100 Gbps Ethernet interfaces (QSFP28+) is achieved and various network monitoring applications can utilize the power of the latest FPGAs and CPUs for data processing. This is especially useful when traffic of both directions of a single 100GbE link needs to be processed. The proposed demonstration will show how the packets can be received from two 100 Gbps Ethernet links at full speed and captured to the host memory at 200 Gbps without any loss. The opposite direction of communication will also be shown, i.e. how the packets can be transmitted from the host memory towards the two 100GbE network interfaces. Achieved speeds will be demonstrated by counters and graphs showing generated, received/transmitted and captured packets. We will also show detailed statistics of CPU load during the packet capture/transmission for different packet lengths.},
   author = {Lukáš Kekely and Martin Špinier and Štepán Friedl and Jiří Sikora and Jan Kořenek},
   doi = {10.1109/NOMS.2018.8406115},
   isbn = {9781538634165},
   journal = {IEEE/IFIP Network Operations and Management Symposium: Cognitive Management in a Cyber World, NOMS 2018},
   pages = {1-3},
   title = {Live demonstration of FPGA based networking accelerator for 200 Gbps data transfers},
   year = {2018},
}
@article{Denzel2008,
   abstract = {We present an end-to-end simulation framework that is capable of simulating High-Performance Computing (HPC) systems with hundreds of thousands of interconnected processors. The tool applies discrete event simulation and is driven by real-world application traces. We refer to it as MARS (MPI Application Replay network Simulator). It maintains reasonable simulation details of both the processors in general and specifically the inter-connection network. Among other things, it features several network topologies, flexible routing schemes, arbitrary application task placement, point-to-point statistics collection, and data visualization. With a few case studies, we demonstrate the usefulness of this tool for assisting high-level system design as well as for performance projection and application tuning of future HPC systems.},
   author = {Wolfgang E. Denzel and Jian Li and Peter Walker and Yuho Jin},
   doi = {10.4108/ICST.SIMUTOOLS2008.3034},
   isbn = {9789639799233},
   journal = {SIMUTools 2008 - 1st International ICST Conference on Simulation Tools and Techniques for Communications, Networks and Systems},
   keywords = {End-to-end simulation,High-Performance Computing,Inter-connection network},
   title = {A framework for end-to-end simulation of highperformance computing systems},
   url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.451.9636&rep=rep1&type=pdf},
   year = {2008},
}
@book_section{Knight2018,
   abstract = {Architecture simulation can aid in predicting and understanding application performance, particularly for proposed hardware or large system designs that do not exist. In network design studies for high-performance computing, most simulators focus on the dominant message passing (MPI) model. Currently, many simulators build and maintain their own simulator-specific implementations of MPI. This approach has several drawbacks. Rather than reusing an existing MPI library, simulator developers must implement all semantics, collectives, and protocols. Additionally, alternative runtimes like GASNet cannot be simulated without again building a simulator-specific version. It would be far more sustainable and flexible to maintain lower-level layers like uGNI or IB-verbs and reuse the production runtime code. Directly building and running production communication runtimes inside a simulator poses technical challenges, however. We discuss these challenges and show how they are overcome via the macroscale components for the Structural Simulation Toolkit (SST), leveraging a basic source-to-source tool to automatically adapt production code for simulation. SST is able to encapsulate and virtualize thousands of MPI ranks in a single simulator process, providing a “supercomputer in a laptop” environment. We demonstrate the approach for the production GASNet runtime over uGNI running inside SST. We then discuss the capabilities enabled, including investigating performance with tunable delays, deterministic debugging of race conditions, and distributed debugging with serial debuggers.},
   author = {Samuel Knight and Joseph P. Kenny and Jeremiah J. Wilke},
   doi = {10.1007/978-3-030-02465-9_23},
   isbn = {9783030024642},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {347-359},
   title = {Supercomputer in a Laptop: Distributed Application and Runtime Development via Architecture Simulation},
   volume = {11203 LNCS},
   url = {https://www.osti.gov/servlets/purl/1515812},
   year = {2018},
}
@article{Ahmed2017,
   abstract = {Performance Prediction Toolkit (PPT) is a simulator mainly developed at Los Alamos National Laboratory to facilitate rapid and accurate performance prediction of large-scale scientific applications on existing and future HPC architectures. In this paper, we present three interconnect models for performance prediction of large-scale HPC applications. They are based on interconnect topologies widely used in HPC systems: torus, dragonfly, and fat-tree. We conduct extensive validation tests of our interconnect models, in particular, using configurations of existing HPC systems. Results show that our models provide good accuracy for predicting the network behavior. We also present a performance study of a parallel computational physics application to show that our model can accurately predict the parallel behavior of large-scale applications.},
   author = {Kishwar Ahmed and Jason Liu and Stephan Eidenbenz and Joe Zerr},
   doi = {10.1109/HPCC-SmartCity-DSS.2016.0151},
   isbn = {9781509042968},
   journal = {Proceedings - 18th IEEE International Conference on High Performance Computing and Communications, 14th IEEE International Conference on Smart City and 2nd IEEE International Conference on Data Science and Systems, HPCC/SmartCity/DSS 2016},
   keywords = {High-performance computing,Interconnection network,Modeling and simulation,Performance evaluation},
   pages = {1069-1078},
   title = {Scalable interconnection network models for rapid performance prediction of HPC applications},
   url = {https://www.academia.edu/download/54474919/hpcc-2016-interconnect.pdf},
   year = {2016},
}
@article{Thesis2016,
   author = {Daniel Maag},
   doi = {10.3929/ethz-a-010740182},
   title = {Congestion-aware Simulation of Large-scale HPC Networks},
   url = {https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/155884/eth-49829-01.pdf},
   year = {2016},
}
@article{Goglin2014,
   author = {Brice Goglin and Joshua Hursey and Jeffrey M Squyres},
   title = {netloc : Towards a Comprehensive View of the HPC System},
   url = {https://hal.inria.fr/hal-01010599/document},
   year = {2014},
}
@article{Grobelny2007,
   abstract = {As systems of computers become more compleX in terms of their architecture, interconnect and heterogeneity, the optimum configuration and utilization of these machines becomes a major challenge. To reduce the penalties caused by poorly configured systems, simulation is often used to predict the performance of key applications to be eXecuted on the new systems. Simulation provides the capability to observe component and system characteristics (e.g. performance and power) in order to make vital design decisions. However, simulating high-fidelity models can be very time consuming and even prohibitive when evaluating large-scale systems. The Fast and Accurate Simulation Environment (FASE) framework seeks to support large-scale system simulation by using high-fidelity models to capture the behavior of only the performance-critical components while employing abstraction techniques to capture the effects of those components with little impact on the system. In order to achieve this balance of accuracy and simulation speed, FASE provides a methodology and associated toolset to evaluate numerous architectural options. This approach allows users to make system design decisions based on quantifiable demands of their key applications rather than using manual analysis which can be error prone and impractical for large systems. The framework accomplishes this evaluation through a novel approach of combining discrete-event simulation with an application characterization scheme in order to remove unnecessary details while focusing on components critical to the performance of the application. In this paper, we present the methodology and techniques behind FASE and include several case studies validating systems constructed using various applications and interconnects. © 2007, Sage Publications. All rights reserved.},
   author = {Eric Grobelny and David Bueno and Ian Troxel and Alan D. George and Jeffrey S. Vetter},
   doi = {10.1177/0037549707084939},
   isbn = {0037549707084},
   issn = {0037-5497},
   issue = {10},
   journal = {SIMULATION},
   keywords = {Performance prediction,application characterization,discrete-event simulation,high-performance computing},
   month = {10},
   pages = {721-745},
   title = {FASE: A Framework for Scalable Performance Prediction of HPC Systems and Applications},
   volume = {83},
   url = {https://journals.sagepub.com/doi/pdf/10.1177/0037549707084939?casa_token=LkFujLNp0qoAAAAA%3A76qMy4HFuL63TF1J73ZOpJVu6qtYZfz8UC8FWf6ZyTOuVvvmbXkP1hOqya4YlnvwTH0_xTyjleL2Qg& http://journals.sagepub.com/doi/10.1177/0037549707084939},
   year = {2007},
}
@article{Calotoiu2016a,
   abstract = {Tuning large applications requires a clever exploration of the design and configuration space. Especially on supercomputers, this space is so large that its exhaustive traversal via performance experiments becomes too expensive, if not impossible. Manually creating analytical performance models provides insights into optimization opportunities but is extremely laborious if done for applications of realistic size. If we must consider multiple performance-relevant parameters and their possible interactions, a common requirement, this task becomes even more complex. We build on previous work on automatic scalability modeling and significantly extend it to allow insightful modeling of any combination of application execution parameters. Multi-parameter modeling has so far been outside the reach of automatic methods due to the exponential growth of the model search space. We develop a new technique to traverse the search space rapidly and generate insightful performance models that enable a wide range of uses from performance predictions for balanced machine design to performance tuning.},
   author = {Alexandru Calotoiu and David Beckingsale and Christopher W. Earl and Torsten Hoefler and Ian Karlin and Martin Schulz and Felix Wolf},
   doi = {10.1109/CLUSTER.2016.57},
   isbn = {9781509036530},
   issn = {15525244},
   journal = {Proceedings - IEEE International Conference on Cluster Computing, ICCC},
   pages = {172-181},
   title = {Fast multi-parameter performance modeling},
   year = {2016},
}
@article{Curtis2010,
   abstract = {Efficient design of hardware and software for large-scale parallel execution requires detailed understanding of the interactions between the application, computer, and network. The authors have developed a macro-scale simulator (SST/macro) that permits the coarse-grained study of distributed-memory applications. In the presented work, applications using the Message Passing Interface (MPI) are simulated; however, the simulator is designed to allow inclusion of other programming models. The simulator is driven from either a trace file or a skeleton application. Trace files can be either a standard format (Open Trace Format) or a more detailed custom format (DUMPI). The simulator architecture is modular, allowing it to easily be extended with additional network models, trace file formats, and more detailed processor models. This paper describes the design of the simulator, provides performance results, and presents studies showing how application performance is affected by machine characteristics.},
   author = {Curtis L. Janssen and Helgi Adalsteinsson and Scott Cranford and Joseph P. Kenny and Ali Pinar and David A. Evensky and Jackson Mayo},
   doi = {10.4018/jdst.2010040104},
   issn = {1947-3532},
   issue = {2},
   journal = {International Journal of Distributed Systems and Technologies},
   month = {4},
   pages = {57-73},
   title = {A Simulator for Large-Scale Parallel Computer Architectures},
   volume = {1},
   url = {https://www.sandia.gov/~apinar/papers/macroscale_simulator_preprint.pdf http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/jdst.2010040104},
   year = {2010},
}
@article{Hendry,
   author = {Gilbert Hendry and Simon Hammond},
   title = {Coarse-Grain Simulation of Networks-on-Chip using SST / Macro},
   url = {https://www.osti.gov/servlets/purl/1143907},
   year = {2011},
}
@article{Janssen2011,
   abstract = {A key problem facing application developers is that they are expected to utilize extreme levels of parallelism soon after delivery of future leadership class machines, but developing applications capable of exposing sufficient concurrency is a time consuming process requiring experimentation. At the same time, due to the expense of building and operating an exascale machine, it will be necessary to apply tighter engineering margins to their design. Simple metrics such as the computation-communication ratio will not sufficiently specify machine requirements. Simulation fills this gap, allowing the study of extreme-scale architectures with the explicit inclusion of the complex interactions between the various hardware and software components, and can be used for correctness-checking as well as performance estimation. The simulator we discuss in this paper can be driven by reading trace files, typically generated by an actual application that has been run on real hardware, or by using a skeleton application. The skeleton application is designed to have the control flow of a real application, but with expensive computations and large data transfers replaced by discrete events for which the timings are determined by models. Using skeleton applications, we can predict application performance at levels of parallelism unobtainable on any current computational platform. The skeleton application can be modified to experiment with different communication strategies and programming models. Since the machine being simulated is in our control, we can experiment with different network topologies, routing algorithms, bandwidths, latencies, failure modes, core-to-node ratios, etc. In this paper, we use the Structural Simulation Toolkit macroscale components for coarse-grained simulation to illustrate the exploration of alternative programming models at extreme scale.},
   author = {Curtis L. Janssen and Helgi Adalsteinsson and Joseph P. Kenny},
   doi = {10.1145/1964218.1964220},
   issn = {0163-5999},
   issue = {4},
   journal = {ACM SIGMETRICS Performance Evaluation Review},
   pages = {4-8},
   title = {Using simulation to design extremescale applications and architectures},
   volume = {38},
   url = {https://dl.acm.org/doi/pdf/10.1145/1964218.1964220?casa_token=yJofz0fnZHAAAAAA:DLXNpKRGTEx4sj0nSE1UeSRPcDqe8zetfAAT8KIYMeuRBALgjMRyp-b5RifPYGxjOX1LF8E0mIHMIQ},
   year = {2011},
}
@article{Grun,
   author = {Paul Grun and Sean Hefty and Sayantan Sur and David Goodell and Robert D Russell and Howard Pritchard and Jeffrey M Squyres},
   journal = {Proceedings - 2015 IEEE 23rd Annual Symposium on High-Performance Interconnects, HOTI 2015},
   title = {A Brief Introduction to the OpenFabrics Interfaces - A New Network API for Maximizing High Performance Application Efficiency},
   url = {https://alln-extcloud-storage.cisco.com/ciscoblogs/libfabric-hoti-2015-final-submission.pdf},
   year = {2015},
}
@report{Javier2018,
   author = {Javier Cano and Guillermo T. Fernández and Francisco J. Alfaro and José L. Sánchez},
   title = {OpaSim: an OPA Simulator for High-Performance Interconnections},
   url = {https://www.dsi.uclm.es/descargas/technicalreports/DIAB-18-12-1/TechnicalReport.pdf},
   year = {2018},
}
@report{IBvol2,
   title = {Infiniband Architecture Specification Volume 2 Release 1.4},
   url = {https://cw.infinibandta.org/document/dl/8566},
   year = {2020},
}
@report{IBvol1,
   title = {Infiniband Architecture Specification Volume 1 Release 1.4},
   url = {https://cw.infinibandta.org/document/dl/8567},
   year = {2020},
}
@inproceedings{Rosales2017,
   abstract = {When a new technology is introduced into the HPC community, it is necessary to understand its performance and how it can affect the way applications interact with the hardware. Intel has recently introduced two new elements into the HPC ecosystem that are being widely adopted by many centers: Intel Omni-Path high performance network and Intel Knights Landing processor. While it is possible to find different studies that analyze the efficiency of the Knights Landing processor, it is not the same situation for Omni-Path, the new 100 Gb/s fabric from Intel. This paper presents a set of studies that investigate the effectiveness of system comprised of this processor and network. The outcomes of this work can be used as guidelines for a better exploitation of these resources on production systems. Also, the methodology employed during our tests can be replicated on a variety of systems and centers to find the ideal configurations of their hardware resources and provide users with recommendations that can improve the performance of their codes and the overall throughput of the clusters.},
   author = {Carlos Rosales and Antonio Gómez-Iglesias},
   city = {New York, NY, USA},
   doi = {10.1145/3093338.3093349},
   isbn = {9781450352727},
   journal = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
   keywords = {Characterization,Intel Knights Landing,Intel Omni-Path,Performance},
   month = {7},
   pages = {1-7},
   publisher = {ACM},
   title = {Evaluation of Intel Omni-Path on the Intel Knights Landing Processor},
   volume = {Part F1287},
   url = {https://dl.acm.org/doi/10.1145/3093338.3093349},
   year = {2017},
}
@article{Brightwell2005a,
   abstract = {The Portals data movement interface was developed at Sandia National Laboratories in collaboration with the University of New Mexico over the last ten years. Portals is intended to provide the functionality necessary to scale a distributed memory parallel computing system to thousands of nodes. Previous versions of Portals ran on several large-scale machines, including a 1024-node nCUBE-2, a 1800-node Intel Paragon, and the 4500-node Intel ASCI Red machine. The latest version of Portals was initially developed for an 1800-node Linux/Myrinet cluster and has since been adopted by Cray as the lowest-level network programming interface for their XT3 platform. In this paper, we describe the implementation of Portals 3.3 on the Cray XT3 and present some initial performance results from several micro-benchmark tests. Despite some limitations, the current implementation of Portals is able to achieve a zerolength one-way latency of under six microseconds and a uni-directional bandwidth of more than 1.1 GB/s.},
   author = {Ron Brightwell and Trammell Hudson and Kevin Pedretti and Rolf Riesen and Keith D. Underwood},
   doi = {10.1109/CLUSTR.2005.347061},
   isbn = {0780394852},
   issn = {15525244},
   journal = {Proceedings - IEEE International Conference on Cluster Computing, ICCC},
   title = {Implementation and performance of Portals 3.3 on the Cray XT3},
   year = {2005},
}
@thesis{Romdhanne2013,
   abstract = {The simulation is a primary step on the evaluation process of modern networked systems. The scalability and efficiency of such a tool in view of increasing complexity of the emerging networks is a key to derive valuable results. The discrete event simulation is recognized as the most scalable model that copes with both parallel and distributed architecture. Nevertheless, the recent hardware provides new heterogeneous computing resources that can be exploited in parallel.The main scope of this thesis is to provide a new mechanisms and optimizations that enable efficient and scalable parallel simulation using heterogeneous computing node architecture including multicore CPU and GPU. To address the efficiency, we propose to describe the events that only differs in their data as a single entry to reduce the event management cost. At the run time, the proposed hybrid scheduler will dispatch and inject the events on the most appropriate computing target based on the event descriptor and the current load obtained through a feedback mechanisms such that the hardware usage rate is maximized. Results have shown a significant gain of 100 times compared to traditional CPU based approaches. In order to increase the scalability of the system, we propose a new simulation model, denoted as general purpose coordinator-master-worker, to address jointly the challenge of distributed and parallel simulation at different levels. The performance of a distributed simulation that relies on the GP-CMW architecture tends toward the maximal theoretical efficiency in a homogeneous deployment. The scalability of such a simulation model is validated on the largest European GPU-based supercomputer},
   author = {Bilel Ben Romdhanne},
   institution = {Télécom ParisTech},
   pages = {188},
   title = {Large-scale network simulation over heterogeneous computing architecture},
   url = {https://pastel.archives-ouvertes.fr/tel-01151414/document},
   year = {2013},
}
@generic{sst_macro_doc,
   pages = {88},
   title = {SST/macro 10.0: User’s Manual},
   url = {https://raw.githubusercontent.com/sstsimulator/sst-macro/v10.0.0_beta/manual-sstmacro-10.0.pdf},
   year = {2020},
}
@inproceedings{Wang2018,
   abstract = {Dragonfly networks are being widely adopted in high-performance computing systems. On these networks, however, interference caused by resource sharing can lead to significant network congestion and performance variability. We present a comparative analysis exploring the trade-off between localizing communication and balancing network traffic. We conduct trace-based simulations for applications with different communication patterns, using multiple job placement policies and routing mechanisms. We perform an in-depth performance analysis on representative applications individually and show that different applications have distinct preferences regarding localized communication and balanced network traffic. We further demonstrate the effect of external network interference by introducing background traffic and show that localized communication can help reduce the application performance variation caused by network sharing.},
   author = {Xin Wang and Misbah Mubarak and Xu Yang and Robert B. Ross and Zhiling Lan},
   doi = {10.1109/IPDPS.2018.00120},
   isbn = {978-1-5386-4368-6},
   journal = {2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
   keywords = {Dragonfly networks,High-performance computing,Interference,Job placement},
   month = {5},
   pages = {1113-1122},
   publisher = {IEEE},
   title = {Trade-Off Study of Localizing Communication and Balancing Network Traffic on a Dragonfly System},
   url = {https://press3.mcs.anl.gov/codes/files/2018/03/intraStudy_v8.pdf https://ieeexplore.ieee.org/document/8425264/},
   year = {2018},
}
@inproceedings{Quinson2013,
   author = {Martin Quinson and Cristian Rosa and Christophe Thiéry},
   doi = {10.1109/CCGrid.2012.115},
   isbn = {978-1-4673-1395-7},
   journal = {2012 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (ccgrid 2012)},
   month = {5},
   pages = {668-675},
   publisher = {IEEE},
   title = {Parallel Simulation of Peer-to-Peer Systems},
   url = {https://hal.inria.fr/inria-00602216v4/document http://ieeexplore.ieee.org/document/6217480/},
   year = {2012},
}
@article{Varga2001,
   author = {Andras Varga},
   issue = {January 2001},
   journal = {European Simulation Multiconference (ESM'2001)},
   keywords = {computer systems,discrete simulation,hierarchical,performance analysis,telecommunications},
   title = {The OMNeT++ Discrete Event Simulation System},
   volume = {June},
   year = {2001},
}
@inproceedings{Bobelin2012,
   abstract = {Conducting experiments in large-scale distributed systems is usually time-consuming and labor-intensive. Uncontrolled external load variation prevents to reproduce experiments and such systems are often not available to the purpose of research experiments, e.g. production or yet to deploy systems. Hence, many researchers in the area of distributed computing rely on simulation to perform their studies. However, the simulation of large-scale computing systems raises several scalability issues, in terms of speed and memory. Indeed, such systems now comprise millions of hosts interconnected through a complex network and run billions of processes. Most simulators thus trade accuracy for speed and rely on very simple and easy to implement models. However, the assumptions underlying these models are often questionable, especially when it comes to network modeling. In this paper, we show that, despite a widespread belief in the community, achieving high scalability does not necessarily require to resort to overly simple models and ignore important phenomena. We show that relying on a modular and hierarchical platform representation, while taking advantage of regularity when possible, allows us to model systems such as data and computing centers, peer-to-peer networks, grids, or clouds in a scalable way. This approach has been integrated into the open-source SimGrid simulation toolkit. We show that our solution allows us to model such systems much more accurately than other state-of-the-art simulators without trading for simulation speed. SimGrid is even sometimes orders of magnitude faster. © 2012 IEEE.},
   author = {Laurent Bobelin and Arnaud Legrand and David A. González Márquez and Pierre Navarro and Martin Quinson and Frédéric Suter and Christophe Thiéry},
   doi = {10.1109/CCGrid.2012.31},
   isbn = {978-1-4673-1395-7},
   journal = {2012 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (ccgrid 2012)},
   keywords = {Grid computing,High-performance computing,Large-scale distributed systems,Peer-to-Peer,Simulation,Volunteer Computing},
   month = {5},
   pages = {220-227},
   publisher = {IEEE},
   title = {Scalable Multi-purpose Network Representation for Large Scale Distributed System Simulation},
   url = {https://hal.inria.fr/hal-00650233v2/document http://ieeexplore.ieee.org/document/6217425/},
   year = {2012},
}
@inproceedings{10.1007/978-3-319-27308-2_34,
   abstract = {This paper presents a preliminary evaluation of TraceR, a trace replay tool built upon the ROSS-based CODES simulation framework. TraceR can be used for predicting network performance and understanding network behavior by simulating messaging on interconnection networks. It addresses two major shortcomings in current network simulators. First, it enables fast and scalable simulations of large-scale supercomputer networks. Second, it can simulate production HPC applications using BigSim's emulation framework. In addition to introducing TraceR, this paper studies the impact of input parameters on simulation performance. We also compare TraceR with other network simulators such as SST and BigSim, and demonstrate TraceR 's scalability using various case studies.},
   author = {Bilge Acun and Nikhil Jain and Abhinav Bhatele and Misbah Mubarak and Christopher D Carothers and Laxmikant V Kale},
   city = {Cham},
   editor = {Sascha Hunold and Alexandru Costan and Domingo Giménez and Alexandru Iosup and Laura Ricci and María Engracia Gómez Requena and Vittorio Scarano and Ana Lucia Varbanescu and Stephen L Scott and Stefan Lankes and Josef Weidendorfer and Michael Alexander},
   isbn = {978-3-319-27308-2},
   journal = {Euro-Par 2015: Parallel Processing Workshops},
   pages = {417-429},
   publisher = {Springer International Publishing},
   title = {Preliminary Evaluation of a Parallel Trace Replay Tool for HPC Network Simulations},
   url = {https://link.springer.com/content/pdf/10.1007%2F978-3-319-27308-2_34.pdf},
   year = {2015},
}
@article{Cope2011,
   abstract = {Performance and reliability design constraints for exascale storage systems are significant challenges for HPC system designers. We are developing the CODES simulation toolkit to equip system designers with simulation tools so that they better understand the features and design constraints of exascale storage systems. The goal for CODES is to enable the exploration and co-design of exascale storage systems by providing a detailed, accurate, and highly parallel simulation toolkit for exascale storage. In this paper, we present the capabilities of the CODES tools that allow systems designers to assess exascale storage system designs. We demonstrate the use of CODES to evaluate a potential exascale storage network model and storage system features. \n},
   author = {Jason Cope and Ning Liu and Samuel Lang and Philip H Carns and Christopher D Carothers and Robert B Ross},
   journal = {Workshop on Emerging Supercomputing Technologies 2011 (WEST 2011)},
   pages = {303-312},
   title = {CODES: Enabling Co-Design of Multi-Layer Exascale Storage Architectures},
   url = {https://pdfs.semanticscholar.org/159d/bd0a8c18e2df895b131e33499e2d529210e0.pdf},
   year = {2011},
}
@thesis{Capra2015,
   abstract = {Afin de répondre aux besoins croissants de la simulation numérique et de rester à la pointe de la technologie, les supercalculateurs doivent d’être constamment améliorés. Ces améliorations peuvent être d’ordre matériel ou logiciel. Cela force les applications à s’adapter à un nouvel environnement de programmation au fil de son développement. Il devient alors nécessaire de se poser la question de la pérennité des applications et de leur portabilité d’une machine à une autre. L’utilisation de machines virtuelles peut être une première réponse à ce besoin de pérennisation en stabilisant les environnements de programmation. Grâce à la virtualisation, une application peut être développée au sein d’un environnement figé, sans être directement impactée par l’environnement présent sur une machine physique. Pour autant, l’abstraction supplémentaire induite par les machines virtuelles entraine en pratique une perte de performance. Nous proposons dans cette thèse un ensemble d’outils et de techniques afin de permettre l’utilisation de machines virtuelles en contexte HPC. Tout d’abord nous montrons qu’il est possible d’optimiser le fonctionnement d’un hyperviseur afin de répondre le plus fidèlement aux contraintes du HPC que sont : le placement des fils d’exécution et la localité mémoire des données. Puis en s’appuyant sur ce résultat, nous avons proposé un service de partitionnement des ressources d’un noeud de calcul par le biais des machines virtuelles. Enfin, pour étendre nos travaux à une utilisation pour des applications MPI, nous avons étudié les solutions et performances réseau d’une machine virtuelle.},
   author = {Antoine Capra},
   institution = {Université de Bordeaux},
   keywords = {Calcul haute performance,MPI,OpenMP,Virtualisation},
   title = {Virtualisation en contexte HPC},
   url = {https://tel.archives-ouvertes.fr/tel-01280434/document},
   year = {2015},
}
@inproceedings{Obaida2018,
   abstract = {Parallel application performance models provide valuable insight about the performance in real systems. Capable tools providing fast, accurate, and comprehensive prediction and evaluation of high-performance computing (HPC) applications and system architectures have important value. This paper presents PyPassT, an analysis based modeling framework built on static program analysis and integrated simulation of target HPC architectures. More specifically, the framework analyzes application source code written in C with OpenACC directives and transforms it into an application model describing its computation and communication behavior (including CPU and GPU workloads, memory accesses, and message-passing transactions). The application model is then executed on a simulated HPC architecture for performance analysis. Preliminary experiments demonstrate that the proposed framework can represent the runtime behavior of benchmark applications with good accuracy.},
   author = {Mohammad Abu Obaida and Jason Liu and Gopinath Chennupati and Nandakishore Santhi and Stephan Eidenbenz},
   city = {New York, NY, USA},
   doi = {10.1145/3200921.3200937},
   isbn = {9781450350921},
   journal = {Proceedings of the 2018 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
   keywords = {High-performance computing,Performance modeling,Performance prediction,Program analysis,Simulation},
   month = {5},
   pages = {49-59},
   publisher = {ACM},
   title = {Parallel Application Performance Prediction Using Analysis Based Models and HPC Simulations},
   url = {https://dl.acm.org/doi/10.1145/3200921.3200937},
   year = {2018},
}
@article{Faure2018,
   abstract = {La diversité des plateformes de calcul à haute performance ne fait qu’augmenter. Le gestion- naire de ressources et de tâches (ou RJMS pour Resources and Jobs Management Systems) est res- ponsable d’ordonnancer les tâches (applications souvent parallèles et distribuées) sur ce type de plateformes. Un ordonnancement mal maîtrisé peut dégrader significativement les perfor- mances des applications. Dans ce contexte, étudier et valider des RJMS ainsi que des algo- rithmes d’ordonnancement est un réel défi. Dans cet article nous présentons le logiciel Batsim, un simulateur d’infrastructure qui permet la simulation réaliste d’applications et l’étude de RJMS pour de nombreux problèmes. Nous validons notre approche en montrant que la prise en compte de la localité dans l’algorithme d’ordonnancement peut avoir un impact majeur sur les applications. Ce phénomène, ainsi que de nombreux autres, ne peuvent être observés qu’avec des modèles d’applications (et de plateformes) sensibles au contexte d’exécution.},
   author = {Adrien Faure and Millian Poquet and Olivier Richard},
   title = {Évaluation d’algorithmes d’ordonnancement par simulation réaliste},
   url = {https://hal.inria.fr/hal-01779936},
   year = {2018},
}
@inproceedings{Moy2013,
   abstract = {The SystemC/TLM technologies are widely accepted in the industry for fast system-level simulation. An important limitation of SystemC regarding performance is that the reference implementation is sequential, and the official semantics makes parallel executions difficult. As the number of cores in computers increase quickly, the ability to take advantage of the host parallelism during a simulation is becoming a major concern. Most existing work on parallelization of SystemC targets cycle-accurate simulation, and would be inefficient on loosely timed systems since they cannot run in parallel processes that do not execute simultaneously. We propose an approach that explicitly targets loosely timed systems, and offers the user a set of primitives to express tasks with duration, as opposed to the notion of time in SystemC which allows only instantaneous computations and time elapses without computation. Our tool exploits this notion of duration to run the simulation in parallel. It runs on top of any (unmodified) SystemC implementation, which lets legacy SystemC code continue running as-it-is. This allows the user to focus on the performance-critical parts of the program that need to be parallelized. © 2013 EDAA.},
   author = {Matthieu Moy},
   city = {New Jersey},
   doi = {10.7873/DATE.2013.017},
   isbn = {9781467350716},
   issn = {15301591},
   journal = {Design, Automation & Test in Europe Conference & Exhibition (DATE), 2013},
   pages = {9-14},
   publisher = {IEEE Conference Publications},
   title = {Parallel Programming with SystemC for Loosely Timed Models: A Non-Intrusive Approach},
   url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6513463},
   year = {2013},
}
@article{Ani2020,
   abstract = {Simulation can provide a useful means to understand issues linked to industrial network operations. For transparent, collaborative, cost-effective solutions development, and to attract the broadest interest base, simulation is critical and open source suggested, because it costs less to access, install, and use. This study contributes new insights from security and functionality characteristics metrics to underscore the use and effectiveness of open source simulators. Several open source simulators span applications in communications and wireless sensor networks, industrial control systems, and the Industrial Internet of Things. Some drivers for their use span are as follows: supported license types; programming languages; operating systems platforms; user interface types; documentation and communication types; citations; code commits; and number of contributors. Research in these simulators is built around performance and optimization relative to flexibility, scalability, mobility, and active user support. No single simulator addresses all these conceivable characteristics. In addition to modeling contexts that match real-world scenarios and issues, an effective open source simulator needs to demonstrate credibility, which can be gained partly through actively engaging experts from interdisciplinary teams along with user contributions integrated under tight editorial controls. Government-led policies and regulations are also necessary to support their wider awareness and more productive use for real-world purposes.},
   author = {Uchenna Daniel Ani and Jeremy McKendrick Watson and Madeline Carr and Al Cook and Jason R.C. Nurse},
   doi = {10.1177/1548512920953499},
   isbn = {1548512920953},
   issn = {1548-5129},
   journal = {The Journal of Defense Modeling and Simulation: Applications, Methodology, Technology},
   keywords = {Industrial Internet of Things simulation,Open source simulators,industrial control system simulations,open source tools,security simulations,system simulations},
   month = {9},
   pages = {154851292095349},
   title = {A review of the use and utility of industrial network-based open source simulators: functionality, security, and policy viewpoints},
   url = {http://journals.sagepub.com/doi/10.1177/1548512920953499},
   year = {2020},
}
@thesis{Hamayun2013,
   author = {Mian Muhammad Hamayun},
   isbn = {9782111291799},
   title = {Simulation Native des Systèmes Multiprocesseurs sur Puce à l ’ aide de la Virtualisation Assistée par le Matériel},
   url = {https://tel.archives-ouvertes.fr/tel-00877962v1/document},
   year = {2013},
}
@article{Zimmer2019,
   abstract = {The US Department of Energy deployed the Summit and Sierra supercomputers with the latest state-of-the-art network interconnect technology in 2018 and both systems entered production in 2019. In this paper, we provide an in-depth assessment of the systems' network interconnects that are based on Enhanced Data Rate (EDR) 100 Gb/s Mellanox InfiniBand. Both systems use second-generation EDR Host Channel Adapters (HCAs) and switches with several new features such as Adaptive Routing (AR), switch-based collectives, and HCA-based tag matching. Although based on the same components, Summit's network is "non-blocking" (i.e., a fully provisioned Clos network) and Sierra's network has a 2:1 taper between the racks and aggregation switches. We evaluate the two systems' interconnects using traditional communication benchmarks as well as production applications. We find that the new Adaptive Routing dramatically improves performance but the other new features still need improvement.},
   author = {Christopher Zimmer and Scott Atchley and Ramesh Pankajakshan and Brian E. Smith and Ian Karlin and Matthew L. Leininger and Adam Bertsch and Brian S. Ryujin and Jason Burmark and André Walker-Loud and M. A. Clark and Olga Pearce},
   doi = {10.1145/3295500.3356166},
   isbn = {9781450362290},
   issn = {21674337},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   keywords = {Bandwidth,Congestion,EDR,High performance computing,InfiniBand,Interconnect,Latency,Offload,Switch collectives,Tag matching},
   title = {An evaluation of the CORAL interconnects},
   year = {2019},
}
@article{Ramaswamy2018,
   abstract = {With extremely large design spaces for algorithm and architecture to be explored, there is a need for fast and scalable performance modeling tools for preparing HPC application codes. Behavioral Emulation (BE) is a recent coarse-grained modeling and simulation methodology that has been proposed to solve this co-design problem. In this paper, we introduce a distributed parallel simulation library for Behavioral Emulation called BE-SST, integrated into the Structural Simulation Toolkit (SST). BE-SST provides simple interfaces and framework for development of coarse-grained BE models which can be extended to model new notional architectures. BE-SST also supports Monte Carlo simulations to generate meaningful distributions and summary statistics rather than a single datum for performance. In this paper, we present BE-SST simulations of two existing large DOE machines (Vulcan and Titan), which have been validated against actual testbed measurements and showed 5-10% error. These validated system models (up to 128k cores) are used to make blind predictions of application performance on systems larger than the current machines (up to 512k cores) - a crucial simulator feature for design-space exploration of notional systems. We further studied BE-SST in terms of scalability and performance, simulating up to a million cores, with BE-SST running on more than 2k parallel processes. BE-SST shows good scalability with a linear increase in memory usage and simulation time with increase in simulated system size, and a peak speedup of 7x over single process simulation. With ease of use and good scaling, we assert that BE-SST can significantly speed up design-space exploration.},
   author = {Ajay Ramaswamy and Nalini Kumar and Aravind Neelakantan and Herman Lam and Greg Stitt},
   doi = {10.1145/3225058.3225124},
   isbn = {9781450365109},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Co-design,Performance modeling,Simulation,Virtual prototyping},
   title = {Scalable behavioral emulation of extreme-scale systems using structural simulation toolkit},
   year = {2018},
}
@article{DiGirolamo2019,
   abstract = {Applications often communicate data that is non-contiguous in the send- or the receive-buffer, e.g., when exchanging a column of a matrix stored in row-major order. While non-contiguous transfers are well supported in HPC (e.g., MPI derived datatypes), they can still be up to 5x slower than contiguous transfers of the same size. As we enter the era of network acceleration, we need to investigate which tasks to offload to the NIC: In this work we argue that non-contiguous memory transfers can be transparently network-accelerated, truly achieving zero-copy communications. We implement and extend sPIN, a packet streaming processor, within a Portals 4 NIC SST model, and evaluate strategies for NIC-offloaded processing of MPI datatypes, ranging from datatype-specific handlers to general solutions for any MPI datatype. We demonstrate up to 8x speedup in the unpack throughput of real applications, demonstrating that non-contiguous memory transfers are a first-class candidate for network acceleration.},
   author = {Salvatore Di Girolamo and Konstantin Taranov and Andreas Kurth and Michael Schaffner and Timo Schneider and Jakub Beránek and MacIej Besta and Luca Benini and Duncan Roweth and Torsten Hoefler},
   doi = {10.1145/3295500.3356189},
   isbn = {9781450362290},
   issn = {21674337},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   title = {Network-accelerated non-contiguous memory transfers},
   year = {2019},
}
@inproceedings{Emmanuel2020a,
  TITLE = {{Simulation of the Portals 4 protocol, and case study on the BXI interconnect}},
  AUTHOR = {Emmanuel, Julien and Moy, Matthieu and Henrio, Ludovic and Pichon, Gregoire},
  URL = {https://hal.science/hal-02972297},
  BOOKTITLE = {{HPCS 2020 - International Conference on High Performance Computing \& Simulation}},
  ADDRESS = {Barcelona, Spain},
  PAGES = {1-8},
  YEAR = {2020},
  MONTH = Dec,
  KEYWORDS = {Simulation ; SimGrid ; HPC ; Interconnect ; Portals 4 ; BXI},
  PDF = {https://hal.science/hal-02972297/file/HPCS-2020.pdf},
  HAL_ID = {hal-02972297},
  HAL_VERSION = {v1},
}

@article{Oodendijk2020,
   abstract = {Background: YouTube™ and Dropbox© studies have warned against the deadly potential of push-scooters. Aims: Through three studies, we evaluate the potential of a combination of hydroxychloroquine and azithromycin for preventing push-scooter accidents. Study Design: Studies 1 and 2 are retrospective observational studies in which we rely on archival data to explore the relationship between push-scooters accidents (PSA) and usage of hydroxychloroquine plus azithromycin (HCQ + AZT) in France in 2020 and 2019 respectively (7 participants). Study 3 is a partly randomized clinical trial (pRCT), retrospective, in which the use of HCQ + AZT for preventing PSA was assessed in a radically "direct" way (6 participants). Place and Duration of Study: Studies 1 and 2 were conducted in the authors' office chair (Ikea) in France (multicentric), on July 20th, 2020. Study 3 was conducted in the parking lot of an abandoned factory (Montcuq, Occitan region, France). Methodology: For Studies 1 and 2, we used data from OpenMEDIC to determine usage of hydroxychloroquine in France in 2020 and Google Actuality to determine the rate of PSA in France in 2020. For Study 3, we adopted an experimental approach and had participants exposed to HCQ + AZ (treatment group) or homeopathy (control group) before having them perform a standard push-scooter exercise. Advanced statistical models were used to assess the prophylactic effect of the HCQ + AZT combination on PSA. Results: Wide use of hydroxychloroquine is strongly associated with a very low level of PSA, both in time (2020 VS 2019) and in space (Marseille, Bouches-du-Rhônes versus the rest of France). Moreover, the results of our retrospective pRCT prove without any doubt that prophylactic use of a HCQ + AZT combination helped to prevent PSA. Conclusion: The HCQ + AZT combo should urgently be used in prevention of PSA all around the world.},
   author = {Willard Oodendijk and Michaël Rochoy and Valentin Ruggeri and Florian Cova and Didier Lembrouille and Sylvano Trottinetta and Otter F Hantome and Nemo Macron and Manis Javanica and Pedro Kouri and Ahmed Abdullah Khalleefah},
   doi = {10.9734/AJMAH/2020/v18i930232},
   issn = {24568414},
   issue = {August},
   journal = {Asian Journal of Medicine and Health},
   keywords = {COVID-19,Hydroxychloroquine,azithromycin,motion sickness,push-scooters ACRONYMS HCQ : Hydroxychloroquine AZ,soup,zinc},
   pages = {14-21},
   title = {SARS-CoV-2 was Unexpectedly Deadlier than Push-scooters: Could Hydroxychloroquine be the Unique Solution?},
   url = {https://www.journalajmah.com/index.php/AJMAH/article/view/30232},
   year = {2020},
}
@book_section{Kumar2016,
   author = {Nalini Kumar and Carlo Pascoe and Christopher Hajas and Herman Lam and Greg Stitt and Alan George},
   doi = {10.1007/978-3-319-46079-6_1},
   isbn = {9783319460789},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {behavioral emulation,coarse-grained simulation,design space exploration,performance},
   pages = {5-17},
   title = {Behavioral Emulation for Scalable Design-Space Exploration of Algorithms and Architectures},
   volume = {9945 LNCS},
   url = {http://link.springer.com/10.1007/978-3-319-46079-6_1},
   year = {2016},
}
@inproceedings{Gropp1995,
   abstract = {We describe an architecture for the runtime environment for parallel applications as prelude to describing how parallel application might interface to their environment in a portable way. We propose extensions to the Message-Passing Interface (MPI) Standard that provide for dynamic process management, including spawning of new processes by a running application and connection to existing processes to support client/server applications. Such extensions are needed if more of the runtime environment for parallel programs is to be accessible to MPI programs or to be themselves written using MPI. The extensions proposed here are motivated by real applications and fit cleanly with existing concepts of MPI. No changes to the existing MPI Standard are proposed, thus all present MPI programs will run unchanged.},
   author = {W. Gropp and E. Lusk},
   doi = {10.1109/SPDP.1995.530729},
   isbn = {0-8186-7195-5},
   issn = {10636374},
   journal = {Proceedings.Seventh IEEE Symposium on Parallel and Distributed Processing},
   pages = {530-533},
   publisher = {IEEE Comput. Soc. Press},
   title = {Dynamic process management in an MPI setting},
   url = {http://ieeexplore.ieee.org/document/530729/ http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.9191&rep=rep1&type=pdf},
   year = {1995},
}
@book_section{Mietke2006,
   abstract = {To leverage high speed interconnects like InfiniBand it is important to minimize the communication overhead. The most interfering overhead is the registration of communication memory. In this paper, we present our analysis of the memory registration process inside the Mellanox InfiniBand driver and possible ways out of this bottleneck. We evaluate and characterize the most time consuming parts in the execution path of the memory registration function using the Read Time Stamp Counter (RDTSC) instruction. We present measurements on AMD Opteron and Intel Xeon systems with different types of Host Channel Adapters for PCI-X and PCI-Express. Finally, we conclude with first results using Linux hugepage support to shorten the time of registering a memory region. © Springer-Verlag Berlin Heidelberg 2006.},
   author = {Frank Mietke and Robert Rex and Robert Baumgartl and Torsten Mehlan and Torsten Hoefler and Wolfgang Rehm},
   doi = {10.1007/11823285_13},
   isbn = {3540377832},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {124-133},
   title = {Analysis of the Memory Registration Process in the Mellanox InfiniBand Software Stack},
   volume = {4128 LNCS},
   url = {http://link.springer.com/10.1007/11823285_13},
   year = {2006},
}
@article{Lazarev2020,
   abstract = {Cloud applications are increasingly relying on hundreds of loosely-coupled microservices to complete user requests that meet an application's end-to-end QoS requirements. Communication time between services accounts for a large fraction of the end-to-end latency and can introduce performance unpredictability and QoS violations. This letter presents our early work on Dagger, a hardware acceleration platform for networking, designed specifically with the unique qualities of microservices in mind. The Dagger architecture relies on an FPGA-based NIC, closely coupled with the processor over a configurable memory interconnect, designed to offload and accelerate RPC stacks. Unlike the traditional cloud systems that use PCIe links as the NIC I/O interface, we leverage memory-interconnected FPGAs as networking devices to provide the efficiency, transparency, and programmability needed for fine-grained microservices. We show that this considerably improves CPU utilization and performance for cloud RPCs.},
   author = {Nikita Lazarev and Neil Adit and Shaojie Xiang and Zhiru Zhang and Christina Delimitrou},
   doi = {10.1109/LCA.2020.3020064},
   issn = {15566064},
   issue = {2},
   journal = {IEEE Computer Architecture Letters},
   keywords = {FPGA,Microservices,RPC,memory interconnects,near-memory processing,programmable NICs},
   month = {7},
   pages = {134-138},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Dagger: Towards Efficient RPCs in Cloud Microservices with Near-Memory Reconfigurable NICs},
   volume = {19},
   year = {2020},
}
@inproceedings{Rajovic2016,
   abstract = {High-performance computing (HPC) is recognized as one of the pillars for further progress in science, industry, medicine, and education. Current HPC systems are being developed to overcome emerging architectural challenges in order to reach Exascale level of performance, projected for the year 2020. The much larger embedded and mobile market allows for rapid development of intellectual property (IP) blocks and provides more flexibility in designing an application-specific system-on-chip (SoC), in turn providing the possibility in balancing performance, energy-efficiency, and cost. In the Mont-Blanc project, we advocate for HPC systems being built from such commodity IP blocks, currently used in embedded and mobile SoCs. As a first demonstrator of such an approach, we present the Mont-Blanc prototype; the first HPC system built with commodity SoCs, memories, and network interface cards (NICs) from the embedded and mobile domain, and off-the-shelf HPC networking, storage, cooling, and integration solutions. We present the system's architecture and evaluate both performance and energy efficiency. Further, we compare the system's abilities against a production level supercomputer. At the end, we discuss parallel scalability and estimate the maximum scalability point of this approach across a set of applications.},
   author = {Nikola Rajovic and Alejandro Rico and Filippo Mantovani and Daniel Ruiz and Josep Oriol Vilarrubi and Constantino Gomez and Luna Backes and Diego Nieto and Harald Servat and Xavier Martorell and Jesus Labarta and Eduard Ayguade and Chris Adeniyi-Jones and Said Derradji and Herve Gloaguen and Piero Lanucara and Nico Sanna and Jean Francois Mehaut and Kevin Pouget and Brice Videau and Eric Boyer and Momme Allalen and Axel Auweter and David Brayford and Daniele Tafani and Volker Weinberg and Dirk Brommel and Rene Halver and Jan H. Meinke and Ramon Beivide and Mariano Benito and Enrique Vallejo and Mateo Valero and Alex Ramirez},
   doi = {10.1109/SC.2016.37},
   isbn = {9781467388153},
   issn = {21674337},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   month = {7},
   pages = {444-455},
   publisher = {IEEE Computer Society},
   title = {The Mont-Blanc Prototype: An Alternative Approach for HPC Systems},
   volume = {0},
   year = {2016},
}
@inproceedings{Lee2016,
   abstract = {InfiniBand is the de facto networking technology for commodity HPC clusters and has been widely deployed. However, most production large-scale InfiniBand clusters use simple routing schemes such as the destination-mod-k routing to route traffic, which may result in degraded communication performance. In this work, we investigate using the OpenFlow-style Software-Defined Networking (SDN) technology to overcome the routing deficiency in InfiniBand. We design an enhanced InfiniBand with OpenFlow-style SDN capability and demonstrate a use case that illustrates how the SDN capability can be exploited in HPC clusters to improve the system and application performance. Finally, we quantify the potential benefits of InfiniBand with OpenFlow-style SDN capability in balancing the network load by simulating job traces from production HPC clusters. The results indicate that InfiniBand with SDN capability can achieve much better network load balancing than traditional InfiniBand for HPC clusters.},
   author = {Jason Lee and Zhou Tong and Karthik Achalkar and Xin Yuan and Michael Lang},
   doi = {10.1109/SC.2016.35},
   isbn = {9781467388153},
   issn = {21674337},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   keywords = {Fat Tree,High Performance Computing,InfiniBand,OpenFlow,Simulation,Software Defined Networking},
   month = {7},
   pages = {421-432},
   publisher = {IEEE Computer Society},
   title = {Enhancing InfiniBand with OpenFlow-Style SDN Capability},
   volume = {0},
   year = {2016},
}
@inproceedings{Raffenetti2017,
   abstract = {This paper provides an in-depth analysis of the software overheads in the MPI performance-critical path and exposes mandatory performance overheads that are unavoidable based on the MPI-3.1 speciication. We irst present a highly optimized implementation of the MPI-3.1 standard in which the communication stackDall the way from the application to the low-level network communication APIDtakes only a few tens of instructions. We carefully study these instructions and analyze the root cause of the overheads based on speciic requirements from the MPI standard that are unavoidable under the current MPI standard. We recommend potential changes to the MPI standard that can minimize these overheads. Our experimental results on a variety of network architectures and applications demonstrate signiicant beneits from our proposed changes.},
   author = {Ken Raffenetti and Abdelhalim Amer and Lena Oden and Charles Archer and Wesley Bland and Hajime Fujita and Yanfei Guo and Tomislav Janjusic and Dmitry Durnov and Michael Blocksome and Min Si and Sangmin Seo and Akhil Langer and Gengbin Zheng and Masamichi Takagi and Paul Coffman and Jithin Jose and Sayantan Sur and Alexander Sannikov and Sergey Oblomov and Michael Chuvelev and Masayuki Hatanaka and Xin Zhao and Paul Fischer and Thilina Rathnayake and Matt Otten and Misun Min and Pavan Balaji},
   city = {New York, NY, USA},
   doi = {10.1145/3126908.3126963},
   isbn = {9781450351140},
   journal = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
   month = {11},
   pages = {1-12},
   publisher = {ACM},
   title = {Why is MPI so slow?},
   url = {https://dl.acm.org/doi/10.1145/3126908.3126963},
   year = {2017},
}
@inproceedings{Xiong2020,
   abstract = {MPI collective operations can often be performance killers in HPC applications; we seek to solve this bottleneck by offloading them to reconfigurable hardware within the switch itself, rather than, e.g., the NIC. We have designed a hardware accelerator MPI-FPGA to implement six MPI collectives in the network. Preliminary results show that MPI-FPGA achieves 10 × speedup in the most likely scenarios over conventional clusters. We introduce a novel mechanism that enables the hardware to support a large number of communicators of arbitrary shape, and that is scalable to very large systems. MPI-FPGA is fully integrated into MPICH and so transparent to MPI applications.},
   author = {Qingqing Xiong and Chen Yang and Pouya Haghi and Anthony Skjellum and Martin Herbordt},
   doi = {10.1109/FCCM48280.2020.00046},
   isbn = {9781728158037},
   journal = {Proceedings - 28th IEEE International Symposium on Field-Programmable Custom Computing Machines, FCCM 2020},
   month = {5},
   pages = {215},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Accelerating MPI Collectives with FPGAs in the Network and Novel Communicator Support},
   year = {2020},
}
@inproceedings{Gropp2016,
   abstract = {The "postal" model of communication [3, 8] T = a + ßn, for sending n bytes of data between two processes with latency a and bandwidth 1/ß, is perhaps the most commonly used communication performance model in parallel computing. This performance model is often used in developing and evaluating parallel algorithms in high-performance computing, and was an effective model when it was first proposed. Consequently, numerous tests of "ping pong" communication have been developed in order to measure these parameters in the model. However, with the advent of multicore nodes connected to a single (or a few) network interfaces, the model has become a poor match to modern hardware. In this paper, we show a simple three-parameter model that better captures the behavior of current parallel computing systems, and demonstrate its accuracy on several systems. In support of this model, which we call the max-rate model, we have developed an open source benchmark1 that can be used to determine the model parameters.},
   author = {William Gropp and Luke N. Olson and Philipp Samfass},
   city = {New York, NY, USA},
   doi = {10.1145/2966884.2966919},
   isbn = {9781450342346},
   journal = {Proceedings of the 23rd European MPI Users' Group Meeting},
   keywords = {Bandwidth saturation,Benchmark,Communication,Multicore,Parallel computing,Performance model,Ping pong,Symmetric multiprocessor cluster},
   month = {9},
   pages = {41-50},
   publisher = {ACM},
   title = {Modeling MPI Communication Performance on SMP Nodes},
   volume = {25-28-Sept},
   url = {https://dl.acm.org/doi/10.1145/2966884.2966919},
   year = {2016},
}
@inproceedings{,
   abstract = {Applications often communicate data that is non-contiguous in the send- or the receive-buffer, e.g., when exchanging a column of a matrix stored in row-major order. While non-contiguous transfers are well supported in HPC (e.g., MPI derived datatypes), they can still be up to 5x slower than contiguous transfers of the same size. As we enter the era of network acceleration, we need to investigate which tasks to offload to the NIC: In this work we argue that non-contiguous memory transfers can be transparently network-accelerated, truly achieving zero-copy communications. We implement and extend sPIN, a packet streaming processor, within a Portals 4 NIC SST model, and evaluate strategies for NIC-offloaded processing of MPI datatypes, ranging from datatype-specific handlers to general solutions for any MPI datatype. We demonstrate up to 8x speedup in the unpack throughput of real applications, demonstrating that non-contiguous memory transfers are a first-class candidate for network acceleration.},
   author = {Salvatore Di Girolamo and Konstantin Taranov and Andreas Kurth and Michael Schaffner and Timo Schneider and Jakub Beránek and MacIej Besta and Luca Benini and Duncan Roweth and Torsten Hoefler},
   doi = {10.1145/3295500.3356189},
   isbn = {9781450362290},
   issn = {21674337},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   month = {11},
   publisher = {IEEE Computer Society},
   title = {Network-accelerated non-contiguous memory transfers},
   year = {2019},
}
@inproceedings{Zimmer2019,
   abstract = {The US Department of Energy deployed the Summit and Sierra supercomputers with the latest state-of-the-art network interconnect technology in 2018 and both systems entered production in 2019. In this paper, we provide an in-depth assessment of the systems' network interconnects that are based on Enhanced Data Rate (EDR) 100 Gb/s Mellanox InfiniBand. Both systems use second-generation EDR Host Channel Adapters (HCAs) and switches with several new features such as Adaptive Routing (AR), switch-based collectives, and HCA-based tag matching. Although based on the same components, Summit's network is "non-blocking" (i.e., a fully provisioned Clos network) and Sierra's network has a 2:1 taper between the racks and aggregation switches. We evaluate the two systems' interconnects using traditional communication benchmarks as well as production applications. We find that the new Adaptive Routing dramatically improves performance but the other new features still need improvement.},
   author = {Christopher Zimmer and Scott Atchley and Ramesh Pankajakshan and Brian E. Smith and Ian Karlin and Matthew L. Leininger and Adam Bertsch and Brian S. Ryujin and Jason Burmark and André Walker-Loud and M. A. Clark and Olga Pearce},
   doi = {10.1145/3295500.3356166},
   isbn = {9781450362290},
   issn = {21674337},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   keywords = {Bandwidth,Congestion,EDR,High performance computing,InfiniBand,Interconnect,Latency,Offload,Switch collectives,Tag matching},
   month = {11},
   publisher = {IEEE Computer Society},
   title = {An evaluation of the CORAL interconnects},
   year = {2019},
}
@inproceedings{DeMatteis2019,
   abstract = {Distributed memory programming is the established paradigm used in high-performance computing (HPC) systems, requiring explicit communication between nodes and devices. When FPGAs are deployed in distributed settings, communication is typically handled either by going through the host machine, sacrificing performance, or by streaming across fixed device-to-device connections, sacrificing flexibility. We present Streaming Message Interface (SMI), a communication model and API that unifies explicit message passing with a hardware-oriented programming model, facilitating minimal-overhead, flexible, and productive inter-FPGA communication. Instead of bulk transmission, messages are streamed across the network during computation, allowing communication to be seamlessly integrated into pipelined designs. We present a high-level synthesis implementation of SMI targeting a dedicated FPGA interconnect, exposing runtime-configurable routing with support for arbitrary network topologies, and implement a set of distributed memory benchmarks. Using SMI, programmers can implement distributed, scalable HPC programs on reconfigurable hardware, without deviating from best practices for hardware design.},
   author = {Tiziano De Matteis and Johannes de Fine Licht and Jakub Beránek and Torsten Hoefler},
   city = {New York, NY, USA},
   doi = {10.1145/3295500.3356201},
   isbn = {9781450362290},
   issn = {21674337},
   journal = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
   keywords = {Distributed memory programming,High-level synthesis tools,Reconfigurable computing},
   month = {11},
   pages = {1-33},
   publisher = {ACM},
   title = {Streaming message interface},
   url = {https://dl.acm.org/doi/10.1145/3295500.3356201},
   year = {2019},
}
@article{Do2021,
   abstract = {The emergence of Big Data in recent years has resulted in a growing need for efficient data processing solutions. While infrastructures with sufficient compute power are available, the I/O bottleneck remains. The Linux page cache is an efficient approach to reduce I/O overheads, but few experimental studies of its interactions with Big Data applications exist, partly due to limitations of real-world experiments. Simulation is a popular approach to address these issues, however, existing simulation frameworks do not simulate page caching fully, or even at all. As a result, simulation-based performance studies of data-intensive applications lead to inaccurate results. In this paper, we propose an I/O simulation model that includes the key features of the Linux page cache. We have implemented this model as part of the WRENCH workflow simulation framework, which itself builds on the popular SimGrid distributed systems simulation framework. Our model and its implementation enable the simulation of both single-threaded and multithreaded applications, and of both writeback and writethrough caches for local or network-based filesystems. We evaluate the accuracy of our model in different conditions, including sequential and concurrent applications, as well as local and remote I/Os. We find that our page cache model reduces the simulation error by up to an order of magnitude when compared to state-of-the-art, cacheless simulations.},
   author = {Hoang-Dung Do and Valerie Hayot-Sasson and Rafael Ferreira da Silva and Christopher Steele and Henri Casanova and Tristan Glatard},
   month = {1},
   title = {Modeling the Linux page cache for accurate simulation of data-intensive applications},
   url = {http://arxiv.org/abs/2101.01335},
   year = {2021},
}
@report{Bessad2015,
   author = {Louisa Bessad and Martin Quinson},
   title = {Real-time online emulation of real applications on SimGrid with Simterpose},
   url = {https://people.irisa.fr/Martin.Quinson/Research/Students/2015-M2R-simterpose-rapport.pdf},
   year = {2015},
}
@article{Do2021a,
   abstract = {The emergence of Big Data in recent years has resulted in a growing need for efficient data processing solutions. While infrastructures with sufficient compute power are available, the I/O bottleneck remains. The Linux page cache is an efficient approach to reduce I/O overheads, but few experimental studies of its interactions with Big Data applications exist, partly due to limitations of real-world experiments. Simulation is a popular approach to address these issues, however, existing simulation frameworks do not simulate page caching fully, or even at all. As a result, simulation-based performance studies of data-intensive applications lead to inaccurate results. In this paper, we propose an I/O simulation model that includes the key features of the Linux page cache. We have implemented this model as part of the WRENCH workflow simulation framework, which itself builds on the popular SimGrid distributed systems simulation framework. Our model and its implementation enable the simulation of both single-threaded and multithreaded applications, and of both writeback and writethrough caches for local or network-based filesystems. We evaluate the accuracy of our model in different conditions, including sequential and concurrent applications, as well as local and remote I/Os. We find that our page cache model reduces the simulation error by up to an order of magnitude when compared to state-of-the-art, cacheless simulations.},
   author = {Hoang-Dung Do and Valerie Hayot-Sasson and Rafael Ferreira da Silva and Christopher Steele and Henri Casanova and Tristan Glatard},
   title = {Modeling the Linux page cache for accurate simulation of data-intensive applications},
   url = {http://arxiv.org/abs/2101.01335},
   year = {2021},
}
@thesis{Faure2020,
   abstract = {High-Performance Computing (HPC) provides the computational power dedicatedto solving complex problems of our society. HPC computers are large scale anddistributed infrastructures composed of several thousands of computing cores. Themanagement of theses systems is left to unique software: the Resource and JobManagement System (RJMS). The objective of the RJMS is multiple: Managing thephysical infrastructure, and handling the user requests to access to the computingpower. The scheduling algorithm is the cornerstone of the RJMS, it decides whereand when the user’s jobs will be executed. Scheduling is a difficult problem; to man-age large scale platforms RJMS needs to dispose of efficient yet scalable schedulingheuristics Evaluating and testing new scheduling algorithms is crucial before releas-ing it in production. Any failure can have a dramatic impact on the HPC platformleading to wasted time, energy, and resources. The lack of a platform dedicatedexperiments and tests compels RJMS designers and HPC center’s administrators touse different tools and methodologies to evaluate new algorithms.In the first part of this dissertation, we present and evaluate a new scheduling heuris-tics with job redirection. The evaluation is done using a large simulation campaign,it results that by redirecting jobs can improve the efficiency of the scheduling. Inthe second part, we focus on and extend the tools and methodologies available toexperiment with RJMS. This part is twofold: Firstly, we propose to extend schedulingsimulations with job models to simulate network contention between jobs. Secondly,we propose new tools that enable experiment with production RJMS without theneed for an HPC platform. This dissertation aims to broaden the experimentallandscape of tools and methodologies to experiment with RJMS and therefore helpthe release in the production of new scheduling algorithms.},
   author = {Adrien Faure},
   institution = {Université Grenoble Alpes},
   title = {Advanced Simulation for Resource Management},
   url = {https://tel.archives-ouvertes.fr/tel-03155702/document},
   year = {2020},
}
@inproceedings{DeMatteis2019a,
   abstract = {Distributed memory programming is the established paradigm used in high-performance computing (HPC) systems, requiring explicit communication between nodes and devices. When FPGAs are deployed in distributed settings, communication is typically handled either by going through the host machine, sacrificing performance, or by streaming across fixed device-to-device connections, sacrificing flexibility. We present Streaming Message Interface (SMI), a communication model and API that unifies explicit message passing with a hardware-oriented programming model, facilitating minimal-overhead, flexible, and productive inter-FPGA communication. Instead of bulk transmission, messages are streamed across the network during computation, allowing communication to be seamlessly integrated into pipelined designs. We present a high-level synthesis implementation of SMI targeting a dedicated FPGA interconnect, exposing runtime-configurable routing with support for arbitrary network topologies, and implement a set of distributed memory benchmarks. Using SMI, programmers can implement distributed, scalable HPC programs on reconfigurable hardware, without deviating from best practices for hardware design.},
   author = {Tiziano De Matteis and Johannes de Fine Licht and Jakub Beránek and Torsten Hoefler},
   city = {New York, NY, USA},
   doi = {10.1145/3295500.3356201},
   isbn = {9781450362290},
   issn = {21674337},
   journal = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
   keywords = {Distributed memory programming,High-level synthesis tools,Reconfigurable computing},
   month = {11},
   pages = {1-33},
   publisher = {ACM},
   title = {Streaming message interface},
   url = {https://dl.acm.org/doi/10.1145/3295500.3356201},
   year = {2019},
}
@article{Brief2007,
   author = {Connectx Architecture Brief and Industry-leading Integration and Flexible Layered Architecture},
   title = {4th Generation Server & Storage Adapter Architecture},
   year = {2007},
}
@article{,
   author = {Antonio Morán Muñoz and Jesús Escudero-sahuquillo and Pedro Javier and García García and Francisco J Quiles Flor and Samuel Rodrigo-mocholí},
   keywords = {arbiter,fpga,netfpga-sume,simulation,verilog,weighted round-robin},
   title = {Modeling Weighted Round-Robin Switch Arbitration in NetFPGA-SUME switches},
}
@inproceedings{Margolin2019,
   abstract = {In most MPI implementations, abstraction layers separate the collective operation algorithms from the communication primitives, thus hindering its optimization with network acceleration technologies, such as RDMA. Open UCX is an RDMA-based point-ot-point communication library, that can reduce the latency between processes in MPI applications, particularly in large-scale system. This paper presents a design and implementation of a library for MPI collective operations, by extending Open UCX. Our approach is transparent to MPI applications, and can reduce the latency of repeated calls to such operations by an average of 8% for relatively small message sizes and as much as 90% for larger messages.},
   author = {Alexander Margolin and Amnon Barak},
   doi = {10.1109/ExaMPI49596.2019.00010},
   isbn = {978-1-7281-6009-2},
   journal = {2019 IEEE/ACM Workshop on Exascale MPI (ExaMPI)},
   month = {11},
   pages = {39-46},
   publisher = {IEEE},
   title = {RDMA-Based Library for Collective Operations in MPI},
   url = {https://ieeexplore.ieee.org/document/8955451/},
   year = {2019},
}
@inproceedings{Vicino2015,
   abstract = {Parallel Discrete Event System Specification (PDEVS) is a well-known formalism used to model and simulate Discrete Event Systems. This formalism uses an abstract simulator that defines a set of abstract algorithms that are parallel by nature. To implement simulators using these abstract algorithms, several architectures were proposed. Most of these architectures follow distributed approaches that may not be appropriate for single core processors or microcontrollers. In order to reuse efficiently PDEVS models in this type of systems, we define a new architecture that provides a single threaded execution by passing messages in a call/return fashion to simplify the execution time analysis.},
   author = {Damián Vicino and Daniella Niyonkuru and Gabriel Wainer and Olivier Dalle},
   issn = {07359276},
   issue = {8},
   journal = {Proceedings of 2015 SCS/ACM/IEEE Symposium on Theory of Modeling and Simulation},
   keywords = {Architecture,PDEVS,Sequential,Simulator},
   pages = {165-172},
   title = {Sequential PDEVS architecture},
   volume = {47},
   url = {http://cell-devs.sce.carleton.ca/publications/2015/VNWD15/SequentialPDEVSArchitecture.pdf},
   year = {2015},
}
@article{Hemmert2010,
   abstract = {Efficient collective operations are a major component of application scalability. Offload of collective operations onto the network interface reduces many of the latencies that are inherent in network communications and, consequently, reduces the time to perform the collective operation. To support offload, it is desirable to expose semantic building blocks that are simple to offload and yet powerful enough to implement a variety of collective algorithms. This paper presents the implementation of barrier and broadcast leveraging triggered operations - a semantic building block for collective offload. Triggered operations are shown to be both semantically powerful and capable of improving performance. © 2010 Springer-Verlag.},
   author = {K. Scott Hemmert and Brian Barrett and Keith D. Underwood},
   doi = {10.1007/978-3-642-15646-5_26},
   isbn = {3642156452},
   issn = {03029743},
   issue = {August 2014},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {249-256},
   title = {Using triggered operations to offload collective communication operations},
   volume = {6305 LNCS},
   year = {2010},
}
@report{Karlin2012,
   author = {Ian Karlin},
   city = {Livermore, CA (United States)},
   doi = {10.2172/1059462},
   isbn = {LLNL-TR-608824},
   issue = {LLNL-TR-608824},
   institution = {Lawrence Livermore National Laboratory (LLNL)},
   month = {12},
   pages = {1-17},
   title = {LULESH Programming Model and Performance Ports Overview},
   url = {https://asc.llnl.gov/sites/asc/files/2021-01/lulesh_ports1.pdf},
   year = {2012},
}
@article{Murphy2004,
   abstract = {Modern supercomputers involve complex interactions be-tween complex components crossing tens of clock domains — per node. Thousands (and soon hundreds of thousands) of nodes are used to achieve the level of performance needed to address the nation's most challenging problems. This growing complexity makes system design ever more chal-lenging; however, microprocessors are facing mounting chal-lenges in delivering performance on scientific applications. This has led to a drive for innovation in systems where the impact of innovation is hard to predict. The Structural Sim-ulation Toolkit (SST) was developed to explore innovations in systems where the processor and memory interact with the programming model and communications system. It is designed to be an open source framework that unifies hybrid discrete event simulation and time-stepping simulation to enable both detailed and abstracted simulations. The focus on parallel systems has been validated through experimen-tation with models of everything from processing in memory to conventional microprocessors connected by conventional network interfaces.},
   author = {Richard Murphy and Keith Underwood and Fitzpatrick Hall and Notre Dame},
   title = {The Structural Simulation Toolkit : A Tool for Bridging the Architectural / Microarchitectural Evaluation Gap},
   url = {http://sst.sandia.gov/files/SAND2004-6238c.pdf},
   year = {2004},
}
@article{Rodrigues2011,
   abstract = {As supercomputers grow, understanding their behavior and performance has become increasingly challenging. New hurdles in scalability, programmability, power consumption, reliability, cost, and cooling are emerging, along with new technologies such as 3D integration, GP-GPUs, silicon-photonics, and other "game changers". Currently, they HPC community lacks a unified toolset to evaluate these technologies and design for these challenges.},
   author = {A. F. Rodrigues and K. S. Hemmert and B. W. Barrett and C. Kersey and R. Oldfield and M. Weston and R. Risen and J. Cook and P. Rosenfeld and E. Cooper-Balis and B. Jacob},
   doi = {10.1145/1964218.1964225},
   issn = {0163-5999},
   issue = {4},
   journal = {ACM SIGMETRICS Performance Evaluation Review},
   keywords = {architecture,performance analysis,simulation,sst},
   month = {3},
   pages = {37-42},
   title = {The structural simulation toolkit},
   volume = {38},
   url = {https://dl.acm.org/doi/10.1145/1964218.1964225},
   year = {2011},
}
@inproceedings{Chapman2010,
   abstract = {The OpenSHMEM community would like to announce a new effort to standardize SHMEM, a communications library that uses one-sided communication and utilizes a partitioned global address space. OpenSHMEM is an effort to bring together a variety of SHMEM and SHMEM-like implementations into an open standard using a community-driven model. By creating an open-source specification and reference implementation of OpenSHMEM, there will be a wider availability of a PGAS library model on current and future architectures. In addition, the availability of an OpenSHMEM model will enable the development of performance and validation tools. We propose an OpenSHMEM specification to help tie together a number of divergent implementations of SHMEM that are currently available. To support an existing and growing user community, we will develop the OpenSHMEM web presence, including a community wiki and training material, and face-to-face interaction, including workshops and conference participation. © 2010 ACM.},
   author = {Barbara Chapman and Tony Curtis and Swaroop Pophale and Stephen Poole and Jeff Kuehn and Chuck Koelbel and Lauren Smith},
   city = {New York, New York, USA},
   doi = {10.1145/2020373.2020375},
   isbn = {9781450304610},
   issue = {October},
   journal = {Proceedings of the Fourth Conference on Partitioned Global Address Space Programming Model - PGAS '10},
   keywords = {ACM proceedings,OpenSHMEM,PGAS,SHMEM},
   pages = {1-3},
   publisher = {ACM Press},
   title = {Introducing OpenSHMEM},
   url = {http://dl.acm.org/citation.cfm?doid=2020373.2020375},
   year = {2010},
}
@article{Levenhagen,
   author = {M J Levenhagen and S D Hammond and K S Hemmert},
   keywords = {performance analysis,shmem,simulation},
   title = {Towards Lightweight and Scalable Simulation of Large-Scale OpenSHMEM Applications},
}
@article{Baranov2021,
   author = {A. V. Baranov and D. S. Lyakhovets},
   doi = {10.1134/S199508022111007X},
   issn = {1995-0802},
   issue = {11},
   journal = {Lobachevskii Journal of Mathematics},
   month = {11},
   pages = {2510-2519},
   title = {Accuracy Comparison of Various Supercomputer Job Management System Models},
   volume = {42},
   url = {https://link.springer.com/10.1134/S199508022111007X},
   year = {2021},
}
@thesis{Cornebize2021,
   abstract = {The scientific community relies more and more on computations, notably for nu- merical simulation and data processing. While many scientific advances were made possible by the technological progress of computers, additional performance gains are still required for larger scale projects. The race for performance is addressed with a growing hardware and software complexity, which in turn increases the performance variability. This can make the experimental study of performance extremely challenging, raising concerns of reproducibility of the experiments, akin to the problems already faced by natural sciences. Our contributions are twofold. First, we present a methodology for predicting the performance of parallel non-trivial applications through simulation. We describe several models for communications and computations, with an increasing complex- ity. We compare these models through an extensive validation by matching our predictions with real experiments. This validation shows that modeling the spatial and temporal variability of the platform is essential for faithful predictions. As a consequence, predictions require careful sensibility analysis accounting for the uncertainty on the resource models, which we illustrate through several case studies. Second, we present the lessons learned while making the numerous experiments required in the first part and how we improved our methodology. We show that mea- surements can suffer from multiple experimental biases and we explain how some of these biases can be overcome. We also present how we implemented systematic performance non-regression testing, which allowed us to detect many significant changes of the platform throughout this thesis.},
   author = {Tom Cornebize},
   keywords = {Measure,Modeling,Performance,Prediction,Simulation,Test},
   title = {High Performance Computing: Towards Better Performance Predictions and Experiments},
   url = {https://tel.archives-ouvertes.fr/tel-03328956},
   year = {2021},
}
@article{Meswani2012,
   author = {Mitesh R Meswani and Laura Carrington and Allan Snavely and Stephen W Poole},
   journal = {Proceedings Cray user group conference (CUG)},
   title = {Tools for benchmarking, tracing, and simulating SHMEM applications},
   year = {2012},
}
@report{Sips2009,
   abstract = {The size of supercomputers in numbers of processors is growing exponentially. Today's largest supercomputers have upwards of a hundred thousand processors and tomorrow's may have on the order one million. The applications that run on these systems commonly coordinate their parallel activities via MPI; a trace of these MPI communication events is an important input for tools that visualize , simulate, or enable tuning of parallel applications. We introduce an efficient , accurate and flexible trace-driven performance modeling and prediction tool, PMaC's Open Source Interconnect and Network Simulator (PSINS), for MPI applications. A principal feature of PSINS is its usability for applications that scale up to large processor counts. PSINS generates compact and tractable event traces for fast and efficient simulations while producing accurate performance predictions. It also allows researchers to easily plug in different event trace formats and communication models, allowing it to interface gracefully with other tools. This provides a flexible framework for collaboratively exploring the implications of constantly growing supercomputers on application scaling, in the context of network architectures and topologies of state-of-the-art and future planned large-scale systems.},
   author = {H Sips and D Epema and H.-X Lin and Mustafa M Tikir and Michael A Laurenzano and Laura Carrington and Allan Snavely},
   journal = {LNCS},
   keywords = {High Performance Computing,Message Passing Applications,Performance Prediction,Trace-Driven Simulation,and Supercomputers},
   pages = {135-148},
   title = {PSINS: An Open Source Event Tracer and Execution Simulator for MPI Applications},
   volume = {5704},
   year = {2009},
}
@report{OSB,
   abstract = {The assessment of application performance is a fundamental task in high-performance computing (HPC). The OpenSHMEM Benchmark (OSB) suite is a collection of micro-benchmarks and mini-applications/compute kernels that have been ported to use OpenSHMEM. Some, like the NPB OpenSHMEM benchmarks , have been published before while most others have been used for evaluations but never formally introduced or discussed. This suite puts them together and is useful for assessing the performance of different use cases of OpenSH-MEM. This offers system implementers a useful means of measuring performance and assessing the effects of new features as well as implementation strategies. The suite is also useful for application developers to assess the performance of the growing number of OpenSHMEM implementations that are emerging. In this paper, we describe the current set of codes available within the OSB suite, how they are intended to be used, and, where possible, a snapshot of their behavior on one of the OpenSHMEM implementations available to us. We also include detailed descriptions of every benchmark and kernel, focusing on how Open-SHMEM was used. This includes details on the enhancements we made to the benchmarks to support multithreaded variants. We encourage the OpenSHMEM community to use, review, and provide feedback on the benchmarks.},
   author = {Thomas Naughton and Ferrol Aderholdt and Matt Baker and Swaroop Pophale and Manjunath Gorentla Venkata and Neena Imam},
   title = {Oak Ridge OpenSHMEM Benchmark Suite},
   url = {https://www.osti.gov/servlets/purl/1558563},
}
@report{,
   abstract = {This document is a collaborative effort consisting of several releases of OpenSHMEM versions 1.0 through 1.5. This section lists the authors and contributors in reverse chronological order, starting with OpenSHMEM 1.5.},
   author = {Matthew Baker and Swen Boehm and Aurelien Bouteiller and Bob Cernohous and Hewlett Packard Enterprise and James Culhane and Tony Curtis and James Dinan and Mike Dubman and Anshuman Goswami and Bryant Lam and Akhil Langer and John Linford and Arm Inc and Jens Manser and Tiffany M Mintz and David Ozog and Nicholas Park and Steve Poole and Wendy Poole and Swaroop Pophale and Sreeram Potluri and Howard Pritchard and Md Wasi-ur-Rahman and Naveen Ravichandrasekaran and Michael Raymond and James Ross},
   title = {OpenSHMEM 1.5},
   url = {http://www.defense.gov/http://www.ornl.gov/http://www.lanl.gov/},
}
@book_section{Pritchard2011,
   abstract = {Recent versions of MPICH2 have featured Nemesis-a scalable, high-performance, multi-network communication subsystem. Nemesis provides a framework for developing Network Modules (Netmods) for interfacing the Nemesis subsystem to various high speed network protocols. Cray has developed a User-Level Generic Network Interface (uGNI) for interfacing MPI implementations to the internal high speed network of Cray XE and follow-on compute systems. This paper describes the design of a uGNI Netmod for the MPICH2 nemesis subsystem. MPICH2 performance data on the Cray XE will be presented. Planned future enhancements to the uGNI MPICH2 Netmod will also be discussed.},
   author = {Howard Pritchard and Igor Gorodetsky and Darius Buntinas},
   doi = {10.1007/978-3-642-24449-0_14},
   pages = {110-119},
   title = {A uGNI-Based MPICH2 Nemesis Network Module for the Cray XE Computer Systems},
   url = {http://link.springer.com/10.1007/978-3-642-24449-0_14},
   year = {2011},
}
@inproceedings{Emmanuel2021,
  TITLE = {{S4BXI: the MPI-ready Portals 4 Simulator}},
  AUTHOR = {Emmanuel, Julien and Moy, Matthieu and Henrio, Ludovic and Pichon, Gregoire},
  URL = {https://hal.inria.fr/hal-03366573},
  BOOKTITLE = {{MASCOTS 2021 - 29th IEEE International Symposium on the Modeling, Analysis, and Simulation of Computer and Telecommunication Systems}},
  ADDRESS = {Houston, United States},
  PUBLISHER = {{IEEE}},
  PAGES = {1-8},
  YEAR = {2021},
  MONTH = Nov,
  DOI = {10.1109/MASCOTS53633.2021.9614285},
  KEYWORDS = {Simulation ; SimGrid ; HPC ; MPI ; Interconnect ; Portals 4 ; BXI},
  PDF = {https://hal.inria.fr/hal-03366573/file/s4bxi-the-mpi-ready-portals-4-simulator.pdf},
  HAL_ID = {hal-03366573},
  HAL_VERSION = {v1},
}

@article{Cook1983,
   abstract = {An historical overview of computational complexity is presented. Emphasis is on the fundamental issues of defining the intrinsic computational complexity of a problem and proving upper and lower bounds on the complexity of problems. Probabilistic and parallel computation are discussed.},
   author = {Stephen A. Cook},
   doi = {10.1145/358141.358144},
   issn = {0001-0782},
   issue = {6},
   journal = {Communications of the ACM},
   month = {6},
   pages = {400-408},
   title = {An overview of computational complexity},
   volume = {26},
   url = {https://dl.acm.org/doi/10.1145/358141.358144},
   year = {1983},
}
@article{Turing1937,
   author = {A. M. Turing},
   doi = {10.1112/plms/s2-42.1.230},
   issn = {00246115},
   issue = {1},
   journal = {Proceedings of the London Mathematical Society},
   pages = {230-265},
   title = {On Computable Numbers, with an Application to the Entscheidungsproblem},
   volume = {s2-42},
   url = {http://doi.wiley.com/10.1112/plms/s2-42.1.230},
   year = {1937},
}
@article{Ferreira2018,
   author = {Kurt B. Ferreira and Scott Levy and Kevin Pedretti and Ryan E. Grant},
   doi = {10.1016/j.parco.2018.05.005},
   issn = {01678191},
   journal = {Parallel Computing},
   keywords = {Collaborative environments,Empirical studies,Secure software development},
   month = {9},
   pages = {57-83},
   publisher = {IEEE Computer Society},
   title = {Characterizing MPI matching via trace-based simulation},
   volume = {77},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0167819118301467},
   year = {2018},
}
@inproceedings{Levy2019,
   abstract = {Although its demise has been frequently predicted, the Message Passing Interface (MPI) remains the dominant programming model for scientific applications running on high-performance computing (HPC) systems. MPI specifies powerful semantics for interprocess communication that have enabled scientists to write applications for simulating important physical phenomena. However, these semantics have also presented several significant challenges. For example, the existence of wildcard values has made the efficient enforcement of MPI message matching semantics challenging. Significant research has been dedicated to accelerating MPI message matching. One common approach has been to offload matching to dedicated hardware. One of the challenges that hardware designers have faced is knowing how to size hardware structures to accommodate outstanding match requests. Applications that exceed the capacity of specialized hardware typically must fall back to storing match requests in bulk memory, e.g. DRAM on the host processor. In this paper, we examine the implications of hardware matching and develop guidance on sizing hardware matching structure to strike a balance between minimizing expensive dedicated hardware resources and overall matching performance. By examining the message matching behavior of several important HPC workloads, we show that when specialized hardware matching is not dramatically faster than matching in memory the offload hardware's match queue capacity can be reduced without significantly increasing match time. On the other hand, effectively exploiting the benefits of very fast specialized matching hardware requires sufficient storage resources to ensure that every search completes in the specialized hardware. The data and analysis in this paper provide important guidance for designers of MPI message matching hardware.},
   author = {Scott Levy and Kurt B. Ferreira},
   city = {New York, New York, USA},
   doi = {10.1145/3343211.3343223},
   isbn = {9781450371759},
   journal = {Proceedings of the 26th European MPI Users' Group Meeting on   - EuroMPI '19},
   month = {9},
   pages = {1-11},
   publisher = {ACM Press},
   title = {Evaluating tradeoffs between MPI message matching offload hardware capacity and performance},
   url = {http://dl.acm.org/citation.cfm?doid=3343211.3343223},
   year = {2019},
}
@report{Forum2021,
   author = {Message Passing Interface Forum},
   month = {6},
   title = {MPI: A Message-Passing Interface Standard},
   url = {https://lists.mpi-forum.org/mailman/listinfo/mpi-comments},
   year = {2021},
}
@book_section{Graham2006,
   abstract = {A large number of MPI implementations are currently available , each of which emphasize different aspects of high-performance computing or are intended to solve a specific research problem. The result is a myriad of incompatible MPI implementations, all of which require separate installation, and the combination of which present significant logisti-cal challenges for end users. Building upon prior research, and influenced by experience gained from the code bases of the LAM/MPI, LA-MPI, FT-MPI, and PACX-MPI projects, Open MPI is an all-new, production-quality MPI-2 implementation that is fundamentally centered around component concepts. Open MPI provides a unique combination of novel features previously unavailable in an open-source, production-quality implementation of MPI. Its component architecture provides both a stable platform for third-party research as well as enabling the run-time composition of independent software add-ons. This paper presents a high-level overview the goals, design, and implementation of Open MPI, as well as performance results for it's point-to-point implementation.},
   author = {Richard L. Graham and Timothy S. Woodall and Jeffrey M. Squyres},
   doi = {10.1007/11752578_29},
   pages = {228-239},
   title = {Open MPI: A Flexible High Performance MPI},
   url = {http://link.springer.com/10.1007/11752578_29},
   year = {2006},
}
@techreport{Carter1975,
address = {Los Alamos, NM},
author = {Carter, L.L. and Cashwell, E.D.},
booktitle = {Development},
doi = {10.2172/4167844},
institution = {Los Alamos National Laboratory (LANL)},
isbn = {0870790218},
month = {jan},
title = {{Particle-transport simulation with the Monte Carlo method}},
url = {http://www.osti.gov/servlets/purl/4167844-hO0xdX/},
year = {1975}
}
@techreport{Hornung2011,
address = {Livermore, CA (United States)},
author = {Hornung, R. and Keasler, J. and Gokhale, M.},
booktitle = {October},
doi = {10.2172/1117905},
institution = {Lawrence Livermore National Laboratory (LLNL)},
month = {jun},
number = {9},
pages = {2011--2011},
title = {{Hydrodynamics challenge problem}},
url = {https://www.osti.gov/servlets/purl/1117905/},
year = {2011}
}
@inproceedings{Karlin2013,
abstract = {Parallel machines are becoming more complex with increasing core counts and more heterogeneous architectures. However, the commonly used parallel programming models, C/C++ with MPI and/or OpenMP, make it difficult to write source code that is easily tuned for many targets. Newer language approaches attempt to ease this burden by providing optimization features such as automatic load balancing, overlap of computation and communication, message-driven execution, and implicit data layout optimizations. In this paper, we compare several implementations of LULESH, a proxy application for shock hydrodynamics, to determine strengths and weaknesses of different programming models for parallel computation. We focus on four traditional (OpenMP, MPI, MPI+OpenMP, CUDA) and four emerging (Chapel, Charm++, Liszt, Loci) programming models. In evaluating these models, we focus on programmer productivity, performance and ease of applying optimizations. {\textcopyright} 2013 IEEE.},
author = {Karlin, Ian and Bhatele, Abhinav and Keasler, Jeff and Chamberlain, Bradford L. and Cohen, Jonathan and Devito, Zachary and Haque, Riyaz and Laney, Dan and Luke, Edward and Wang, Felix and Richards, David and Schulz, Martin and Still, Charles H.},
booktitle = {2013 IEEE 27th International Symposium on Parallel and Distributed Processing},
doi = {10.1109/IPDPS.2013.115},
isbn = {978-1-4673-6066-1},
keywords = {co-design,parallel programming models,performance,productivity,proxy application},
month = {may},
pages = {919--932},
publisher = {IEEE},
title = {{Exploring Traditional and Emerging Parallel Programming Models Using a Proxy Application}},
url = {http://ieeexplore.ieee.org/document/6569874/},
year = {2013}
}
@article{VanZee2015,
abstract = {The BLAS-like Library Instantiation Software (BLIS) framework is a new infrastructure for rapidly instantiating Basic Linear Algebra Subprograms (BLAS) functionality. Its fundamental innovation is that virtually all computation within level-2 (matrix-vector) and level-3 (matrix-matrix) BLAS operations can be expressed and optimized in terms of very simple kernels. While others have had similar insights, BLIS reduces the necessary kernels to what we believe is the simplest set that still supports the high performance that the computational science community demands. Higher-level framework code is generalized and implemented in ISO C99 so that it can be reused and/or reparameterized for different operations (and different architectures) with little to no modification. Inserting high-performance kernels into the framework facilitates the immediate optimization of any BLAS-like operations which are cast in terms of these kernels, and thus the framework acts as a productivity multiplier. Users of BLAS-dependent applications are given a choice of using the traditional Fortran-77 BLAS interface, a generalized C interface, or any other higher level interface that builds upon this latter API. Preliminary performance of level-2 and level-3 operations is observed to be competitive with two mature open source libraries (OpenBLAS and ATLAS) as well as an established commercial product (Intel MKL).},
author = {{Van Zee}, Field G. and van de Geijn, Robert A.},
doi = {10.1145/2764454},
issn = {0098-3500},
journal = {ACM Transactions on Mathematical Software},
keywords = {Body sensor network,Inertial navigation,Motion capture,Particle filter},
month = {jun},
number = {3},
pages = {1--33},
title = {{BLIS: A Framework for Rapidly Instantiating BLAS Functionality}},
url = {https://dl.acm.org/doi/10.1145/2764454},
volume = {41},
year = {2015}
}
@article{Lawson1979,
abstract = {A package of 38 low level subprograms for many of the basic operations of numerical linear algebra m presented. The package is intended to be used with Fortran. The operations m the package include dot product, elementary vector operation, Givens transformation, vector copy and swap, vector norm, vector scaling, and the determination of the index of the vector component of largest magnitude. The subprograms and a test driver are avadable in portable Fortran. Versions of the subprograms are also provided in assembly language for the IBM 360/67, the CDC 6600 and CDC 7600, and the Univac 1108. {\textcopyright} 1979, ACM. All rights reserved.},
author = {Lawson, C. L. and Hanson, R. J. and Kincaid, D. R. and Krogh, F. T.},
doi = {10.1145/355841.355847},
issn = {0098-3500},
journal = {ACM Transactions on Mathematical Software},
keywords = {ACM Trans,Basic Linear Algebra Subprograms for Fortran Usage,Math,Software},
month = {sep},
number = {3},
pages = {308--323},
title = {{Basic Linear Algebra Subprograms for Fortran Usage}},
url = {https://dl.acm.org/doi/10.1145/355841.355847},
volume = {5},
year = {1979}
}
@article{Ferenbaugh2015,
abstract = {HPCTOOLKIT is an integrated suite of tools that supports measurement, analysis, attribution, and presentation of application performance for both sequential and parallel programs. HPCTOOLKIT can pinpoint and quantify scalability bottlenecks in fully optimized parallel programs with a measurement overhead of only a few percent. Recently, new capabilities were added to HPCTOOLKIT for collecting call path profiles for fully optimized codes without any compiler support, pinpointing and quantifying bottlenecks in multithreaded programs, exploring performance information and source code using a new user interface, and displaying hierarchical space-time diagrams based on traces of asynchronous call path samples. This paper provides an overview of HPCTOOLKIT and illustrates its utility for performance analysis of parallel applications. Copyright {\textcopyright} 2009 John Wiley & Sons, Ltd.},
author = {Ferenbaugh, Charles R.},
doi = {10.1002/cpe.3422},
issn = {15320626},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {Binary analysis,Call path profiling,Execution monitoring,Performance tools,Tracing},
month = {dec},
number = {17},
pages = {4555--4572},
title = {{PENNANT: an unstructured mesh mini-app for advanced architecture research}},
url = {https://onlinelibrary.wiley.com/doi/10.1002/cpe.3422},
volume = {27},
year = {2015}
}
@article{Ajima2009,
author = {Ajima, Yuichiro and Sumimoto, Shinji and Shimizu, Toshiyuki},
doi = {10.1109/MC.2009.370},
file = {:home/julien/Downloads/tofu-a-6d-mesh-torus-interconnect-for-exascale-computers.pdf:pdf},
issn = {0018-9162},
journal = {Computer},
month = {nov},
number = {11},
pages = {36--40},
title = {{Tofu: A 6D Mesh/Torus Interconnect for Exascale Computers}},
url = {http://ieeexplore.ieee.org/document/5331902/},
volume = {42},
year = {2009}
}
@article{DeSensi2020,
abstract = {The interconnect is one of the most critical components in large scale computing systems, and its impact on the performance of applications is going to increase with the system size. In this paper, we will describe Slingshot, an interconnection network for large scale computing systems. Slingshot is based on high-radix switches, which allow building exascale and hyperscale datacenters networks with at most three switch-to-switch hops. Moreover, Slingshot provides efficient adaptive routing and congestion control algorithms, and highly tunable traffic classes. Slingshot uses an optimized Ethernet protocol, which allows it to be interoperable with standard Ethernet devices while providing high performance to HPC applications. We analyze the extent to which Slingshot provides these features, evaluating it on microbenchmarks and on several applications from the datacenter and AI worlds, as well as on HPC applications. We find that applications running on Slingshot are less affected by congestion compared to previous generation networks.},
archivePrefix = {arXiv},
arxivId = {2008.08886},
author = {{De Sensi}, Daniele and {Di Girolamo}, Salvatore and McMahon, Kim H. and Roweth, Duncan and Hoefler, Torsten},
doi = {10.1109/SC41405.2020.00039},
eprint = {2008.08886},
isbn = {9781728199986},
issn = {21674337},
journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
keywords = {congestion,datacenters,dragonfly,exascale,interconnection network},
month = {aug},
title = {{An In-Depth Analysis of the Slingshot Interconnect}},
url = {http://arxiv.org/abs/2008.08886},
volume = {2020-Novem},
year = {2020}
}
@inproceedings{Grospellier2009,
abstract = {In this paper, we introduce the Arcane software development framework for 2D and 3D numerical simulation codes. First, we describe the Arcane core, the mesh management and the parallelism strategy. Then, we focus on the concepts introduced to speed up the development of numerical codes: numerical modules, variables, entry points and services. We explain the execution model and enumerate the available debugging tools. Finally, the main functionalities of Arcane are described through an example. As a conclusion, we present the future works. Copyright 2009 ACM.},
address = {New York, New York, USA},
author = {Grospellier, Gilles and Lelandais, Benoit},
booktitle = {Proceedings of the 8th workshop on Parallel/High-Performance Object-Oriented Scientific Computing - POOSC '09},
doi = {10.1145/1595655.1595659},
isbn = {9781605585475},
keywords = {Arcane,Framework,Mesh,Numerical simulation,Parallelism,Performance,Software},
pages = {1--11},
publisher = {ACM Press},
title = {{The Arcane development framework}},
url = {http://portal.acm.org/citation.cfm?doid=1595655.1595659},
year = {2009}
}
@inproceedings{Enmyren2010,
abstract = {We present SkePU, a C++ template library which provides a simple and unified interface for specifying data-parallel computations with the help of skeletons on GPUs using CUDA and OpenCL. The interface is also general enough to support other architectures, and SkePU implements both a sequential CPU and a parallel OpenMP backend. It also supports multi-GPU systems. Copying data between the host and the GPU device memory can be a performance bottleneck. A key technique in SkePU is the implementation of lazy memory copying in the container type used to represent skeleton operands, which allows to avoid unnecessary memory transfers. We evaluate SkePU with small benchmarks and a larger application, a Runge-Kutta ODE solver. The results show that a skeleton approach to GPU programming is viable, especially when the computation burden is large compared to memory I/O (the lazy memory copying can help to achieve this). It also shows that utilizing several GPUs have a potential for performance gains. We see that SkePU offers good performance with a more complex and realistic task such as ODE solving, with up to 10 times faster run times when using SkePU with a GPU backend compared to a sequential solver running on a fast CPU. {\textcopyright} 2010 ACM.},
address = {New York, New York, USA},
author = {Enmyren, Johan and Kessler, Christoph W.},
booktitle = {Proceedings of the fourth international workshop on High-level parallel programming and applications - HLPP '10},
doi = {10.1145/1863482.1863487},
isbn = {9781450302548},
keywords = {cuda,data parallelism,gpu,opencl,skeleton programming},
pages = {5},
publisher = {ACM Press},
title = {{SkePU}},
url = {http://portal.acm.org/citation.cfm?doid=1863482.1863487},
year = {2010}
}
@article{Kale1993,
abstract = {We describe Charm++, an object oriented portable parallel programming language based on Cff. Its design philosophy, implementation, sample applications and their performance on various parallel machines are described. Charm++ is an explicitly parallel language consisting of Cft with a few extensions. It provides a clear separation between sequential and parallel objects. The execution model of Charm++ is message driven, thus helping one write programs that are latencytolerant. The language supports multiple inheritance, dynamic binding, overloading, strong typing, and reuse for parallel objects. Charm++ provides specific modes for sharing information between parallel objects. Extensive dynamic load balancing strategies are provided. It is based on the Charm parallel programming system, and its runtime system implementation reuses most of the runtime system for Charm. {\textcopyright} 1993, ACM. All rights reserved.},
author = {Kale, Laxmikant V. and Krishnan, Sanjeev},
doi = {10.1145/167962.165874},
issn = {0362-1340},
journal = {ACM SIGPLAN Notices},
month = {oct},
number = {10},
pages = {91--108},
title = {{CHARM++}},
url = {https://dl.acm.org/doi/10.1145/167962.165874},
volume = {28},
year = {1993}
}
@article{Augonnet2011,
abstract = {In the field of HPC, the current hardware trend is to design multiprocessor architectures featuring heterogeneous technologies such as specialized coprocessors (e.g. Cell/BE) or data-parallel accelerators (e.g. GPUs). Approaching the theoretical performance of these architectures is a complex issue. Indeed, substantial efforts have already been devoted to efficiently offload parts of the computations. However, designing an execution model that unifies all computing units and associated embedded memory remains a main challenge. We therefore designed StarPU, an original runtime system providing a high-level, unified execution model tightly coupled with an expressive data management library. The main goal of StarPU is to provide numerical kernel designers with a convenient way to generate parallel tasks over heterogeneous hardware on the one hand, and easily develop and tune powerful scheduling algorithms on the other hand. We have developed several strategies that can be selected seamlessly at run-time, and we have analyzed their efficiency on several algorithms running simultaneously over multiple cores and a GPU. In addition to substantial improvements regarding execution times, we have obtained consistent superlinear parallelism by actually exploiting the heterogeneous nature of the machine. We eventually show that our dynamic approach competes with the highly optimized MAGMA library and overcomes the limitations of the corresponding static scheduling in a portable way. {\textcopyright} 2010 John Wiley & Sons, Ltd.},
author = {Augonnet, C{\'{e}}dric and Thibault, Samuel and Namyst, Raymond and Wacrenier, Pierre-Andr{\'{e}}},
doi = {10.1002/cpe.1631},
issn = {15320626},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {GPU,accelerator,multicore,runtime system,scheduling},
month = {feb},
number = {2},
pages = {187--198},
title = {{StarPU: a unified platform for task scheduling on heterogeneous multicore architectures}},
url = {https://onlinelibrary.wiley.com/doi/10.1002/cpe.1631},
volume = {23},
year = {2011}
}
@inproceedings{Parenteau2021,
abstract = {Traditionally, Computational Fluid Dynamics (CFD) software uses MPI (Message Passing Interface) to handle the parallelism over distributed memory systems and relies mostly on C, C++ and Fortran to ensure high performance. Consequently, the barrier of entry can be quite high for research and development, and productivity is therefore impacted. The Chapel programming language offers an interesting alternative tailored for research and development of CFD applications. In this paper, the developments of two CFD applications are presented: the first one as an experiment in rewriting a 2D structured flow solver and the second one as writing from scratch a 3D unstructured RANS simulation software named CHAMPS. Details are given on both applications with emphasis on the Chapel features which were used positively in the code design, in particular, to improve flexibility and extend the application from shared memory to distributed memory. Strong and weak scaling is evaluated up to 256 compute nodes on a Cray XC30 for a total of 9216 cores. Finally, CHAMPS is verified against well-established CFD software (FLO82, FUN3D, CFL3D and NSU3D).},
address = {Reston, Virginia},
author = {Parenteau, Matthieu and Bourgault-Cote, Simon and Plante, Fr{\'{e}}d{\'{e}}ric and Kayraklioglu, Engin and Laurendeau, Eric},
booktitle = {AIAA Scitech 2021 Forum},
doi = {10.2514/6.2021-0749},
isbn = {978-1-62410-609-5},
month = {jan},
pages = {1--16},
publisher = {American Institute of Aeronautics and Astronautics},
title = {{Development of Parallel CFD Applications with the Chapel Programming Language}},
url = {https://arc.aiaa.org/doi/10.2514/6.2021-0749},
year = {2021}
}
@inproceedings{Bohm2011,
   abstract = {Investigating parallel application performance at scale is an important part of high-performance computing (HPC) application development. The Extreme-scale Simulator (xSim) is a performance toolkit that permits running an application in a controlled environment at extreme scale without the need for a respective extreme-scale HPC system. Using a lightweight parallel discrete event simulation, xSim executes a parallel application with a virtual wall clock time, such that performance data can be extracted based on a processor and a network model. This paper presents significant enhancements to the xSim toolkit that provide a more complete Message Passing Interface (MPI) support and improve its versatility. These enhancements include full virtual MPI group, communicator and collective communication support, and global variables support. The new capabilities are demonstrated by executing the entire NAS Parallel Benchmark suite in a simulated HPC environment.},
   author = {Swen Bohm and Christian Engelmann},
   doi = {10.1109/HPCSim.2011.5999835},
   isbn = {978-1-61284-380-3},
   journal = {2011 International Conference on High Performance Computing & Simulation},
   keywords = {Message Passing Interface,hardware/software co-design,high-performance computing,parallel discrete event simulation,performance evaluation},
   month = {7},
   pages = {280-286},
   publisher = {IEEE},
   title = {xSim: The extreme-scale simulator},
   year = {2011},
}
@inproceedings{Zheng2004,
   abstract = {We present a parallel simulator-BigSim-for predicting performance of machines with a very large number of processors. The simulator provides the ability to make performance predictions for machines such as Blue-Gene/L, based on actual execution of real applications. We present this capability using case-studies of some application benchmarks. Such a simulator is useful to evaluate the performance of specific applications on such machines even before they are built. A sequential simulator may be too slow or infeasible. However, a parallel simulator faces problems of causality violations. We describe our scheme based on ideas from parallel discrete event simulation and utilize inherent determinacy of many parallel applications. We also explore techniques for optimizing such parallel simulations of machines with large number of processors on existing machines with fewer number of processors. 1},
   author = {Gengbin Zheng and G. Kakulapati and L.V. Kale},
   doi = {10.1109/IPDPS.2004.1303013},
   isbn = {0-7695-2132-0},
   journal = {18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.},
   pages = {78-87},
   publisher = {IEEE},
   title = {BigSim: a parallel simulator for performance prediction of extremely large parallel machines},
   url = {http://ieeexplore.ieee.org/document/1303013/},
   year = {2004},
}
@article{Hsieh2012,
abstract = {In this paper, we describe the integrated power, area and thermal modeling framework in the structural simulation toolkit (SST) for large-scale high performance computer simulation. It integrates various power and thermal modeling tools and computes run-time energy dissipation for core, network on chip, memory controller and shared cache. It also provides functionality to update the leakage power as temperature changes. We illustrate the utilization of the framework by applying it to explore interconnect options in manycore systems with consideration of temperature variation and leakage feedback. We compare power, energy-delay-area product (EDAP) and energy-delay product (EDP) of four manycore configurations-1 core, 2 cores, 4 cores and 8 cores per cluster. Results from simulation with or without consideration of temperature variation both show that the 4-core per cluster configuration has the best EDAP and EDP. Even so, considering that temperature variation increases total power dissipation, we demonstrate the importance of considering temperature variation in the design flow. With this power, area and thermal modeling capability, the SST can be used for hardware/software co-design of future exascale systems. {\textcopyright} 2011. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved.},
author = {Hsieh, M.-y. and Riesen, Rolf and Thompson, Kevin and Song, William and Rodrigues, Arun},
doi = {10.1093/comjnl/bxr069},
issn = {0010-4620},
journal = {The Computer Journal},
keywords = {NoC,performance modeling,power consumption,simulation framework},
mendeley-groups = {Simulation},
month = {feb},
number = {2},
pages = {181--191},
title = {{SST: A Scalable Parallel Framework for Architecture-Level Performance, Power, Area and Thermal Simulation}},
url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/bxr069},
volume = {55},
year = {2012}
}
@article{Mattson1998,
abstract = {Computer simulations needed by the U.S. Department of Energy (DOE) greatly exceed the capacity of the world's most powerful supercomputers. To satisfy this need, the DOE created the Accelerated Strategic Computing Initiative (ASCI). This program accelerates the development of new scalable supercomputers and will lead to a supercomputer early in the next century that can run at a rate of 100 trillion floating point operations per second (TFLOPS). Intel built the first computer in this program, the ASCI Option Red Supercomputer (also known as the Intel TFLOPS supercomputer). This system has over 4500 nodes, 594 Gbytes of RAM, and two independent 1 Tbyte disk systems. Late in the spring of 1997, we set the MP LINPACK world record of 1.34 TFLOPS. In this paper, we give an overview of the ASCI Option Red Supercomputer. The motivation for building this supercomputer is presented and the hardware and software views of the machine are described in detail. We also briefly discuss what it is like to use the machine.},
author = {Mattson, Timothy and Henry, Greg},
doi = {10.1.1.19.7228},
journal = {Intel Technology Journal},
number = {1},
pages = {1--12},
title = {{An Overview of the Intel TFLOPS Supercomputer}},
volume = {2},
year = {1998}
}
@techreport{Jansen2012,
abstract = {Tor is a large and popular overlay network providing both anonymity to its users and a platform for anonymous communication research. New design proposals and attacks on the system are challenging to test in the live network because of deployment issues and the risk of invading users' privacy, while alternative Tor experimentation techniques are limited in scale, are inaccurate, or create results that are difficult to reproduce or verify. We present the design and implementation of Shadow, an architecture for efficiently running accurate Tor experiments on a single machine. We validate Shadow's accuracy with a private Tor deployment on PlanetLab and a comparison to live network performance statistics. To demonstrate Shadow's powerful capabilities, we investigate circuit scheduling and find that the EWMA circuit scheduler reduces aggregate client performance under certain loads when deployed to the entire Tor network. Our software runs without root privileges, is open source, and is publicly available for download.},
author = {Jansen, Rob and Hopper, Nicholas},
mendeley-groups = {Simulation},
title = {{Shadow: Running Tor in a Box for Accurate and Efficient Experimentation}},
year = {2012}
}
@book{Jansen2022,
abstract = {Network experimentation tools are vitally important to the process of developing, evaluating, and testing distributed systems. The state-of-the-art simulation tools are either prohibitively inefficient at large scales or are limited by nontrivial architectural challenges, inhibiting their widespread adoption. In this paper, we present the design and implementation of Phantom, 1 a novel tool for conducting distributed system experiments. In Phantom, a discrete-event network simulator directly executes unmodified applications as Linux processes and innovatively synthesizes efficient process control, system call interposition, and data transfer methods to co-opt the processes into the simulation environment. Our evaluation demonstrates that Phantom is up to 2.2× faster than Shadow, up to 3.4× faster than NS-3, and up to 43× faster than gRaIL in large P2P benchmarks while offering performance comparable to Shadow in large Tor network simulations.},
author = {Jansen, Rob and Newsome, Jim and Project, Tor and Wails, Ryan},
isbn = {978-1-939133-29-52},
mendeley-groups = {Simulation},
pages = {327--350},
title = {{Co-opting Linux Processes for High-Performance Network Simulation}},
url = {https://www.usenix.org/conference/atc22/presentation/jansen},
year = {2022}
}
@misc{Top500,
  author = "TOP500.org",
  title = {{Top500's website}},
  howpublished = "\url{https://www.top500.org}",
  year = {2022},
  note = "[Online; accessed 12-Sept-2022]"
}
@misc{Top500_tera1000,
  author = "TOP500.org",
  title = {{TERA-1000-2 on the Top500's website}},
  howpublished = "\url{https://www.top500.org/system/179412/}",
  year = {2017},
  note = "[Online; accessed 12-Sept-2022]"
}
@misc{Top500_exa1,
  author = "TOP500.org",
  title = {{Exa1-HF on the Top500's website}},
  howpublished = "\url{https://www.top500.org/system/180031/}",
  year = {2021},
  note = "[Online; accessed 30-Dec-2022]"
}
@inproceedings{Gomez2007,
   abstract = {Clusters of PCs have become very popular to build high performance computers. These machines use commodity PCs linked by a high speed interconnect. Routing is one of the most important design issues of interconnection networks. Adaptive routing usually better balances network traffic, thus allowing the network to obtain a higher throughput. However, adaptive routing introduces out-of-order packet delivery, which is unacceptable for some applications. Concerning topology, most of the commercially available interconnects are based on fat-tree. Fat-trees offer a rich connectivity among nodes, making possible to obtain paths between all source-destination pairs that do not share any link. We exploit this idea to propose a deterministic routing algorithm for fat-trees, comparing it with adaptive routing in several workloads. The results show that determi-nistic routing can achieve a similar, and in some scenarios higher, level of performance than adaptive routing, while providing in-order packet delivery.},
   author = {C. Gomez and F. Gilabert and M.E. Gomez and P. Lopez and J. Duato},
   doi = {10.1109/IPDPS.2007.370482},
   isbn = {1-4244-0909-8},
   journal = {2007 IEEE International Parallel and Distributed Processing Symposium},
   keywords = {()},
   pages = {1-8},
   publisher = {IEEE},
   title = {Deterministic versus Adaptive Routing in Fat-Trees},
   url = {http://ieeexplore.ieee.org/document/4228210/},
   year = {2007},
}
@InProceedings{aevol,
  title         = {The Complexity Ratchet: Stronger than selection, weaker than robustness},
  author        = {Vincent Liard and David P. Parsons and Jonathan Rouzaud-Cornabas and Guillaume Beslon},
  year          = {2018},
  month         = jul,
  booktitle     = {Proceedings of Artificial Life 2018},
  pages         = {250--257},
  language      = {en}
}
@inproceedings{bellard2005qemu,
  title={QEMU, a fast and portable dynamic translator.},
  author={Bellard, Fabrice},
  booktitle={USENIX annual technical conference, FREENIX Track},
  volume={41},
  number={46},
  pages={10--5555},
  year={2005},
  organization={Califor-nia, USA}
}
